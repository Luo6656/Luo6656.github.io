<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Eric个人博客</title>
  
  
  <link href="http://luo6656.github.io/atom.xml" rel="self"/>
  
  <link href="http://luo6656.github.io/"/>
  <updated>2020-10-27T07:50:13.814Z</updated>
  <id>http://luo6656.github.io/</id>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据仓库</title>
    <link href="http://luo6656.github.io/2020/10/27/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    <id>http://luo6656.github.io/2020/10/27/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/</id>
    <published>2020-10-27T07:50:13.814Z</published>
    <updated>2020-10-27T07:50:13.814Z</updated>
    
    
    
    
    <category term="数据仓库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Kettle</title>
    <link href="http://luo6656.github.io/2020/09/03/ETL/Kettle/"/>
    <id>http://luo6656.github.io/2020/09/03/ETL/Kettle/</id>
    <published>2020-09-02T16:00:00.000Z</published>
    <updated>2020-10-27T06:29:00.192Z</updated>
    
    
    
    
    <category term="ETL工具" scheme="http://luo6656.github.io/categories/ETL%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
  <entry>
    <title>Tebleau</title>
    <link href="http://luo6656.github.io/2020/08/10/BI/Tebleau/"/>
    <id>http://luo6656.github.io/2020/08/10/BI/Tebleau/</id>
    <published>2020-08-09T16:00:00.000Z</published>
    <updated>2020-10-27T06:28:18.184Z</updated>
    
    
    
    
    <category term="BI工具" scheme="http://luo6656.github.io/categories/BI%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
  <entry>
    <title>PowerBI</title>
    <link href="http://luo6656.github.io/2020/08/05/BI/PowerBI/"/>
    <id>http://luo6656.github.io/2020/08/05/BI/PowerBI/</id>
    <published>2020-08-04T16:00:00.000Z</published>
    <updated>2020-10-27T06:28:18.132Z</updated>
    
    
    
    
    <category term="BI工具" scheme="http://luo6656.github.io/categories/BI%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
  <entry>
    <title>SQL语句调优</title>
    <link href="http://luo6656.github.io/2020/07/28/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E8%B0%83%E4%BC%98/"/>
    <id>http://luo6656.github.io/2020/07/28/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E8%B0%83%E4%BC%98/</id>
    <published>2020-07-27T16:00:00.000Z</published>
    <updated>2020-10-27T06:33:38.346Z</updated>
    
    
    
    
    <category term="数据库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Mysql底层原理</title>
    <link href="http://luo6656.github.io/2020/07/25/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql%E5%8E%9F%E7%90%86/"/>
    <id>http://luo6656.github.io/2020/07/25/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql%E5%8E%9F%E7%90%86/</id>
    <published>2020-07-24T16:00:00.000Z</published>
    <updated>2020-10-27T06:29:55.822Z</updated>
    
    
    
    
    <category term="数据库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase入门</title>
    <link href="http://luo6656.github.io/2020/07/15/BigDataFrame/HBase%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/07/15/BigDataFrame/HBase%E5%85%A5%E9%97%A8/</id>
    <published>2020-07-14T16:00:00.000Z</published>
    <updated>2020-10-27T05:49:46.459Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 HBase简介 / 第二章 HBase安装 / 第三章 HBase Shell操作 /<br>第四章 HBase数据结构 / 第五章 HBase原理 / 第六章 HBase API操作</p><a id="more"></a><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h3 id="第一章-HBase简介"><a href="#第一章-HBase简介" class="headerlink" title="第一章 HBase简介"></a>第一章 HBase简介</h3><h4 id="1-1-什么是HBase"><a href="#1-1-什么是HBase" class="headerlink" title="1.1 什么是HBase"></a>1.1 什么是HBase</h4><p>  HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p><h4 id="1-2-HBase特点"><a href="#1-2-HBase特点" class="headerlink" title="1.2 HBase特点"></a>1.2 HBase特点</h4><p>(1) <strong>海量存储</strong>：适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到几百毫秒内返回数据。这与Hbase的极易扩展性息息相关。正式因为Hbase良好的扩展性，才为海量数据的存储提供了便利。</p><p>(2) <strong>列式存储</strong>：这里的列式存储其实说的是<font color="red">列族（ColumnFamily）存储</font>，Hbase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。</p><p>(3) <strong>极易扩展</strong>：Hbase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。</p><p>通过横向添加RegionSever的机器，进行水平扩展，提升Hbase上层的处理能力，提升Hbsae服务更多Region的能力。</p><p>备注：RegionServer的作用是管理region、承接业务的访问，这个后面会详细的介绍通过横向添加Datanode的机器，进行存储层扩容，提升Hbase的数据存储能力和提升后端存储的读写能力。</p><p>(4) <strong>高并发（多核）</strong>：由于目前大部分使用Hbase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，Hbase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。</p><p>(5) <strong>稀疏</strong>：稀疏主要是针对Hbase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。</p><h4 id="1-3-HBase架构"><a href="#1-3-HBase架构" class="headerlink" title="1.3 HBase架构"></a>1.3 HBase架构</h4><p><img src="https://i.loli.net/2020/10/27/BtuKpVU9rScGnyi.png"></p><p>从图中可以看出Hbase是由Client、Zookeeper、Master、HRegionServer、HDFS等几个组件组成，下面来介绍一下几个组件的相关功能：</p><p><strong>(1) Client</strong></p><p>Client包含了访问Hbase的接口，另外Client还维护了对应的cache来加速Hbase的访问，比如cache的.META.元数据的信息。</p><p><strong>(2) Zookeeper</strong></p><p>HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。具体工作如下：</p><p>通过Zoopkeeper来保证集群中只有1个master在运行，如果master异常，会通过竞争机制产生新的master提供服务</p><p>通过Zoopkeeper来监控RegionServer的状态，当RegionSevrer有异常的时候，通过回调的形式通知Master RegionServer上下线的信息</p><p>通过Zoopkeeper存储元数据的统一入口地址</p><p><strong>(3) Hmaster(NameNode)</strong></p><p>master节点的主要职责如下：<br>为RegionServer分配Region<br>维护整个集群的负载均衡<br>维护集群的元数据信息<br>发现失效的Region，并将失效的Region分配到正常的RegionServer上<br>当RegionSever失效的时候，协调对应Hlog的拆分</p><p><strong>(4) HregionServer(DataNode)</strong></p><p><font color="red">HregionServer直接对接用户的读写请求</font>，是真正的“干活”的节点。它的功能概括如下：<br>管理master为其分配的Region<br>处理来自客户端的读写请求<br>负责和底层HDFS的交互，存储数据到HDFS<br>负责Region变大以后的拆分<br>负责Storefile的合并工作</p><p><strong>(5) HDFS</strong></p><p>HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用（Hlog存储在HDFS）的支持，具体功能概括如下：<br>提供元数据和表数据的底层分布式存储服务<br>数据多副本，保证的高可靠和高可用性</p><h4 id="1-3-HBase中的角色"><a href="#1-3-HBase中的角色" class="headerlink" title="1.3 HBase中的角色"></a>1.3 HBase中的角色</h4><h5 id="1-3-1-HMaster"><a href="#1-3-1-HMaster" class="headerlink" title="1.3.1 HMaster"></a>1.3.1 HMaster</h5><p><strong>功能</strong></p><p>1．监控RegionServer</p><p>2．处理RegionServer故障转移</p><p>3．处理元数据的变更</p><p>4．处理region的分配或转移</p><p>5．在空闲时间进行数据的负载均衡</p><p>6．通过Zookeeper发布自己的位置给客户端</p><h5 id="1-3-2-RegionServer"><a href="#1-3-2-RegionServer" class="headerlink" title="1.3.2 RegionServer"></a>1.3.2 RegionServer</h5><p><strong>功能</strong></p><p>1．负责存储HBase的实际数据</p><p>2．处理分配给它的Region</p><p>3．刷新缓存到HDFS</p><p>4．维护Hlog</p><p>5．执行压缩</p><p>6．负责处理Region分片</p><h5 id="1-3-3-其他组件"><a href="#1-3-3-其他组件" class="headerlink" title="1.3.3 其他组件"></a>1.3.3 其他组件</h5><p><strong>1．Write-Ahead logs</strong></p><p>HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p><p><strong>2．Region</strong></p><p>Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。</p><p><strong>3．Store</strong></p><p>HFile存储在Store中，一个Store对应HBase表中的一个列族(列簇， Column Family)。</p><p><strong>4．MemStore</strong></p><p>顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。</p><p><strong>5．HFile</strong></p><p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。StoreFile是以Hfile的形式存储在HDFS的。</p><h3 id="第二章-HBase安装"><a href="#第二章-HBase安装" class="headerlink" title="第二章 HBase安装"></a>第二章 HBase安装</h3><h4 id="2-1-Zookeeper正常部署"><a href="#2-1-Zookeeper正常部署" class="headerlink" title="2.1 Zookeeper正常部署"></a>2.1 Zookeeper正常部署</h4><p>首先保证Zookeeper集群的正常部署，并启动之：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><h4 id="2-2-Hadoop正常部署"><a href="#2-2-Hadoop正常部署" class="headerlink" title="2.2 Hadoop正常部署"></a>2.2 Hadoop正常部署</h4><p>Hadoop集群的正常部署并启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><h4 id="2-3-HBase的解压"><a href="#2-3-HBase的解压" class="headerlink" title="2.3 HBase的解压"></a>2.3 HBase的解压</h4><p>解压HBase到指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module</span><br></pre></td></tr></table></figure><h4 id="2-4-HBase的配置文件"><a href="#2-4-HBase的配置文件" class="headerlink" title="2.4 HBase的配置文件"></a>2.4 HBase的配置文件</h4><p>修改HBase对应的配置文件</p><p>(1) hbase-env.sh修改内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line">JDK1.8需要注释</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HBASE_MASTER_OPTS。。。。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HBASE_REGIONSERVER_OPTS。。。</span></span><br></pre></td></tr></table></figure><p>(2) hbase-site.xml修改内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">&lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.4.10/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(3) regionservers:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>(4) 软连接hadoop配置文件到hbase</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml </span><br><span class="line">/opt/module/hbase/conf/core-site.xml</span><br><span class="line">[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml </span><br><span class="line">/opt/module/hbase/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure><h4 id="2-5-HBase远程发送到其他集群"><a href="#2-5-HBase远程发送到其他集群" class="headerlink" title="2.5 HBase远程发送到其他集群"></a>2.5 HBase远程发送到其他集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync hbase/ </span><br></pre></td></tr></table></figure><h4 id="2-6-HBase服务的启动"><a href="#2-6-HBase服务的启动" class="headerlink" title="2.6 HBase服务的启动"></a>2.6 HBase服务的启动</h4><ol><li>启动方式1</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</p><p><font color="red">修复提示</font>：</p><p>a、同步时间服务：参照Hadoop入门</p><p>b、属性：hbase.master.maxclockskew设置更发的值</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>  2.启动方式2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/start-hbase.sh</span><br></pre></td></tr></table></figure><p>对应的停止服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure><h4 id="2-7-查看HBase页面"><a href="#2-7-查看HBase页面" class="headerlink" title="2.7 查看HBase页面"></a>2.7 查看HBase页面</h4><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p><p><a href="http://hadoop102:16010/">http://hadoop102:16010</a></p><h3 id="第三章-HBase-Shell操作"><a href="#第三章-HBase-Shell操作" class="headerlink" title="第三章 HBase Shell操作"></a>第三章 HBase Shell操作</h3><h4 id="3-1-基本操作"><a href="#3-1-基本操作" class="headerlink" title="3.1 基本操作"></a>3.1 基本操作</h4><ol><li><p>进入HBase客户端命令行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure></li><li><p>查看帮助命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; help</span><br></pre></td></tr></table></figure></li><li><p>查看当前数据库中有哪些表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-2-表的操作"><a href="#3-2-表的操作" class="headerlink" title="3.2 表的操作"></a>3.2 表的操作</h4><ol><li><p>创建表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure></li><li><p>插入数据到表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;</span><br><span class="line">hbase(main):004:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;</span><br><span class="line">hbase(main):005:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;</span><br><span class="line">hbase(main):006:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">hbase(main):007:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br></pre></td></tr></table></figure></li><li><p>扫描查看表数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan &#x27;student&#x27;</span><br><span class="line">hbase(main):009:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;, STOPROW  =&gt; &#x27;1001&#x27;&#125;</span><br><span class="line">hbase(main):010:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;&#125;</span><br></pre></td></tr></table></figure></li><li><p>查看表结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe &#x27;student&#x27;</span><br></pre></td></tr></table></figure></li><li><p>更新指定字段的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):012:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;</span><br><span class="line">hbase(main):013:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;</span><br></pre></td></tr></table></figure></li><li><p>查看“指定行”或“指定列族：列”的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;</span><br><span class="line">hbase(main):015:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;</span><br></pre></td></tr></table></figure></li><li><p>统计表数据行数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):021:0&gt; count &#x27;student&#x27;</span><br></pre></td></tr></table></figure></li><li><p>删除数据</p><p>删除某rowkey的全部数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall &#x27;student&#x27;,&#x27;1001&#x27;</span><br></pre></td></tr></table></figure><p>删除某rowkey的一列数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;</span><br></pre></td></tr></table></figure></li><li><p>清空表数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>清空表的操作顺序为先disable，然后再truncate</p></li><li><p>删除表</p><p>首先需要先让该表为disable状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):019:0&gt; disable &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p>然后才能drop这个表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>如果直接drop表，会报错：Error: Table student is enabled. Disable it first.</p></li><li><p>变更表信息</p><p>将info列族中的数据存放3个版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;</span><br><span class="line">hbase(main):022:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#123;COLUMN=&gt;&#x27;info:name&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="第四章-HBase数据结构"><a href="#第四章-HBase数据结构" class="headerlink" title="第四章 HBase数据结构"></a>第四章 HBase数据结构</h3><h4 id="4-1-RowKey"><a href="#4-1-RowKey" class="headerlink" title="4.1 RowKey"></a>4.1 RowKey</h4><p>与nosql数据库们一样,RowKey是用来检索记录的主键。访问HBASE table中的行，只有三种方式：</p><p><strong>1.通过单个RowKey访问(get)</strong></p><p><strong>2.通过RowKey的range（正则）(like)</strong></p><p><strong>3.全表扫描(scan)</strong></p><p>RowKey行键 (RowKey)可以是<strong>任意字符串</strong>(最大长度是64KB，实际应用中长度一般为 10-100bytes)，在HBASE内部，RowKey保存为字节数组。存储时，数据按照RowKey的字典序(byte order)排序存储。设计RowKey时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。<font color="red">(位置相关性)</font></p><h4 id="4-2-Column-Family"><a href="#4-2-Column-Family" class="headerlink" title="4.2 Column Family"></a>4.2 Column Family</h4><p>列族：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部 分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如 courses:history，courses:math都属于courses 这个列族。</p><h4 id="4-3-Cell"><a href="#4-3-Cell" class="headerlink" title="4.3 Cell"></a>4.3 Cell</h4><p>由{rowkey, column Family:columu, version} 唯一确定的单元。<font color="red">cell中的数据是没有类型的，全部是字节码形式存贮</font>。</p><p>关键字：无类型、字节码</p><h4 id="4-4-Time-Stamp"><a href="#4-4-Time-Stamp" class="headerlink" title="4.4 Time Stamp"></a>4.4 Time Stamp</h4><p>HBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒 的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版 本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</p><p>为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。</p><h4 id="4-5-命名空间"><a href="#4-5-命名空间" class="headerlink" title="4.5 命名空间"></a>4.5 命名空间</h4><p>命名空间的结构</p><p><img src="https://i.loli.net/2020/10/27/8wgkpNXH47WfjmU.png"></p><p>(1) Table：表，所有的表都是命名空间的成员，即表必属于某个命名空间，如果没有指定，则在default默认的命名空间中。</p><p>(2) RegionServer group：一个命名空间包含了默认的RegionServer Group。</p><p>(3) Permission：权限，命名空间能够让我们来定义访问控制列表ACL（Access Control List）。例如，创建表，读取表，删除，更新等等操作。</p><p>(4) Quota：限额，可以强制一个命名空间可包含的region的数量。</p><h3 id="第五章-HBase原理"><a href="#第五章-HBase原理" class="headerlink" title="第五章 HBase原理"></a>第五章 HBase原理</h3><h4 id="5-1-读流程"><a href="#5-1-读流程" class="headerlink" title="5.1 读流程"></a>5.1 读流程</h4><p><img src="https://i.loli.net/2020/10/27/JkdYl4fx7SnRPjb.png"></p><p>1）Client先访问zookeeper，从meta表读取region的位置，然后读取meta表中的数据。meta中又存储了用户表的region信息；</p><p>2）根据namespace、表名和rowkey在meta表中找到对应的region信息；</p><p>3）找到这个region对应的regionserver；</p><p>4）查找对应的region；</p><p>5）先从MemStore找数据，如果没有，再到BlockCache里面读；</p><p>6）BlockCache还没有，再到StoreFile上读(为了读取的效率)；</p><p>7）<font color="red">如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。</font></p><h4 id="5-2-写流程"><a href="#5-2-写流程" class="headerlink" title="5.2 写流程"></a>5.2 写流程</h4><p><img src="https://i.loli.net/2020/10/27/atWvhcZoDiJrbH4.png"></p><p>1）Client向HregionServer发送写请求；</p><p>2）HregionServer将数据写到HLog（write ahead log）。为了数据的持久化和恢复；</p><p>3）HregionServer将数据写到内存（MemStore）；</p><p>4）反馈Client写成功。</p><h4 id="5-3-数据flush过程"><a href="#5-3-数据flush过程" class="headerlink" title="5.3 数据flush过程"></a>5.3 数据flush过程</h4><p>1）当MemStore数据达到阈值（默认是128M，老版本是64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据；</p><p>2）并将数据存储到HDFS中；</p><p>3）在HLog中做标记点。</p><h4 id="5-4-数据合并过程"><a href="#5-4-数据合并过程" class="headerlink" title="5.4 数据合并过程"></a>5.4 数据合并过程</h4><p>1）当数据块达到3块，Hmaster触发合并操作，Region将数据块加载到本地，进行合并；</p><p>2）当合并的数据超过256M，进行拆分，将拆分后的Region分配给不同的HregionServer管理；</p><p>3）当HregionServer宕机后，将HregionServer上的hlog拆分，然后分配给不同的HregionServer加载，修改.META.；</p><p>4）注意：HLog会同步到HDFS。</p><h3 id="第六章-HBase-API操作"><a href="#第六章-HBase-API操作" class="headerlink" title="第六章 HBase API操作"></a>第六章 HBase API操作</h3><h4 id="6-1-环境准备"><a href="#6-1-环境准备" class="headerlink" title="6.1 环境准备"></a>6.1 环境准备</h4><p>新建项目后在pom.xml中添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="6-2-HBase-API"><a href="#6-2-HBase-API" class="headerlink" title="6.2 HBase API"></a>6.2 HBase API</h4><h5 id="6-2-1-获取Configuration-对象"><a href="#6-2-1-获取Configuration-对象" class="headerlink" title="6.2.1 获取Configuration 对象"></a>6.2.1 获取Configuration 对象</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Configuration conf;</span><br><span class="line"><span class="keyword">static</span>&#123;</span><br><span class="line"><span class="comment">//使用HBaseConfiguration的单例方法实例化</span></span><br><span class="line">conf = HBaseConfiguration.create();</span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;192.168.9.102&quot;</span>);</span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-2-判断表是否存在"><a href="#6-2-2-判断表是否存在" class="headerlink" title="6.2.2 判断表是否存在"></a>6.2.2 判断表是否存在</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExist</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException,</span></span><br><span class="line"><span class="function"> ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line"><span class="comment">//在HBase中管理、访问表需要先创建HBaseAdmin对象</span></span><br><span class="line"><span class="comment">//Connection connection = ConnectionFactory.createConnection(conf);</span></span><br><span class="line"><span class="comment">//HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span></span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="keyword">return</span> admin.tableExists(tableName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-3-创建表"><a href="#6-2-3-创建表" class="headerlink" title="6.2.3 创建表"></a>6.2.3 创建表</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName, String... columnFamily)</span> <span class="keyword">throws</span></span></span><br><span class="line"><span class="function"> MasterNotRunningException, ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="comment">//判断表是否存在</span></span><br><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;已存在&quot;</span>);</span><br><span class="line"><span class="comment">//System.exit(0);</span></span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="comment">//创建表属性对象,表名需要转字节</span></span><br><span class="line">HTableDescriptor descriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span><br><span class="line"><span class="comment">//创建多个列族</span></span><br><span class="line"><span class="keyword">for</span>(String cf : columnFamily)&#123;</span><br><span class="line">descriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(cf));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据对表的配置，创建表</span></span><br><span class="line">admin.createTable(descriptor);</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;创建成功！&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-4-删除表"><a href="#6-2-4-删除表" class="headerlink" title="6.2.4 删除表"></a>6.2.4 删除表</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException,</span></span><br><span class="line"><span class="function"> ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">admin.deleteTable(tableName);</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;删除成功！&quot;</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;不存在！&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-5-向表中插入数据"><a href="#6-2-5-向表中插入数据" class="headerlink" title="6.2.5 向表中插入数据"></a>6.2.5 向表中插入数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addRowData</span><span class="params">(String tableName, String rowKey, String columnFamily, String</span></span></span><br><span class="line"><span class="function"><span class="params"> column, String value)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line"><span class="comment">//创建HTable对象</span></span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line"><span class="comment">//向表中插入数据</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line"><span class="comment">//向Put对象中组装数据</span></span><br><span class="line">put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));</span><br><span class="line">hTable.put(put);</span><br><span class="line">hTable.close();</span><br><span class="line">System.out.println(<span class="string">&quot;插入数据成功&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-6-删除多行数据"><a href="#6-2-6-删除多行数据" class="headerlink" title="6.2.6 删除多行数据"></a>6.2.6 删除多行数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteMultiRow</span><span class="params">(String tableName, String... rows)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">List&lt;Delete&gt; deleteList = <span class="keyword">new</span> ArrayList&lt;Delete&gt;();</span><br><span class="line"><span class="keyword">for</span>(String row : rows)&#123;</span><br><span class="line">Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(row));</span><br><span class="line">deleteList.add(delete);</span><br><span class="line">&#125;</span><br><span class="line">hTable.delete(deleteList);</span><br><span class="line">hTable.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-7-获取所有数据"><a href="#6-2-7-获取所有数据" class="headerlink" title="6.2.7 获取所有数据"></a>6.2.7 获取所有数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getAllRows</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line"><span class="comment">//得到用于扫描region的对象</span></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"><span class="comment">//使用HTable得到resultcanner实现类的对象</span></span><br><span class="line">ResultScanner resultScanner = hTable.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span>(Result result : resultScanner)&#123;</span><br><span class="line">Cell[] cells = result.rawCells();</span><br><span class="line"><span class="keyword">for</span>(Cell cell : cells)&#123;</span><br><span class="line"><span class="comment">//得到rowkey</span></span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span><br><span class="line"><span class="comment">//得到列族</span></span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-8-获取某一行数据"><a href="#6-2-8-获取某一行数据" class="headerlink" title="6.2.8 获取某一行数据"></a>6.2.8 获取某一行数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line"><span class="comment">//get.setMaxVersions();显示所有版本</span></span><br><span class="line">    <span class="comment">//get.setTimeStamp();显示指定时间戳的版本</span></span><br><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(result.getRow()));</span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;时间戳:&quot;</span> + cell.getTimestamp());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-9-获取某一行指定“列族-列”的数据"><a href="#6-2-9-获取某一行指定“列族-列”的数据" class="headerlink" title="6.2.9 获取某一行指定“列族:列”的数据"></a>6.2.9 获取某一行指定“列族:列”的数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRowQualifier</span><span class="params">(String tableName, String rowKey, String family, String</span></span></span><br><span class="line"><span class="function"><span class="params"> qualifier)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));</span><br><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(result.getRow()));</span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-3-MapReduce"><a href="#6-3-MapReduce" class="headerlink" title="6.3 MapReduce"></a>6.3 MapReduce</h4><p>通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。</p><h5 id="6-3-1-官方HBase-MapReduce"><a href="#6-3-1-官方HBase-MapReduce" class="headerlink" title="6.3.1 官方HBase-MapReduce"></a>6.3.1 官方HBase-MapReduce</h5><ol><li><p><strong>查看HBase的MapReduce任务的执行</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/hbase mapredcp</span></span><br></pre></td></tr></table></figure></li><li><p><strong>环境变量的导入</strong></p><p>(1) 执行环境变量的导入（临时生效，在命令行执行下述操作）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HBASE_HOME=/opt/module/hbase-1.3.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_CLASSPATH=`<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin/hbase mapredcp`</span></span><br></pre></td></tr></table></figure><p>(2) 永久生效：在/etc/profile配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/module/hbase-1.3.1</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><p>并在hadoop-env.sh中配置：(注意：在for循环之后配)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><strong>运行官方的MapReduce任务</strong></li></ol><p>​     – 案例一：统计Student表中有多少行数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter student</span></span><br></pre></td></tr></table></figure><p>​     – 案例二：使用MapReduce将本地数据导入到HBase</p><p>​     (1) 在本地创建一个tsv格式的文件：fruit.tsv</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1001AppleRed</span><br><span class="line">1002PearYellow</span><br><span class="line">1003PineappleYellow</span><br></pre></td></tr></table></figure><p>​     (2) 创建HBase表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#39;fruit&#39;,&#39;info&#39;</span><br></pre></td></tr></table></figure><p>​     (3) 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/</span></span><br></pre></td></tr></table></figure><p>​     (4) 执行MapReduce到HBase的fruit表中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;bin&#x2F;yarn jar lib&#x2F;hbase-server-1.3.1.jar importtsv \</span><br><span class="line">-Dimporttsv.columns&#x3D;HBASE_ROW_KEY,info:name,info:color fruit \</span><br><span class="line">hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F;input_fruit</span><br></pre></td></tr></table></figure><p>​     (5) 使用scan命令查看导入后的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; scan ‘fruit’</span><br></pre></td></tr></table></figure><h5 id="6-3-2-自定义HBase-MapReduce1"><a href="#6-3-2-自定义HBase-MapReduce1" class="headerlink" title="6.3.2 自定义HBase-MapReduce1"></a>6.3.2 自定义HBase-MapReduce1</h5><p>目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。</p><p>分步实现：</p><p><strong>1．构建ReadFruitMapper类，用于读取fruit表中的数据</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.Cell;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.CellUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Result;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFruitMapper</span> <span class="keyword">extends</span> <span class="title">TableMapper</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(ImmutableBytesWritable key, Result value, Context context)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(key.get());</span><br><span class="line"><span class="comment">//遍历添加column行</span></span><br><span class="line"><span class="keyword">for</span>(Cell cell: value.rawCells())&#123;</span><br><span class="line"><span class="comment">//添加/克隆列族:info</span></span><br><span class="line"><span class="keyword">if</span>(<span class="string">&quot;info&quot;</span>.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123;</span><br><span class="line"><span class="comment">//添加/克隆列：name</span></span><br><span class="line"><span class="keyword">if</span>(<span class="string">&quot;name&quot;</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line"><span class="comment">//将该列cell加入到put对象中</span></span><br><span class="line">put.add(cell);</span><br><span class="line"><span class="comment">//添加/克隆列:color</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;color&quot;</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line"><span class="comment">//向该列cell加入到put对象中</span></span><br><span class="line">put.add(cell);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将从fruit读取到的每行数据写入到context中作为map的输出</span></span><br><span class="line">context.write(key, put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2． 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hbase_mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteFruitMRReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//读出来的每一行数据写入到fruit_mr表中</span></span><br><span class="line"><span class="keyword">for</span>(Put put: values)&#123;</span><br><span class="line">context.write(NullWritable.get(), put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3．构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//组装Job</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//得到Configuration</span></span><br><span class="line">Configuration conf = <span class="keyword">this</span>.getConf();</span><br><span class="line"><span class="comment">//创建Job任务</span></span><br><span class="line">Job job = Job.getInstance(conf, <span class="keyword">this</span>.getClass().getSimpleName());</span><br><span class="line">job.setJarByClass(Fruit2FruitMRRunner.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//配置Job</span></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setCacheBlocks(<span class="keyword">false</span>);</span><br><span class="line">scan.setCaching(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本</span></span><br><span class="line">TableMapReduceUtil.initTableMapperJob(</span><br><span class="line"><span class="string">&quot;fruit&quot;</span>, <span class="comment">//数据源的表名</span></span><br><span class="line">scan, <span class="comment">//scan扫描控制器</span></span><br><span class="line">ReadFruitMapper.class,<span class="comment">//设置Mapper类</span></span><br><span class="line">ImmutableBytesWritable.class,<span class="comment">//设置Mapper输出key类型</span></span><br><span class="line">Put.class,<span class="comment">//设置Mapper输出value值类型</span></span><br><span class="line">job<span class="comment">//设置给哪个JOB</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">//设置Reducer</span></span><br><span class="line">TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;fruit_mr&quot;</span>, WriteFruitMRReducer.class, job);</span><br><span class="line"><span class="comment">//设置Reduce数量，最少1个</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> isSuccess = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">if</span>(!isSuccess)&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Job running with error&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> isSuccess ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4．主函数中调用运行该Job任务</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line"><span class="keyword">int</span> status = ToolRunner.run(conf, <span class="keyword">new</span> Fruit2FruitMRRunner(), args);</span><br><span class="line">System.exit(status);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5．打包运行任务</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;bin&#x2F;yarn jar ~&#x2F;softwares&#x2F;jars&#x2F;hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr1.Fruit2FruitMRRunner</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：运行任务前，如果待数据导入的表不存在，则需要提前创建。</p><p><font color="red">提示</font>：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p><h5 id="6-3-3-自定义HBase-MapReduce2"><a href="#6-3-3-自定义HBase-MapReduce2" class="headerlink" title="6.3.3 自定义HBase-MapReduce2"></a>6.3.3 自定义HBase-MapReduce2</h5><p>目标：实现将HDFS中的数据写入到HBase表中。</p><p>分步实现：</p><p><strong>1．构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFruitFromHDFSMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//从HDFS中读取的数据</span></span><br><span class="line">String lineValue = value.toString();</span><br><span class="line"><span class="comment">//读取出来的每行数据使用\t进行分割，存于String数组</span></span><br><span class="line">String[] values = lineValue.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据数据中值的含义取值</span></span><br><span class="line">String rowKey = values[<span class="number">0</span>];</span><br><span class="line">String name = values[<span class="number">1</span>];</span><br><span class="line">String color = values[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化rowKey</span></span><br><span class="line">ImmutableBytesWritable rowKeyWritable = <span class="keyword">new</span> ImmutableBytesWritable(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化put对象</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line"><span class="comment">//参数分别:列族、列、值  </span></span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;name&quot;</span>),  Bytes.toBytes(name)); </span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;color&quot;</span>),  Bytes.toBytes(color)); </span><br><span class="line">        </span><br><span class="line">        context.write(rowKeyWritable, put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2．构建WriteFruitMRFromTxtReducer类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.z.hbase.mr2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteFruitMRFromTxtReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//读出来的每一行数据写入到fruit_hdfs表中</span></span><br><span class="line"><span class="keyword">for</span>(Put put: values)&#123;</span><br><span class="line">context.write(NullWritable.get(), put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3．创建Txt2FruitRunner组装Job</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//得到Configuration</span></span><br><span class="line">Configuration conf = <span class="keyword">this</span>.getConf();</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建Job任务</span></span><br><span class="line">Job job = Job.getInstance(conf, <span class="keyword">this</span>.getClass().getSimpleName());</span><br><span class="line">job.setJarByClass(Txt2FruitRunner.class);</span><br><span class="line">Path inPath = <span class="keyword">new</span> Path(<span class="string">&quot;hdfs://hadoop102:9000/input_fruit/fruit.tsv&quot;</span>);</span><br><span class="line">FileInputFormat.addInputPath(job, inPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Mapper</span></span><br><span class="line">job.setMapperClass(ReadFruitFromHDFSMapper.class);</span><br><span class="line">job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Reducer</span></span><br><span class="line">TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;fruit_mr&quot;</span>, WriteFruitMRFromTxtReducer.class, job);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Reduce数量，最少1个</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> isSuccess = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">if</span>(!isSuccess)&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Job running with error&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> isSuccess ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4．调用执行Job</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">    <span class="keyword">int</span> status = ToolRunner.run(conf, <span class="keyword">new</span> Txt2FruitRunner(), args);</span><br><span class="line">    System.exit(status);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5．打包运行</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/yarn jar hbase-0.0.1-SNAPSHOT.jar com.atguigu.hbase.mr2.Txt2FruitRunner</span></span><br></pre></td></tr></table></figure><p>提示：运行任务前，如果待数据导入的表不存在，则需要提前创建之。</p><p>提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p><h4 id="6-4-与Hive的集成"><a href="#6-4-与Hive的集成" class="headerlink" title="6.4 与Hive的集成"></a>6.4 与Hive的集成</h4><h5 id="6-4-1-HBase与Hive的对比"><a href="#6-4-1-HBase与Hive的对比" class="headerlink" title="6.4.1 HBase与Hive的对比"></a>6.4.1 HBase与Hive的对比</h5><p><strong>1．Hive</strong></p><p>(1) 数据仓库</p><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p><p>(2) 用于数据分析、清洗</p><p>Hive适用于离线的数据分析和清洗，延迟较高。</p><p>(3) 基于HDFS、MapReduce</p><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p><p><strong>2．HBase</strong></p><p>(1) 数据库</p><p>是一种面向列存储的非关系型数据库。</p><p>(2) 用于存储结构化和非结构化的数据</p><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p><p>(3) 基于HDFS</p><p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p><p>(4) 延迟较低，接入在线业务使用</p><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 HBase简介 / 第二章 HBase安装 / 第三章 HBase Shell操作 /&lt;br&gt;第四章 HBase数据结构 / 第五章 HBase原理 / 第六章 HBase API操作&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive高阶</title>
    <link href="http://luo6656.github.io/2020/07/05/BigDataFrame/Hive%E9%AB%98%E9%98%B6/"/>
    <id>http://luo6656.github.io/2020/07/05/BigDataFrame/Hive%E9%AB%98%E9%98%B6/</id>
    <published>2020-07-04T16:00:00.000Z</published>
    <updated>2020-10-27T05:11:13.734Z</updated>
    
    <content type="html"><![CDATA[<p>第七章 函数 / 第八章 压缩和存储 </p><a id="more"></a><h3 id="第七章-函数"><a href="#第七章-函数" class="headerlink" title="第七章 函数"></a>第七章 函数</h3><h4 id="7-1-系统内置函数"><a href="#7-1-系统内置函数" class="headerlink" title="7.1 系统内置函数"></a>7.1 系统内置函数</h4><ol><li><p>查看系统自带的函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br></pre></td></tr></table></figure></li><li><p>显示自带的函数的用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function upper;</span><br></pre></td></tr></table></figure></li><li><p>详细显示自带的函数的用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function extended upper;</span><br></pre></td></tr></table></figure></li></ol><h4 id="7-2-自定义函数"><a href="#7-2-自定义函数" class="headerlink" title="7.2 自定义函数"></a>7.2 自定义函数</h4><ol><li><p>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</p></li><li><p>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户定义函数（UDF：user-defined function）</p></li><li><p>根据用户自定义函数类别分为以下三种    （多针对的是行）</p><p>（1）UDF（User-Defined-Function）</p><p>​        一进一出</p><p>（2）UDAF（User-Defined Aggregation Function）</p><p>​        聚集函数，多进一出</p><p>​        类似于：count/max/min</p><p>（3）UDTF（User-Defined Table-Generating Functions）</p><p>​        一进多出</p><p>​        如lateral view explore()</p></li><li><p>官方文档地址</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p></blockquote></li><li><p>编程步骤</p><p>（1）<font color="red">继承org.apache.hadoop.hive.ql.exec.UDF</font></p><p>（2）<font color="red">需要实现evaluate函数；evaluate函数支持重载；</font></p><p>（3）<font color="red">在hive的命令行窗口创建函数</font></p><p>​        a）<font color="red">添加jar</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure><p>​        b）<font color="red">创建function</font></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [<span class="keyword">temporary</span>] <span class="keyword">function</span> [dbname.]function_name <span class="keyword">AS</span> class_name;</span><br></pre></td></tr></table></figure><p>​    （4）在hive的命令行窗口删除函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Drop</span> [<span class="keyword">temporary</span>] <span class="keyword">function</span> [<span class="keyword">if</span> <span class="keyword">exists</span>] [dbname.]function_name;</span><br></pre></td></tr></table></figure></li><li><p>注意事项</p><p>（1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p></li></ol><h4 id="7-3-自定义UDF函数"><a href="#7-3-自定义UDF函数" class="headerlink" title="7.3 自定义UDF函数"></a>7.3 自定义UDF函数</h4><ol><li><p>创建一个Maven工程Hive</p></li><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>创建一个类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lower</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span> <span class="params">(<span class="keyword">final</span> String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s.toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打包成jar包上传到服务器/opt/module/jars/udf.jar</p></li><li><p>将jar包添加到hive的classpath</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/datas/udf.jar;</span><br></pre></td></tr></table></figure></li><li><p>创建临时函数与开发好的Java class关联</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function mylower as &quot;com.atguigu.hive.Lower&quot;; </span><br><span class="line"><span class="comment">/*退出hive就不可用了*/</span></span><br></pre></td></tr></table></figure></li><li><p>即可在hql中使用自定义的函数strip</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, mylower(ename) lowername from emp;</span><br></pre></td></tr></table></figure></li></ol><h3 id="第八章-压缩和存储"><a href="#第八章-压缩和存储" class="headerlink" title="第八章 压缩和存储"></a>第八章 压缩和存储</h3><h4 id="8-1-Hadoop源码编译支持Snappy压缩"><a href="#8-1-Hadoop源码编译支持Snappy压缩" class="headerlink" title="8.1 Hadoop源码编译支持Snappy压缩"></a>8.1 Hadoop源码编译支持Snappy压缩</h4><h5 id="8-1-1-资源准备"><a href="#8-1-1-资源准备" class="headerlink" title="8.1.1 资源准备"></a>8.1.1 资源准备</h5><ol><li><p>CentOS联网</p><p>配置CentOS能连接外网。Linux虚拟机ping <a href="http://www.baidu.com是畅通的/">www.baidu.com是畅通的</a></p><p>注意：<font color="red">采用root角色编译</font>，减少文件夹权限出现问题</p></li><li><p>jar包准备(hadoop源码、JDK8、maven、protobuf)</p><p>（1）hadoop-2.7.2-src.tar.gz</p><p>（2）jdk-8u144-linux-x64.tar.gz</p><p>（3）snappy-1.1.3.tar.gz</p><p>（4）apache-maven-3.0.5-bin.tar.gz</p><p>（5）protobuf-2.5.0.tar.gz</p></li></ol><h5 id="8-1-2-jar包安装"><a href="#8-1-2-jar包安装" class="headerlink" title="8.1.2 jar包安装"></a>8.1.2 jar包安装</h5><p>​        <font color="red">注意：所有操作必须在root用户下完成</font></p><ol><li><p>JDK解压、配置环境变量JAVA_HOME和PATH，验证<a href="http://lib.csdn.net/base/javase">java</a>-version(如下都需要验证是否配置成功)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 software]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@hadoop101 software]#source /etc/profile</span><br></pre></td></tr></table></figure><p>验证命令：java -version</p></li><li><p>Maven解压、配置 MAVEN_HOME和PATH</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.0.5</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line">[root@hadoop101 software]#source /etc/profile</span><br></pre></td></tr></table></figure><p>验证命令：mvn -version</p></li></ol><h5 id="8-1-3-编译源码"><a href="#8-1-3-编译源码" class="headerlink" title="8.1.3 编译源码"></a>8.1.3 编译源码</h5><ol><li><p>准备编译环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# yum install svn</span><br><span class="line">[root@hadoop101 software]# yum install autoconf automake libtool cmake</span><br><span class="line">[root@hadoop101 software]# yum install ncurses-devel</span><br><span class="line">[root@hadoop101 software]# yum install openssl-devel</span><br><span class="line">[root@hadoop101 software]# yum install gcc*</span><br></pre></td></tr></table></figure></li><li><p>编译安装snappy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 module]# cd snappy-1.1.3/</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# ./configure</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# make</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# make install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看snappy库文件</span></span><br><span class="line">[root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy</span><br></pre></td></tr></table></figure></li><li><p>编译安装protobuf </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 module]# cd protobuf-2.5.0/</span><br><span class="line">[root@hadoop101 protobuf-2.5.0]# ./configure </span><br><span class="line">[root@hadoop101 protobuf-2.5.0]#  make </span><br><span class="line">[root@hadoop101 protobuf-2.5.0]#  make install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看protobuf版本以测试是否安装成功</span></span><br><span class="line">[root@hadoop101 protobuf-2.5.0]# protoc --version</span><br></pre></td></tr></table></figure></li><li><p>编译hadoop native</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz</span><br><span class="line">[root@hadoop101 software]# cd hadoop-2.7.2-src/</span><br><span class="line">[root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy</span><br></pre></td></tr></table></figure><p>执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/<a href="http://lib.csdn.net/base/hadoop">hadoop</a>-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。</p></li></ol><h4 id="8-2-Hadoop压缩配置"><a href="#8-2-Hadoop压缩配置" class="headerlink" title="8.2 Hadoop压缩配置"></a>8.2 Hadoop压缩配置</h4><h5 id="8-2-1-MR支持的压缩编码"><a href="#8-2-1-MR支持的压缩编码" class="headerlink" title="8.2.1 MR支持的压缩编码"></a>8.2.1 MR支持的压缩编码</h5><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFLATE</td><td>无</td><td>DEFLATE</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>无</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较：</p><p>表6-10</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h5 id="8-2-2-压缩参数配置"><a href="#8-2-2-压缩参数配置" class="headerlink" title="8.2.2 压缩参数配置"></a>8.2.2 压缩参数配置</h5><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h4 id="8-3-开启Map输出阶段压缩"><a href="#8-3-开启Map输出阶段压缩" class="headerlink" title="8.3 开启Map输出阶段压缩"></a>8.3 开启Map输出阶段压缩</h4><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p><p><strong>案例实操</strong></p><ol><li><p>开启hive中间传输数据压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure></li><li><p>开启mapreduce中map输出压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce中map输出数据的压缩方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li><li><p>执行查询语句</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure></li></ol><h4 id="8-4-开启Reduce输出阶段压缩"><a href="#8-4-开启Reduce输出阶段压缩" class="headerlink" title="8.4 开启Reduce输出阶段压缩"></a>8.4 开启Reduce输出阶段压缩</h4><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p><p><strong>案例实操</strong></p><ol><li><p>开启hive最终输出数据压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure></li><li><p>开启mapreduce最终输出数据压缩</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce最终数据输出压缩方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce最终数据输出压缩为块压缩</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure></li><li><p>测试一下输出结果是否是压缩文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/module/datas/distribute-result&#x27; <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="8-5-文件存储格式"><a href="#8-5-文件存储格式" class="headerlink" title="8.5 文件存储格式"></a>8.5 文件存储格式</h4><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h5 id="8-5-1-列式存储和行式存储"><a href="#8-5-1-列式存储和行式存储" class="headerlink" title="8.5.1 列式存储和行式存储"></a>8.5.1 列式存储和行式存储</h5><p><img src="https://i.loli.net/2020/10/27/oWu7lQi6reEbjKI.png"></p><p>如图6-10所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><p>1．行存储的特点</p><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><p>2．列存储的特点</p><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><p>​    <font color="red">TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</font></p><p>​    <font color="red">ORC和PARQUET是基于列式存储的。</font></p><h5 id="8-5-2-TextFile格式"><a href="#8-5-2-TextFile格式" class="headerlink" title="8.5.2 TextFile格式"></a>8.5.2 TextFile格式</h5><p>​        默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h5 id="8-5-3-Orc格式"><a href="#8-5-3-Orc格式" class="headerlink" title="8.5.3 Orc格式"></a>8.5.3 Orc格式</h5><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p><p>如图6-11所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p><p><img src="https://i.loli.net/2020/10/27/SZj1Bvfl3KchVGb.png"></p><p>​     1）Index Data：一个轻量级的index，默认是<font color="red">每隔1W行做一个索引</font>。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p><pre><code>  2）Row Data：存的是具体的数据，&lt;font color=&quot;red&quot;&gt;先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储&lt;/font&gt;。  3）Stripe Footer：存的是各个Stream的类型，长度等信息。</code></pre><p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h5 id="8-5-4-Parquet格式"><a href="#8-5-4-Parquet格式" class="headerlink" title="8.5.4 Parquet格式"></a>8.5.4 Parquet格式</h5><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，<font color="red">因此Parquet格式文件是自解析的。</font></p><ol><li><p>行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p></li><li><p>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p></li><li><p>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p></li></ol><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把<font color="red">每一个行组由一个Mapper任务处理，增大任务执行并行度。</font>Parquet文件的格式如图6-12所示。</p><p><img src="https://i.loli.net/2020/10/27/h7MIWijkGQXaut1.png"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h5 id="8-5-5-主流文件存储格式对比实验"><a href="#8-5-5-主流文件存储格式对比实验" class="headerlink" title="8.5.5 主流文件存储格式对比实验"></a>8.5.5 主流文件存储格式对比实验</h5><p>从存储文件的压缩比和查询速度两个角度对比。</p><p><strong>存储文件的压缩比测试：</strong></p><ol><li><p>测试数据</p><p>log.data</p></li><li><p>TextFile</p><p>(1)创建表，存储数据格式为TEXTFILE </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/log.data&#x27; into table log_text ;</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;</span><br></pre></td></tr></table></figure><p>18.1 M  /user/hive/warehouse/log_text/log.data</p></li><li><p>ORC（ORC格式默认开启压缩）</p><p>(1)创建表，存储数据格式为ORC </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc select * from log_text ; /*不能使用load，因为load是直接put的*/</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/;</span><br><span class="line">2.8 M  /user/hive/warehouse/log_orc/000000_0</span><br></pre></td></tr></table></figure></li><li><p>Parquet</p><p>(1)创建表，存储数据格式为parquet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;</span><br><span class="line">13.1 M /user/hive/warehouse/log_parquet/000000_0</span><br></pre></td></tr></table></figure><p>存储文件的压缩比总结：</p><p>ORC &gt;  Parquet &gt;  textFile</p></li></ol><p><strong>存储文件的查询速度测试：</strong></p><p>TextFile </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_text;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 21.54 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 21.08 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 19.298 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>ORC </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_orc;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 20.867 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 22.667 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 18.36 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>Parquet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_parquet;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 22.922 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 21.074 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 18.384 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>存储文件的查询速度总结：查询速度相近。</p><h4 id="8-6-存储和压缩结合"><a href="#8-6-存储和压缩结合" class="headerlink" title="8.6 存储和压缩结合"></a>8.6 存储和压缩结合</h4><h5 id="8-6-1-修改Hadoop集群具有Snappy压缩方法"><a href="#8-6-1-修改Hadoop集群具有Snappy压缩方法" class="headerlink" title="8.6.1 修改Hadoop集群具有Snappy压缩方法"></a>8.6.1 修改Hadoop集群具有Snappy压缩方法</h5><ol><li><p>查看hadoop checknative命令使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ hadoop</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br></pre></td></tr></table></figure></li><li><p>查看hadoop支持的压缩方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative</span><br><span class="line">17&#x2F;12&#x2F;24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load&#x2F;initialize native-bzip2 library system-native, will use pure-Java version</span><br><span class="line">17&#x2F;12&#x2F;24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libhadoop.so</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  false </span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   false</span><br></pre></td></tr></table></figure></li><li><p>将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中</p></li></ol><ol start="4"><li><p>解压hadoop-2.7.2.tar.gz到当前路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 native]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native</span><br><span class="line">[atguigu@hadoop102 native]$ ll</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  472950 9月   1 10:19 libsnappy.a</span><br><span class="line">-rwxr-xr-x. 1 atguigu atguigu     955 9月   1 10:19 libsnappy.la</span><br><span class="line">lrwxrwxrwx. 1 atguigu atguigu      18 12月 24 20:39 libsnappy.so -&gt; libsnappy.so.1.3.0</span><br><span class="line">lrwxrwxrwx. 1 atguigu atguigu      18 12月 24 20:39 libsnappy.so.1 -&gt; libsnappy.so.1.3.0</span><br><span class="line">-rwxr-xr-x. 1 atguigu atguigu  228177 9月   1 10:19 libsnappy.so.1.3.0</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 native]$ cp ..&#x2F;native&#x2F;* &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>分发集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$ xsync native&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>再次查看hadoop支持的压缩类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop checknative</span><br><span class="line">17&#x2F;12&#x2F;24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load&#x2F;initialize native-bzip2 library system-native, will use pure-Java version</span><br><span class="line">17&#x2F;12&#x2F;24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libhadoop.so</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libsnappy.so.1</span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   false</span><br></pre></td></tr></table></figure></li><li><p>重新启动hadoop集群和hive</p></li></ol><h5 id="8-6-2-测试存储和压缩"><a href="#8-6-2-测试存储和压缩" class="headerlink" title="8.6.2 测试存储和压缩"></a>8.6.2 测试存储和压缩</h5><p>官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p><p>ORC存储方式的压缩：</p><table><thead><tr><th>Key</th><th>Default</th><th>Notes</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>high level compression (one of NONE, ZLIB, SNAPPY)</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>number of bytes in each compression chunk</td></tr><tr><td>orc.stripe.size</td><td>268,435,456</td><td>number of bytes in each stripe</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>number of rows between index entries (must be &gt;= 1000)</td></tr><tr><td>orc.create.index</td><td>true</td><td>whether to create row indexes</td></tr><tr><td>orc.bloom.filter.columns</td><td>“”</td><td>comma separated list of column names for which bloom filter should be created</td></tr><tr><td>orc.bloom.filter.fpp</td><td>0.05</td><td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td></tr></tbody></table><p>注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现</p><ol><li><p>创建一个非压缩的ORC存储方式</p><p>（1）建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">&quot;orc.compress&quot;</span>=<span class="string">&quot;NONE&quot;</span>);</span><br></pre></td></tr></table></figure><p>（2）插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc_none select * from log_text ;</span><br></pre></td></tr></table></figure><p>（3）查看插入后数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ;</span><br><span class="line">7.7 M  /user/hive/warehouse/log_orc_none/000000_0</span><br></pre></td></tr></table></figure></li><li><p>创建一个SNAPPY压缩的ORC存储方式</p><p>（1）建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">&quot;orc.compress&quot;</span>=<span class="string">&quot;SNAPPY&quot;</span>);</span><br></pre></td></tr></table></figure><p>（2）插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc_snappy select * from log_text ;</span><br></pre></td></tr></table></figure><p>（3）查看插入后数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ;</span><br></pre></td></tr></table></figure><p>3.8 M  /user/hive/warehouse/log_orc_snappy/000000_0</p></li><li><p>上一节中默认创建的ORC存储方式，导入数据后的大小为2.8 M /user/hive/warehouse/log_orc/000000_0</p><p>比Snappy压缩的还小。原因是orc存储文件默认采用<font color="red">ZLIB压缩</font>，ZLIB采用的是deflate压缩算法。比snappy压缩的小。</p></li><li><p>存储方式和压缩总结</p><p>在实际的项目开发当中，hive表的数据存储格式一般选择:orc或parquet。压缩方式一般选择snappy，lzo</p></li></ol><h3 id="第九章-企业级调优"><a href="#第九章-企业级调优" class="headerlink" title="第九章 企业级调优"></a>第九章 企业级调优</h3><h4 id="9-1-Fetch抓取"><a href="#9-1-Fetch抓取" class="headerlink" title="9.1 Fetch抓取"></a>9.1 Fetch抓取</h4><p>Fetch抓取是指，<font color="red">Hive中对某些情况的查询可以不必使用MapReduce计算</font>。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是<font color="red">minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce</font>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;more&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some <span class="keyword">select</span> queries can be converted <span class="keyword">to</span> single <span class="keyword">FETCH</span> task minimizing latency.</span><br><span class="line">      Currently the <span class="keyword">query</span> should be single sourced <span class="keyword">not</span> <span class="keyword">having</span> <span class="keyword">any</span> subquery <span class="keyword">and</span> should <span class="keyword">not</span> have <span class="keyword">any</span> aggregations <span class="keyword">or</span> distincts (which incurs RS), <span class="keyword">lateral</span> views <span class="keyword">and</span> joins.</span><br><span class="line">      <span class="number">0.</span> <span class="keyword">none</span> : <span class="keyword">disable</span> hive.fetch.task.conversion</span><br><span class="line">      <span class="number">1.</span> minimal : <span class="keyword">SELECT</span> STAR, FILTER <span class="keyword">on</span> <span class="keyword">partition</span> <span class="keyword">columns</span>, <span class="keyword">LIMIT</span> <span class="keyword">only</span></span><br><span class="line">      <span class="number">2.</span> more  : <span class="keyword">SELECT</span>, FILTER, <span class="keyword">LIMIT</span> <span class="keyword">only</span> (support <span class="keyword">TABLESAMPLE</span> <span class="keyword">and</span> <span class="keyword">virtual</span> <span class="keyword">columns</span>)</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><p><strong>案例实操：</strong></p><p>​    1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.fetch.task.conversion=none;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp limit 3;</span><br></pre></td></tr></table></figure><p>​    2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.fetch.task.conversion=more;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp limit 3;</span><br></pre></td></tr></table></figure><h4 id="9-2-本地模式"><a href="#9-2-本地模式" class="headerlink" title="9.2 本地模式"></a>9.2 本地模式</h4><p>​        大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，<font color="red">Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</font></p><p>​        用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化，默认是false。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  //开启本地mr</span><br><span class="line">//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line">//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max=<span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><strong>案例实操：</strong></p><p>1）开启本地模式，并执行查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.exec.mode.local.auto=true; </span><br><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">Time taken: 1.328 seconds, Fetched: 14 row(s)</span><br></pre></td></tr></table></figure><p>2）关闭本地模式，并执行查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.exec.mode.local.auto=false; </span><br><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">Time taken: 20.09 seconds, Fetched: 14 row(s)</span><br></pre></td></tr></table></figure><h4 id="9-3-表的优化"><a href="#9-3-表的优化" class="headerlink" title="9.3 表的优化"></a>9.3 表的优化</h4><h5 id="9-3-1-小表、大表Join"><a href="#9-3-1-小表、大表Join" class="headerlink" title="9.3.1 小表、大表Join"></a>9.3.1 小表、大表Join</h5><p>​        将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p>​        <font color="red">实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</font></p><p><strong>案例实操</strong></p><ol><li><p>需求</p><p>测试大表join小表和小表join大表的效率</p></li><li><p>建大表、小表和JOIN后表的语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建大表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建小表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建join后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>分别向大表和小表中导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/bigtable&#x27; into table bigtable;</span><br><span class="line">hive (default)&gt;load data local inpath &#x27;/opt/module/datas/smalltable&#x27; into table smalltable;</span><br></pre></td></tr></table></figure></li><li><p>关闭mapjoin功能（默认是打开的）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">false</span>;</span><br></pre></td></tr></table></figure></li><li><p>执行小表JOIN大表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable  b</span><br><span class="line"><span class="keyword">on</span> b.id = s.id;</span><br></pre></td></tr></table></figure><p>Time taken: 35.921 seconds</p><p>No rows affected (44.456 seconds)</p></li><li><p>执行大表JOIN小表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> smalltable  s</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 34.196 seconds</p><p>No rows affected (26.287 seconds)</p></li></ol><h5 id="9-3-2-大表join大表"><a href="#9-3-2-大表join大表" class="headerlink" title="9.3.2 大表join大表"></a>9.3.2 大表join大表</h5><ol><li><p>空key过滤</p><p>​        有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p><p><strong>案例实操</strong></p><p>（1）配置历史服务器</p><p>配置mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>查看jobhistroy</p><p><a href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p>（2）创建原始数据表、空id表、合并后数据表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建原始表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建空id表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> nullidtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建join后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）分别加载原始数据和空id数据到对应表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/ori&#x27; into table ori;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/nullid&#x27; into table nullidtable;</span><br></pre></td></tr></table></figure><p>（4）测试不过滤空id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table jointable select n.* from nullidtable n</span><br><span class="line">left join ori o on n.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 42.038 seconds</p><p>Time taken: 37.284 seconds</p><p>（5）测试过滤空id</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table jointable select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 31.725 seconds</p><p>Time taken: 28.876 seconds</p></li><li><p>空key转换</p><p>​        有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p><p><strong>案例实操</strong>：</p><p>不随机分布空null值：</p><p>（1）设置5个reduce个数</p><p>​            set mapreduce.job.reduces = 5;</p><p>（2）join两张表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> ori b <span class="keyword">on</span> n.id = b.id;</span><br></pre></td></tr></table></figure><p><strong>结果：可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</strong></p><p><img src="https://i.loli.net/2020/10/27/pd6UjBXHtNvPcem.png"></p><p>（1）设置分布空null值</p><p>​        set mapreduce.job.reduces = 5;</p><p>（2）JOIN两张表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">&#x27;hive&#x27;</span>, <span class="keyword">rand</span>()) <span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span><br></pre></td></tr></table></figure><p><strong>结果：如图6-14所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</strong></p><p><img src="https://i.loli.net/2020/10/27/OlcrFoHfz28nY5m.png"></p></li></ol><h5 id="9-3-3-MapJoin-小表join大表"><a href="#9-3-3-MapJoin-小表join大表" class="headerlink" title="9.3.3 MapJoin(小表join大表)"></a>9.3.3 MapJoin(小表join大表)</h5><p>​        如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p><ol><li><p>开启MapJoin参数设置</p><p>（1）设置自动选择mapjoin</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; <span class="comment">/*默认为true*/</span></span><br></pre></td></tr></table></figure><p>（2）大表小表的阈值设置（默认25M一下认为是小表）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>MapJoin工作机制，如下图</p><p><img src="https://i.loli.net/2020/10/27/wNUjZQpnEmACX26.png"></p></li></ol><p><strong>案例实操</strong></p><p>​    （1）开启Mapjoin功能</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; 默认为true</span><br></pre></td></tr></table></figure><p>​    （2）执行小表join大表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">join</span> bigtable  b</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 24.594 seconds</p><p>​    （3）执行大表join小表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line"><span class="keyword">join</span> smalltable  s</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 24.315 seconds</p><h5 id="9-3-4-Group-By"><a href="#9-3-4-Group-By" class="headerlink" title="9.3.4 Group By"></a>9.3.4 Group By</h5><p><font color="red">默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了</font>。</p><p><img src="https://i.loli.net/2020/10/27/S6WBaLj18eMoAtw.png"></p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><p><strong>开启Map端聚合参数设置</strong></p><ol><li><p>是否在Map端进行聚合，默认为true</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>在Map端进行聚合操作的条目数目</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval = <span class="number">100000</span></span><br></pre></td></tr></table></figure></li><li><p>有数据倾斜的时候进行负载均衡（默认是false）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno from emp group by deptno;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 23.68 sec   HDFS Read: 19987 HDFS Write: 9 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 23 seconds 680 msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line">10</span><br><span class="line">20</span><br><span class="line">30</span><br></pre></td></tr></table></figure><p>优化之后</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.groupby.skewindata = true;</span><br><span class="line">hive (default)&gt; select deptno from emp group by deptno;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 28.53 sec   HDFS Read: 18209 HDFS Write: 534 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 1  Reduce: 5   Cumulative CPU: 38.32 sec   HDFS Read: 15014 HDFS Write: 9 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line">10</span><br><span class="line">20</span><br><span class="line">30</span><br></pre></td></tr></table></figure></li></ol><h5 id="9-3-5-Count-distinct-去重统计—-会内存溢出"><a href="#9-3-5-Count-distinct-去重统计—-会内存溢出" class="headerlink" title="9.3.5 Count(distinct) 去重统计—-会内存溢出"></a>9.3.5 Count(distinct) 去重统计—-会内存溢出</h5><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT的全聚合操作，即使设定了reduce task个数，set mapred.reduce.tasks=100；hive也只会启动一个reducer。，这就造成一个Reduce处理的数据量太大，导致整个Job很难完成，<font color="red">一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</font></p><p><strong>案例实操</strong></p><ol><li><p>创建一张大表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table bigtable(id bigint, time bigint, uid string, keyword</span><br><span class="line">string, url_rank int, click_num int, click_url string) row format delimited</span><br><span class="line">fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>加载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/bigtable&#x27; into table</span><br><span class="line"> bigtable;</span><br></pre></td></tr></table></figure></li><li><p>设置5个reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces = <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li><li><p>执行去重id查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(distinct id) from bigtable;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.12 sec   HDFS Read: 120741990 HDFS Write: 7 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 7 seconds 120 msec</span><br><span class="line">OK</span><br><span class="line">c0</span><br><span class="line">100001</span><br><span class="line">Time taken: 23.607 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li><li><p>采用group by去重id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(id) from (select id from bigtable group by id) a;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 17.53 sec   HDFS Read: 120752703 HDFS Write: 580 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.29 sec   HDFS Read: 9409 HDFS Write: 7 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 21 seconds 820 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">100001</span><br><span class="line">Time taken: 50.795 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p></li></ol><h5 id="9-3-6-笛卡尔积"><a href="#9-3-6-笛卡尔积" class="headerlink" title="9.3.6 笛卡尔积"></a>9.3.6 笛卡尔积</h5><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h5 id="9-3-7-行列过滤"><a href="#9-3-7-行列过滤" class="headerlink" title="9.3.7 行列过滤"></a>9.3.7 行列过滤</h5><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p><p><strong>案例实操：</strong></p><ol><li><p>测试先关联两张表，再用where条件过滤</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select o.id from bigtable b</span><br><span class="line">join ori o on o.id = b.id</span><br><span class="line">where o.id &lt;= 10;</span><br></pre></td></tr></table></figure><p>Time taken: 34.406 seconds, Fetched: 100 row(s)</p></li><li><p>通过子查询后，再关联表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select b.id from bigtable b</span><br><span class="line">join (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> ori <span class="keyword">where</span> <span class="keyword">id</span> &lt;= <span class="number">10</span> ) o <span class="keyword">on</span> b.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 30.058 seconds, Fetched: 100 row(s)</p></li></ol><h5 id="9-3-8-动态分区调整"><a href="#9-3-8-动态分区调整" class="headerlink" title="9.3.8 动态分区调整"></a>9.3.8 动态分区调整</h5><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p><ol><li><p>开启动态分区参数设置</p><p>(1)开启动态分区功能（默认true，开启）</p><p><font color="red">hive.exec.dynamic.partition=true</font></p><p>(2)设置为非严格模式（动态分区的模式，默认strict,表示必须指定一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区）</p><p>注：在严格模式下，插入数据必须指定一个分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode&#x3D;nonstrict</span><br></pre></td></tr></table></figure><p>(3)在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure><p>(4)在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。与上面哪个设置成一样大。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode&#x3D;100</span><br></pre></td></tr></table></figure><p>(5)整个MR job中，最大可以创建多少个HDFS文件，默认100000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files&#x3D;100000</span><br></pre></td></tr></table></figure><p>(6)当有空分区生成时，是否抛出异常。一般不需要设置。默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition&#x3D;false</span><br></pre></td></tr></table></figure></li><li><p><strong>案例实操</strong></p><p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p><p>(1)创建目标分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(id int, name string) partitioned</span><br><span class="line">by (location int) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>(2)设置动态分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">hive (default)&gt; insert into table dept_partition partition(location) select deptno, dname, loc from dept; /*最后一个字段loc来的*/</span><br></pre></td></tr></table></figure><p>(3)查看目标分区表的分区情况</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure><p><font color="red">思考：目标分区表是如何匹配到分区字段的？</font></p></li></ol><h5 id="9-3-9-分桶"><a href="#9-3-9-分桶" class="headerlink" title="9.3.9 分桶"></a>9.3.9 分桶</h5><p>详见6.6章。</p><h5 id="9-3-10-分区"><a href="#9-3-10-分区" class="headerlink" title="9.3.10 分区"></a>9.3.10 分区</h5><p>详见4.6章。</p><h4 id="9-4-MR优化"><a href="#9-4-MR优化" class="headerlink" title="9.4 MR优化"></a>9.4 MR优化</h4><h5 id="9-4-1-合理设置Map数"><a href="#9-4-1-合理设置Map数" class="headerlink" title="9.4.1 合理设置Map数"></a>9.4.1 合理设置Map数</h5><ol><li><p>通常情况下，作业会通过input的目录产生一个或者多个map任务</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p></li><li><p>是不是map数越多越好</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p></li><li><p>是不是保证每个map处理接近128m的文件块，就高枕无忧了?</p><p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p></li></ol><h5 id="9-4-1-复杂文件增加map数"><a href="#9-4-1-复杂文件增加map数" class="headerlink" title="9.4.1 复杂文件增加map数"></a>9.4.1 复杂文件增加map数</h5><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p><strong>案例实操</strong></p><ol><li><p>执行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from emp;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br></pre></td></tr></table></figure></li><li><p>设置最大切片值为100个字节</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;</span><br><span class="line">hive (default)&gt; select count(*) from emp;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</span><br></pre></td></tr></table></figure></li></ol><h5 id="9-4-2-小文件进行合并"><a href="#9-4-2-小文件进行合并" class="headerlink" title="9.4.2 小文件进行合并"></a>9.4.2 小文件进行合并</h5><p>（1）在map执行前合并小文件，减少map数：  CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><p>（2）在Map-Reduce的任务结束时合并小文件的设置：</p><p>在map-only任务结束时合并小文件，默认true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.mapfiles &#x3D; true;</span><br></pre></td></tr></table></figure><p>在map-reduce任务结束时合并小文件，默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.mapredfiles &#x3D; true;</span><br></pre></td></tr></table></figure><p>合并文件的大小，默认256M</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.size.per.task &#x3D; 268435456;</span><br></pre></td></tr></table></figure><p>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.smallfiles.avgsize &#x3D; 16777216;</span><br></pre></td></tr></table></figure><h5 id="9-4-3-合理设置Reduce数"><a href="#9-4-3-合理设置Reduce数" class="headerlink" title="9.4.3 合理设置Reduce数"></a>9.4.3 合理设置Reduce数</h5><p>1．调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer&#x3D;256000000</span><br></pre></td></tr></table></figure><p>（2）每个任务最大的reduce数，默认为1009</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.max&#x3D;1009</span><br></pre></td></tr></table></figure><p>（3）计算reducer数的公式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N&#x3D;min(参数2，总输入数据量&#x2F;参数1)</span><br></pre></td></tr></table></figure><p>2．调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces &#x3D; 15;</span><br></pre></td></tr></table></figure><p>3．reduce个数并不是越多越好</p><p>1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：<font color="red">处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</font></p><h4 id="9-5-并行执行"><a href="#9-5-并行执行" class="headerlink" title="9.5 并行执行"></a>9.5 并行执行</h4><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;              //打开任务并行执行</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>;  //同一个sql允许最大并行度，默认为8。</span><br></pre></td></tr></table></figure><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h4 id="9-6-严格模式"><a href="#9-6-严格模式" class="headerlink" title="9.6 严格模式"></a>9.6 严格模式</h4><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p><p>通过设置属性hive.mapred.mode值为默认是非严格模式<font color="red">nonstrict</font> 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The mode in which the Hive operations are being performed. </span><br><span class="line">      In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">        Cartesian Product.</span><br><span class="line">        No partition being picked up for a query.</span><br><span class="line">        Comparing bigints and strings.</span><br><span class="line">        Comparing bigints and doubles.</span><br><span class="line">        Orderby without limit.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li><p>对于分区表，<font color="red">除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。</font>换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p></li><li><p>对于<font color="red">使用了order by语句的查询，要求必须使用limit语句。</font>因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p></li><li><p><font color="red">限制笛卡尔积的查询</font>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p></li></ol><h4 id="9-7-JVM重用"><a href="#9-7-JVM重用" class="headerlink" title="9.7 JVM重用"></a>9.7 JVM重用</h4><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;mapreduce.job.jvm.numtasks&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;10&lt;&#x2F;value&gt;</span><br><span class="line"> &lt;description&gt;How many tasks to run per jvm. If set to -1, there is</span><br><span class="line"> no limit. </span><br><span class="line"> &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h4 id="9-8-推测执行"><a href="#9-8-推测执行" class="headerlink" title="9.8 推测执行"></a>9.8 推测执行</h4><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置，默认是true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>不过hive本身也提供了配置项来控制reduce-side的推测执行：默认是true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h4 id="9-9-压缩"><a href="#9-9-压缩" class="headerlink" title="9.9 压缩"></a>9.9 压缩</h4><p>详见第8章</p><h4 id="9-10-执行计划（Explain）"><a href="#9-10-执行计划（Explain）" class="headerlink" title="9.10 执行计划（Explain）"></a>9.10 执行计划（Explain）</h4><p>1．基本语法</p><p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p><p>2．案例实操</p><p>（1）查看下面这条语句的执行计划</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; explain select * from emp;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure><p>（2）查看详细执行计划</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; explain extended select * from emp;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure><h3 id="第十章-Hive实战之谷粒影音"><a href="#第十章-Hive实战之谷粒影音" class="headerlink" title="第十章 Hive实战之谷粒影音"></a>第十章 Hive实战之谷粒影音</h3><h4 id="10-1-需求描述"><a href="#10-1-需求描述" class="headerlink" title="10.1 需求描述"></a>10.1 需求描述</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><p>–统计视频观看数Top10</p><p>–统计视频类别热度Top10</p><p>–统计视频观看数Top20所属类别以及类别包含的Top20的视频个数</p><p>–统计视频观看数Top50所关联视频的所属类别Rank</p><p>–统计每个类别中的视频热度Top10</p><p>–统计每个类别中视频流量Top10</p><p>–统计上传视频最多的用户Top10以及他们上传的观看次数前20视频</p><p>–统计每个类别视频观看数Top10</p><h4 id="10-2-项目"><a href="#10-2-项目" class="headerlink" title="10.2 项目"></a>10.2 项目</h4><h5 id="10-2-1-数据结构"><a href="#10-2-1-数据结构" class="headerlink" title="10.2.1 数据结构"></a>10.2.1 数据结构</h5><ol><li><p>视频表</p><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>video id</td><td>视频唯一id</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别</td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分</td><td>满分5分</td></tr><tr><td>Ratings</td><td>流量</td><td>视频的流量，整型数字</td></tr><tr><td>conments</td><td>评论数</td><td>一个视频的整数评论数</td></tr><tr><td>related ids</td><td>相关视频id</td><td>相关视频的id，最多20个</td></tr></tbody></table></li><li><p>用户表</p><table><thead><tr><th>字段</th><th>备注</th><th>字段类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table></li></ol><h5 id="10-2-2-ETL原始数据"><a href="#10-2-2-ETL原始数据" class="headerlink" title="10.2.2 ETL原始数据"></a>10.2.2 ETL原始数据</h5>]]></content>
    
    
    <summary type="html">&lt;p&gt;第七章 函数 / 第八章 压缩和存储 &lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive入门</title>
    <link href="http://luo6656.github.io/2020/07/01/BigDataFrame/Hive%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/07/01/BigDataFrame/Hive%E5%85%A5%E9%97%A8/</id>
    <published>2020-06-30T16:00:00.000Z</published>
    <updated>2020-10-27T05:11:12.942Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 / 第二章 Hive安装 / 第三章 Hive数据类型 /<br>第四章 DDL数据定义 / 第五章 DML数据操作 / 第六章 查询</p><a id="more"></a><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="第一章-Hive基本概念"><a href="#第一章-Hive基本概念" class="headerlink" title="第一章 Hive基本概念"></a>第一章 Hive基本概念</h3><h4 id="1-1-什么是Hive-仅仅是一个客户端"><a href="#1-1-什么是Hive-仅仅是一个客户端" class="headerlink" title="1.1 什么是Hive(仅仅是一个客户端)"></a>1.1 什么是Hive(仅仅是一个客户端)</h4><p>​        Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p><p>​        Hive是基于Hadoop的一个数据仓库工具（本身不存储数据），可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p><p>​        <font color="red">本质是：将HQL转化为MapReduce程序</font></p><p><img src="https://i.loli.net/2020/10/27/6FuaSCvWrAP3fkV.png" alt="img"></p><ul><li>​        Hive处理的数据存储在HDFS</li><li>​        Hive分析数据底层的<font color="red">默认实现是MapReducer</font>，也可使用     Spark来作为底层实现。</li><li>​        执行程序运行在Yarn上</li></ul><h4 id="1-2-Hive的优缺点"><a href="#1-2-Hive的优缺点" class="headerlink" title="1.2 Hive的优缺点"></a>1.2 Hive的优缺点</h4><h5 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h5><ul><li>操作接口采用类SQL语法，提供快速开发的能力（简答、容易上手）。</li><li>避免了去写MapReduce，减少了开发人员的学习成本。</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li></ul><h5 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h5><ol><li>Hive的HQL表达能力有限<ul><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长，由于MapReducer数据处理流程的限制，效率更高的算法却无法实现。</li></ul></li><li>Hive的效率比较低<ul><li>Hive自动生成的MapReduce作业，通常情况下不够智能化。</li><li>Hive调优比较困难，粒度较粗</li></ul></li></ol><h4 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h4><p><img src="https://i.loli.net/2020/10/27/paMvQozwPeyS6Vf.png" alt="img"></p><ol><li><p>用户接口：Client</p><p>CLI(command-line interface)、JDBC/ODBC(jdbc访问hive)、WEBUI(浏览器访问hive)</p></li><li><p>元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在的目录等；</p><p><font color="red">默认存储在自带的derby数据库中，推荐使用MySql存储Metastore</font></p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReducer进行计算。</p></li><li><p>驱动器：Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><p><img src="https://i.loli.net/2020/10/27/pESwWXNvCPoy6gk.png" alt="img"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p></li></ol><h4 id="1-4-Hive和数据库比较"><a href="#1-4-Hive和数据库比较" class="headerlink" title="1.4 Hive和数据库比较"></a>1.4 Hive和数据库比较</h4><p>​        由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h5 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h5><p>​        由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h5 id="1-4-2-数据存储位置"><a href="#1-4-2-数据存储位置" class="headerlink" title="1.4.2 数据存储位置"></a>1.4.2 数据存储位置</h5><p>​        Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p><h5 id="1-4-3-数据更新"><a href="#1-4-3-数据更新" class="headerlink" title="1.4.3 数据更新"></a>1.4.3 数据更新</h5><p>​        由于Hive是针对数据仓库应用设计的，而<font color="red">数据仓库的内容是读多写少的。</font>因此，<font color="red">Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</font>而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h5 id="1-4-4-执行"><a href="#1-4-4-执行" class="headerlink" title="1.4.4 执行"></a>1.4.4 执行</h5><p>​        Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。</p><h5 id="1-4-5-执行延迟"><a href="#1-4-5-执行延迟" class="headerlink" title="1.4.5 执行延迟"></a>1.4.5 执行延迟</h5><p>​        Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h5 id="1-4-6-可扩展性"><a href="#1-4-6-可扩展性" class="headerlink" title="1.4.6 可扩展性"></a>1.4.6 可扩展性</h5><p>​        由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle在理论上的扩展能力也只有100台左右。</p><h5 id="1-4-7-数据规模"><a href="#1-4-7-数据规模" class="headerlink" title="1.4.7 数据规模"></a>1.4.7 数据规模</h5><p>​        由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h3 id="第二章-Hive安装"><a href="#第二章-Hive安装" class="headerlink" title="第二章 Hive安装"></a>第二章 Hive安装</h3><h4 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h4><ul><li>Hive官网地址：<a href="http://hive.apache.org/">http://hive.apache.org/</a></li><li>文档查看地址：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li><li>下载地址：<a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></li><li>github地址：<a href="https://github.com/apache/hive">https://github.com/apache/hive</a></li></ul><h4 id="2-2-Hive安装部署"><a href="#2-2-Hive安装部署" class="headerlink" title="2.2 Hive安装部署"></a>2.2 Hive安装部署</h4><h5 id="2-2-1-Hive安装及配置"><a href="#2-2-1-Hive安装及配置" class="headerlink" title="2.2.1 Hive安装及配置"></a>2.2.1 Hive安装及配置</h5><ol><li><p>把apche-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure></li><li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure></li><li><p>配置hive-env.sh文件</p><ol><li><p>配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure></li><li><p>配置HIVE_CONF_DIR路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure></li></ol></li></ol><h5 id="2-2-2-Hadoop集群配置"><a href="#2-2-2-Hadoop集群配置" class="headerlink" title="2.2.2 Hadoop集群配置"></a>2.2.2 Hadoop集群配置</h5><ol><li><p>必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-2-3-Hive基本操作"><a href="#2-2-3-Hive基本操作" class="headerlink" title="2.2.3 Hive基本操作"></a>2.2.3 Hive基本操作</h5><ol><li><p>启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>打开默认数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use default;</span></span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>创建一张表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table student(id int, name string);</span></span><br></pre></td></tr></table></figure></li><li><p>显示数据库中有几张表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>查看表的结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc student;</span></span><br></pre></td></tr></table></figure></li><li><p>向表中插入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert into student values(1000,<span class="string">&quot;ss&quot;</span>);</span></span><br></pre></td></tr></table></figure></li><li><p>查询表中数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from student;</span></span><br></pre></td></tr></table></figure></li><li><p>退出hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> quit;</span></span><br></pre></td></tr></table></figure></li></ol><p>说明：（查看hive在hdfs中的结构）</p><p><font color="red">数据库</font>：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹</p><p> <font color="red"> 表</font>：在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据。</p><h4 id="2-3-将本地文件导入Hive案例"><a href="#2-3-将本地文件导入Hive案例" class="headerlink" title="2.3 将本地文件导入Hive案例"></a>2.3 将本地文件导入Hive案例</h4><h5 id="2-3-1-需求"><a href="#2-3-1-需求" class="headerlink" title="2.3.1 需求"></a>2.3.1 需求</h5><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p><h5 id="2-3-2-数据准备"><a href="#2-3-2-数据准备" class="headerlink" title="2.3.2 数据准备"></a>2.3.2 数据准备</h5><p>在/opt/module/datas这个目录下创建datas</p><ol><li><p>在/opt/module/目录下创建datas</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure></li><li><p>在/opt/module/datas/目录下创建student.txt文件并添加数据</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch student.txt</span><br><span class="line">[atguigu@hadoop102 datas]$ vi student.txt</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意以tab键间隔</font>。</p><h5 id="2-3-3-Hive实际操作"><a href="#2-3-3-Hive实际操作" class="headerlink" title="2.3.3 Hive实际操作"></a>2.3.3 Hive实际操作</h5><ol><li><p>启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>显示数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>使用default数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use default;</span></span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>删除已创建的student表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop table student;</span></span><br></pre></td></tr></table></figure></li><li><p>创建student表，并声明文件分隔符’\t’</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED</span></span><br><span class="line"> BY &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>加载/opt/module/datas/student.txt文件到student数据库表中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> into table student;</span></span><br></pre></td></tr></table></figure></li><li><p>Hive查询结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from student;</span></span><br><span class="line">OK</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-3-4-遇到的问题"><a href="#2-3-4-遇到的问题" class="headerlink" title="2.3.4 遇到的问题"></a>2.3.4 遇到的问题</h5><p><font color="red">再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常</font></p><p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p><p><font color="red">为什么表在/user/hive/warehouse里呢：因为/user/hive/warehouse是创建的default数据库的路径。</font></p><h4 id="2-4-MySql安装"><a href="#2-4-MySql安装" class="headerlink" title="2.4 MySql安装"></a>2.4 MySql安装</h4><h5 id="2-4-1-安装包准备"><a href="#2-4-1-安装包准备" class="headerlink" title="2.4.1 安装包准备"></a>2.4.1 安装包准备</h5><ol><li><p>查看mysql是否安装，如果安装了，卸载mysql</p><ol><li><p>查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -qa|grep mysql</span><br><span class="line"></span><br><span class="line">mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure></li><li><p>卸载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure></li></ol></li><li><p>解压mysql-libs.zip文件到当前目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# unzip mysql-libs.zip</span><br><span class="line">[root@hadoop102 software]# ls</span><br><span class="line">mysql-libs.zip</span><br><span class="line">mysql-libs</span><br></pre></td></tr></table></figure></li><li><p>进入到mysql-libs文件夹下</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# ll</span><br><span class="line"></span><br><span class="line">总用量 76048</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><h5 id="2-4-2-安装MySql服务器"><a href="#2-4-2-安装MySql服务器" class="headerlink" title="2.4.2 安装MySql服务器"></a>2.4.2 安装MySql服务器</h5><ol><li><p>安装mysql服务端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>查看产生的随机密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# cat /root/.mysql_secret</span><br><span class="line"></span><br><span class="line">OEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure></li><li><p>查看mysql状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql status</span><br></pre></td></tr></table></figure></li><li><p>启动mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql start</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-4-3-安装Mysql客户端"><a href="#2-4-3-安装Mysql客户端" class="headerlink" title="2.4.3 安装Mysql客户端"></a>2.4.3 安装Mysql客户端</h5><ol><li><p>安装mysql客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>链接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure></li><li><p>修改密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">SET PASSWORD=PASSWORD(<span class="string">&#x27;000000&#x27;</span>);</span></span><br></pre></td></tr></table></figure></li><li><p>退出mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"><span class="built_in">exit</span></span></span><br></pre></td></tr></table></figure></li></ol><h5 id="2-4-4-MySql中user表中主机配置"><a href="#2-4-4-MySql中user表中主机配置" class="headerlink" title="2.4.4 MySql中user表中主机配置"></a>2.4.4 MySql中user表中主机配置</h5><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。</p><ol><li><p>进入mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -p123456789</span><br></pre></td></tr></table></figure></li><li><p>显示数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>使用mysql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br></pre></td></tr></table></figure></li><li><p>展示mysql数据库中的所有表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show tables;</span><br></pre></td></tr></table></figure></li><li><p>展示user表的结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;desc user;</span><br></pre></td></tr></table></figure></li><li><p>查询user表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;select User, Host, Password from user;</span><br></pre></td></tr></table></figure></li><li><p>修改user表，把host表内容修改为%</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;update user set host&#x3D;&#39;%&#39; where host&#x3D;&#39;localhost&#39;;</span><br></pre></td></tr></table></figure></li><li><p>删除root用户的其他host</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;hadoop102&#39;;</span><br><span class="line"></span><br><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;127.0.0.1&#39;;</span><br><span class="line"></span><br><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;::1&#39;;</span><br></pre></td></tr></table></figure></li><li><p>刷新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure></li><li><p>退出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;quit;</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-5-Hive元数据配置到Mysql"><a href="#2-5-Hive元数据配置到Mysql" class="headerlink" title="2.5 Hive元数据配置到Mysql"></a>2.5 Hive元数据配置到Mysql</h4><h5 id="2-5-1-驱动拷贝"><a href="#2-5-1-驱动拷贝" class="headerlink" title="2.5.1 驱动拷贝"></a>2.5.1 驱动拷贝</h5><ol><li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line"></span><br><span class="line"> /opt/module/hive/lib/</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-5-2-配置Metastore到Mysql"><a href="#2-5-2-配置Metastore到Mysql" class="headerlink" title="2.5.2 配置Metastore到Mysql"></a>2.5.2 配置Metastore到Mysql</h5><ol><li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动Hadoop集群）</p></li></ol><h5 id="2-5-3-多窗口启动Hive测试"><a href="#2-5-3-多窗口启动Hive测试" class="headerlink" title="2.5.3 多窗口启动Hive测试"></a>2.5.3 多窗口启动Hive测试</h5><ol><li><p>先启动MySql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 mysql-libs]$ mysql -uroot -p123456789</span><br></pre></td></tr></table></figure></li><li><p>再次打开多个窗口，分别启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>启动hive后，回到MySql窗口查看数据库，显示<font color="red">增加了metastore数据库</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| metastore          |</span><br><span class="line">| mysql             |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test               |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-6-HiveJDBC访问"><a href="#2-6-HiveJDBC访问" class="headerlink" title="2.6 HiveJDBC访问"></a>2.6 HiveJDBC访问</h4><h5 id="2-6-1-启动hiveserver2服务"><a href="#2-6-1-启动hiveserver2服务" class="headerlink" title="2.6.1 启动hiveserver2服务"></a>2.6.1 启动hiveserver2服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure><h5 id="2-6-2-启动beeline"><a href="#2-6-2-启动beeline" class="headerlink" title="2.6.2 启动beeline"></a>2.6.2 启动beeline</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2-6-3-连接hiveserver2"><a href="#2-6-3-连接hiveserver2" class="headerlink" title="2.6.3 连接hiveserver2"></a>2.6.3 连接hiveserver2</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop102:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://hadoop102:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车）</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop102:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| hive_db2       |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure><h4 id="2-7-Hive常用交互命令"><a href="#2-7-Hive常用交互命令" class="headerlink" title="2.7 Hive常用交互命令"></a>2.7 Hive常用交互命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--help                        Print help information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure><ol><li><p>“e” 不进入hive的交互窗口执行sql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>“f” 执行脚本中的sql语句</p><ol><li><p>在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>文件中写入正确的sql语句</p><p>select * from student;</p></li><li><p>执行文件中的sql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句并将结果写入文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="2-8-Hive其他命令操作"><a href="#2-8-Hive其他命令操作" class="headerlink" title="2.8 Hive其他命令操作"></a>2.8 Hive其他命令操作</h4><ol><li><p>退出hive窗口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls &#x2F;;</span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure></li><li><p>查看在hive中输入的所有历史命令</p><ol><li>进入到当前用户的根目录/root或/home/atguigu</li><li>查看.hivehistory文件</li></ol></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure><h4 id="2-9-Hive常见配置属性"><a href="#2-9-Hive常见配置属性" class="headerlink" title="2.9 Hive常见配置属性"></a>2.9 Hive常见配置属性</h4><h5 id="2-9-1-Hive数据仓库位置的配置"><a href="#2-9-1-Hive数据仓库位置的配置" class="headerlink" title="2.9.1 Hive数据仓库位置的配置"></a>2.9.1 Hive数据仓库位置的配置</h5><ol><li><p>Default数据仓库的最原始位置是在hdfs上的:/user/hive/warehouse路径下。</p></li><li><p><font color="red">在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹</font></p></li><li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-9-2-查询后信息显示的配置"><a href="#2-9-2-查询后信息显示的配置" class="headerlink" title="2.9.2 查询后信息显示的配置"></a>2.9.2 查询后信息显示的配置</h5><ol><li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>重新启动hive，对比配置前后差异。</p><ol><li><p>配置前，如图所示</p><p><img src="https://i.loli.net/2020/10/27/wZdMS2s5pQ4ulaR.png"></p></li><li><p>配置后，如图所示</p></li></ol></li></ol><p><img src="https://i.loli.net/2020/10/27/1c4uR3ZUpnmDJfz.png"></p><h5 id="2-9-3-Hive运行日志信息的配置"><a href="#2-9-3-Hive运行日志信息的配置" class="headerlink" title="2.9.3 Hive运行日志信息的配置"></a>2.9.3 Hive运行日志信息的配置</h5><ol><li><p>Hive的log默认存放在/tmp/atguigu/hive.log目录下（当前用户名下）</p></li><li><p>修改hive的log存放日志到/opt/module/hive/logs</p><ol><li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure></li><li><p>在hive-log4j.properties文件中修改log存放位置</p></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-9-4-参数配置方式"><a href="#2-9-4-参数配置方式" class="headerlink" title="2.9.4 参数配置方式"></a>2.9.4 参数配置方式</h5><ol><li><p>查看当前所有的配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure></li><li><p>参数的配置三种方式</p><p>（1）配置文件方式</p><p>​        默认配置文件：hive-default.xml</p><p>​        用户自定义配置文件：hive-site.xlm</p><p><font color="red">注意</font>：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p><p>（2）命令行参数方式</p><p>​        启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p><p>​        例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意：仅对本次hive启动有效</font></p><p>​        查看参数设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>（3）参数声明方式</p><p>​        可以在HQL中使用SET关键字设定参数</p><p>​        例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks&#x3D;100;</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意：仅对本次hive启动有效</font></p><p>​        查看参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>​        上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p></li></ol><h3 id="第三章-Hive数据类型"><a href="#第三章-Hive数据类型" class="headerlink" title="第三章 Hive数据类型"></a>第三章 Hive数据类型</h3><h4 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h4><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE  FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>​        对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h4 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h4><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()例如struct&lt;street:string, city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()例如array<string></td></tr></tbody></table><p>​        Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>​        <strong>案例实操</strong></p><p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问格式为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据</p><p>​    创建本地测试文件test.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line"></span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意</font>：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p><p>3）Hive上创建测试表test</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table test(</span><br><span class="line">name string,</span><br><span class="line">friends array&lt;string&gt;,</span><br><span class="line">children map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;street:string, city:string&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;</span><br><span class="line">collection items terminated by &#39;_&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><p>字段解释：</p><p>row format delimited fields terminated by ‘,’  – 列分隔符</p><p>collection items terminated by ‘_’  –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’                – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’;                    – 行分隔符</p><p>4）导入文本数据到测试表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘&#x2F;opt&#x2F;module&#x2F;datas&#x2F;test.txt’into table test</span><br></pre></td></tr></table></figure><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from test</span><br><span class="line">where name&#x3D;&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><h4 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h4><p>​        Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><h5 id="3-3-1-隐式类型转换规则如下"><a href="#3-3-1-隐式类型转换规则如下" class="headerlink" title="3.3.1 隐式类型转换规则如下"></a>3.3.1 隐式类型转换规则如下</h5><ol><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li><li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li><li>TINYINT、SMALLINT、INT都可以转换成FLOAT</li><li>BOOLEAN类型不可以转换为任何其他地类型</li></ol><h5 id="3-3-2-可以使用CAST操作显示进行数据类型转换"><a href="#3-3-2-可以使用CAST操作显示进行数据类型转换" class="headerlink" title="3.3.2 可以使用CAST操作显示进行数据类型转换"></a>3.3.2 可以使用CAST操作显示进行数据类型转换</h5><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt; select &#39;1&#39;+2, cast(&#39;1&#39;as int) + 2;</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br><span class="line"></span><br><span class="line">| _c0  | _c1  |</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br><span class="line"></span><br><span class="line">| 3.0  | 3   |</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br></pre></td></tr></table></figure><h3 id="第四章-DDL数据定义"><a href="#第四章-DDL数据定义" class="headerlink" title="第四章 DDL数据定义"></a>第四章 DDL数据定义</h3><p><font color="red">DDL都是操作的元数据</font></p><h4 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><ol><li><p>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure></li><li><p>避免要创建的数据库已经存在的错误，增加if not exists判断(标准写法)</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure></li><li><p>创建一个数据库，指定数据库在HDFS上存放的位置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h4><h5 id="4-2-1-显示数据库"><a href="#4-2-1-显示数据库" class="headerlink" title="4.2.1 显示数据库"></a>4.2.1 显示数据库</h5><ol><li><p>显示数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>过滤显示查询数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like &#39;db_hive*&#39;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h5><ol><li><p>显示数据库信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbatguiguUSER</span><br></pre></td></tr></table></figure></li><li><p>显示数据库详细信息，extended</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbatguiguUSER</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-2-3-切换当前数据库"><a href="#4-2-3-切换当前数据库" class="headerlink" title="4.2.3 切换当前数据库"></a>4.2.3 切换当前数据库</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h4 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h4><p>​        用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。<font color="red">数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</font></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line"></span><br><span class="line">db_name <span class="keyword">comment</span> location     owner_name    owner_type    <span class="keyword">parameters</span></span><br><span class="line"></span><br><span class="line">db_hive     hdfs://hadoop102:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db   atguigu <span class="keyword">USER</span>   &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h4><ol><li><p>删除空数据库</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果删除的数据库不存在，最好采用if exists判断数据库是否存在</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line"></span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line"></span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果数据库不为空，可以采用cascade命令，强制删除</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h4><h5 id="4-5-1-建表语句以及建表参数"><a href="#4-5-1-建表语句以及建表参数" class="headerlink" title="4.5.1 建表语句以及建表参数"></a>4.5.1 建表语句以及建表参数</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">/*注释*/</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]  <span class="comment">/*分区，大部分都是分区表，分的是文件夹*/</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">/*分桶，分的是文件*/</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure><p><strong>字段解释说明</strong> </p><p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p><p>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），<font color="red">在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</font></p><p>（3）COMMENT：为表和列添加注释。</p><p>（4）PARTITIONED BY创建分区表</p><p>（5）CLUSTERED BY创建分桶表</p><p>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</p><p>（7）ROW FORMAT </p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p><p>​    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] </p><p>  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，<font color="red">Hive通过SerDe确定表的具体的列的数据。</font></p><p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p><p>（8）STORED AS指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p><p><font color="red">（9）LOCATION ：指定表在HDFS上的存储位置。</font></p><p><font color="red">（10）AS：后跟查询语句</font>，根据查询结果创建表。</p><p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h5 id="4-5-2-管理表（内部表）"><a href="#4-5-2-管理表（内部表）" class="headerlink" title="4.5.2 管理表（内部表）"></a>4.5.2 管理表（内部表）</h5><ol><li><p>理论</p><p>默认创建的表都是所谓的管理表，有时也被称为<font color="red">内部表</font>。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。    <font color="red">当我们删除一个管理表时，Hive也会删除这个表中数据</font>。管理表不适合和其他工具共享数据。</p></li><li><p>案例操作</p><ol><li><p>普通创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student2&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据查询结果创建表（查询的结果会添加到新创建的表中)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>根据已经存在的表结构创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE  </span><br></pre></td></tr></table></figure></li></ol></li></ol><h5 id="4-5-3-外部表"><a href="#4-5-3-外部表" class="headerlink" title="4.5.3 外部表"></a>4.5.3 外部表</h5><ol><li><p>理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<font color="red">删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</font></p></li><li><p>管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p></li><li><p>案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据</p><ol><li><p>上传数据到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>建表语句（创建外部表）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table stu_external(</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27; </span><br><span class="line">location &#x27;/student&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>查看创建的表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br><span class="line">1003    zhaoliu</span><br></pre></td></tr></table></figure></li><li><p>查看表格式化数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</font></p><p>​    </p></li></ol><h5 id="4-5-4-管理表与外部表的互相转换（注意大小写）"><a href="#4-5-4-管理表与外部表的互相转换（注意大小写）" class="headerlink" title="4.5.4 管理表与外部表的互相转换（注意大小写）"></a>4.5.4 管理表与外部表的互相转换（注意大小写）</h5><ol><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li><li><p>查询内部表student2为外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span>=<span class="string">&#x27;TRUE&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改外部表student2为内部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span>=<span class="string">&#x27;FALSE&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p><font color="red">注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</font></p></li></ol><h4 id="4-6-分区表"><a href="#4-6-分区表" class="headerlink" title="4.6 分区表"></a>4.6 分区表</h4><p>​        分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<font color="red">Hive中的分区就是分目录</font>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h5 id="4-6-1-分区表基本操作"><a href="#4-6-1-分区表基本操作" class="headerlink" title="4.6.1 分区表基本操作"></a>4.6.1 分区表基本操作</h5><ol><li><p>引入分区表（需要根据日期对日志进行管理）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170702&#x2F;20170702.log</span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170703&#x2F;20170703.log</span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170704&#x2F;20170704.log</span><br></pre></td></tr></table></figure></li><li><p>创建分区表语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p></li><li><p>加载数据到分区表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201709&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201708&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201707’);</span><br></pre></td></tr></table></figure><p>注意：分区表加载数据时，必须指定分区</p><p><img src="https://i.loli.net/2020/10/27/W7BVeQ6LygNRiKH.png"></p><p>​                                    加载数据到分区表</p><p><img src="https://i.loli.net/2020/10/27/hmvedFiYBXGtyZn.png"></p><p>​                                           分区表</p></li></ol><ol start="4"><li><p>查询分区表中数据</p><p>（1）单分区查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>（2）多分区联合查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;</span><br><span class="line">              union</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201708&#x27;</span></span><br><span class="line">              <span class="keyword">union</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201707&#x27;</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure></li><li><p>增加分区</p><p>（1）创建单个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201706&#x27;) ;</span><br></pre></td></tr></table></figure><p>（2）同时创建多个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201705&#x27;) partition(month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure></li><li><p>删除分区</p><p>（1）删除单个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>（2）同时删除多个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201705&#x27;), partition (month=&#x27;201706&#x27;);</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>查看分区表有多少分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>查看分区表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string    </span><br></pre></td></tr></table></figure></li></ol><h5 id="4-6-2-分区表注意事项"><a href="#4-6-2-分区表注意事项" class="headerlink" title="4.6.2 分区表注意事项"></a>4.6.2 分区表注意事项</h5><ol><li><p>创建二级分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, dname string, loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>正常的加载数据</p><p>（1）加载数据到二级分区表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> default.dept_partition2 partition(month=&#x27;201709&#x27;, day=&#x27;13&#x27;);</span><br></pre></td></tr></table></figure><p>（2）查询分区数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><p>（1）方式一：上传数据后修复</p><p>​        a.上传数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><p>​        b.查询数据（不能查到，元数据里没有）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>​        c.执行修复命令</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>​        d.再次查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>（2）方式二：上传数据后添加分区</p><p>​        a.上传数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>​        b.执行添加分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month=&#x27;201709&#x27;,</span><br><span class="line"> day=&#x27;11&#x27;);</span><br></pre></td></tr></table></figure><p>​        c.查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;</span><br></pre></td></tr></table></figure><p>（3）方式三：创建文件夹后load数据到分区</p><p>​        a.创建目录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>​        b.上传数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);</span><br></pre></td></tr></table></figure><p>​        c.查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-7-修改表"><a href="#4-7-修改表" class="headerlink" title="4.7 修改表"></a>4.7 修改表</h4><h5 id="4-7-1-重命名表"><a href="#4-7-1-重命名表" class="headerlink" title="4.7.1 重命名表"></a>4.7.1 重命名表</h5><ol><li><p>语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li><li><p>实操案例</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-7-2-增加、修改和删除分区表"><a href="#4-7-2-增加、修改和删除分区表" class="headerlink" title="4.7.2 增加、修改和删除分区表"></a>4.7.2 增加、修改和删除分区表</h5><p><font color="red">详见4.6.1分区表基本操作</font></p><h5 id="4-7-3-增加-修改-替换列信息"><a href="#4-7-3-增加-修改-替换列信息" class="headerlink" title="4.7.3 增加/修改/替换列信息"></a>4.7.3 增加/修改/替换列信息</h5><ol><li><p>语法</p><ol><li><p>更新列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure></li><li><p>增加和替换列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...) </span><br></pre></td></tr></table></figure><p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，<font color="red">REPLACE则是表示替换表中所有字段。</font></p></li></ol></li><li><p>实操案例</p><p>（1）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（2）添加列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure><p>（3）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（4）更新列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure><p>（5）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（6）替换列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure><p>（7）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-8-删除表"><a href="#4-8-删除表" class="headerlink" title="4.8 删除表"></a>4.8 删除表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h3 id="第五章-DML数据操作"><a href="#第五章-DML数据操作" class="headerlink" title="第五章 DML数据操作"></a>第五章 DML数据操作</h3><h4 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h4><h5 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h5><ol><li><p>语法</p><p>hive&gt; load data [local] inpath ‘/opt/module/datas/student.txt’ [overwrite] into table student [partition (partcol1=val1,…)];</p><p>（1）load data:表示加载数据</p><p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p>（3）inpath:表示加载数据的路径</p><p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p><p>（5）into table:表示加载到哪张表</p><p>（6）student:表示具体的表</p><p>（7）partition:表示上传到指定分区</p></li></ol><ol start="2"><li><p>实操案例</p><p>（1）创建一张表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（2）加载本地文件到hive</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（3）加载HDFS文件到hive中</p><p>​        a.上传文件到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;</span><br></pre></td></tr></table></figure><p>​        b.加载HDFS上数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（4）加载数据覆盖表中已有的数据</p><p>​        a.上传文件到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;</span><br></pre></td></tr></table></figure><p>​        b.加载数据覆盖表中已有的数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h5><p>（1）创建一张分区表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（2）基本插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month=&#x27;201709&#x27;) values(1,&#x27;wangwu&#x27;),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure><p>（3）基本模式插入（根据单张表查询结果）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month=&#x27;201708&#x27;)</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure><p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p><p>insert overwrite：会覆盖表或分区中已存在的数据</p><p>（4）多表（多分区）插入模式（根据多张表查询结果）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">&#x27;201707&#x27;</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">&#x27;201706&#x27;</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="5-1-3-查询语句中创建表并加载数据（as-select）"><a href="#5-1-3-查询语句中创建表并加载数据（as-select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（as select）"></a>5.1.3 查询语句中创建表并加载数据（as select）</h5><p>​    <font color="red">详见4.5.1 章创建表</font></p><p>​    根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h5 id="5-1-4-创建表时通过location指定加载数据路径"><a href="#5-1-4-创建表时通过location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过location指定加载数据路径"></a>5.1.4 创建表时通过location指定加载数据路径</h5><ol><li><p>上传数据到hdfs上</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>创建表，并指定载hdfs上的位置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">              location &#x27;/student;</span><br></pre></td></tr></table></figure></li><li><p>查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-1-5-import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）"><a href="#5-1-5-import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）" class="headerlink" title="5.1.5 import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）"></a>5.1.5 import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）</h5><p>​        <font color="red">注意：先用export导出后，再将数据导入</font></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month=&#x27;201709&#x27;) from</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><h4 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h4><h5 id="5-2-1-insert导出"><a href="#5-2-1-insert导出" class="headerlink" title="5.2.1 insert导出"></a>5.2.1 insert导出</h5><ol><li><p>将查询的结果导出到本地</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;</span><br><span class="line">            <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果格式化导出到本地</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;</span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果导出到HDFS上（没有local）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/atguigu/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-2-2-Hadoop命令导出到本地"><a href="#5-2-2-Hadoop命令导出到本地" class="headerlink" title="5.2.2 Hadoop命令导出到本地"></a>5.2.2 Hadoop命令导出到本地</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h5 id="5-2-3-Hive-Shell命令导出"><a href="#5-2-3-Hive-Shell命令导出" class="headerlink" title="5.2.3 Hive Shell命令导出"></a>5.2.3 Hive Shell命令导出</h5><p>​        基本语法：（hive -f/-e  执行语句或者脚本 &gt;file）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h5 id="5-2-4-export导出到HDFS上"><a href="#5-2-4-export导出到HDFS上" class="headerlink" title="5.2.4 export导出到HDFS上"></a>5.2.4 export导出到HDFS上</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><p>​        export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h5 id="5-2-5-sqoop导出"><a href="#5-2-5-sqoop导出" class="headerlink" title="5.2.5 sqoop导出"></a>5.2.5 sqoop导出</h5><p>​        后续课程专门讲</p><h4 id="5-3-清除表中数据（truncate"><a href="#5-3-清除表中数据（truncate" class="headerlink" title="5.3 清除表中数据（truncate)"></a>5.3 清除表中数据（truncate)</h4><p>​        <font color="red">注意：truncate只能删除管理表，不能删除外部表中数据</font></p><h3 id="第六章-查询"><a href="#第六章-查询" class="headerlink" title="第六章 查询"></a>第六章 查询</h3><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><p>查询顺序：from–&gt; join on–&gt; where–&gt; group by–&gt; select|having –&gt; order by–&gt; limit</p><p>查询语句语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*]    (Note: <span class="keyword">Only</span> available</span><br><span class="line"> <span class="keyword">starting</span> <span class="keyword">with</span> Hive <span class="number">0.13</span><span class="number">.0</span>)</span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">    | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><h4 id="6-1-基本查询"><a href="#6-1-基本查询" class="headerlink" title="6.1 基本查询"></a>6.1 基本查询</h4><h5 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h5><p>​    创建部门表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>​    创建员工表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>​    导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line">dept;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table emp;</span><br></pre></td></tr></table></figure><ol><li><p>全表查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br></pre></td></tr></table></figure></li><li><p>选择特定列查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, ename from emp;</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意</font>：（1）SQL语言大小写不敏感</p><p>​            （2）SQL可以写在一行或者多行</p><p>​            （3）关键字不能被缩写也不能分行</p><p>​            （4）各子句一般要分行写</p><p>​            （5）使用缩进提高语句的可读性</p><h5 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h5><ol><li><p>重命名一个列</p></li><li><p>便于计算</p></li><li><p>紧跟列名，<font color="red">也可以在列名和别名之间加入关键字 as</font></p></li><li><p>案例实操</p><p>查询名称和部门</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h5><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B 相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B 相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A|B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table><p>​    案例实操</p><p>​        查询出所有员工的薪水后加1显示</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sal +1 from emp;</span><br></pre></td></tr></table></figure><h5 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h5><ol><li><p>求总行数（count）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的最大值（max）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的最小值（min）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的总和（sum）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure></li><li><p>求工资的平均值（avg）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-1-5-limit语句"><a href="#6-1-5-limit语句" class="headerlink" title="6.1.5 limit语句"></a>6.1.5 limit语句</h5><p>​        典型的查询会返回多行数据。limit子句用于限制返回的行数。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br></pre></td></tr></table></figure><h4 id="6-2-where语句"><a href="#6-2-where语句" class="headerlink" title="6.2 where语句"></a>6.2 where语句</h4><ol><li><p>使用where子句，将不满足条件的行过滤掉</p></li><li><p>where子句紧跟from子句</p></li><li><p>案例实操</p><p>查询出薪水大于1000的所有员工</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000;</span><br></pre></td></tr></table></figure><p><font color="red">注意：where子句中不能使用字段别名</font></p></li></ol><h5 id="6-2-1-比较运算符（between-in-is-null）"><a href="#6-2-1-比较运算符（between-in-is-null）" class="headerlink" title="6.2.1 比较运算符（between/in/is null）"></a>6.2.1 比较运算符（between/in/is null）</h5><p>(1) 下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中.</p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><p>(2) <strong>案例实操</strong></p><ol><li><p>查询出薪水等于5000的所有员工</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal=<span class="number">5000</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询工资在500到1000的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">between</span> <span class="number">500</span> <span class="keyword">and</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询comm为空的所有员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询工资是1500或5000的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">in</span> (<span class="number">1500</span>,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-2-2-like和rlike"><a href="#6-2-2-like和rlike" class="headerlink" title="6.2.2 like和rlike"></a>6.2.2 like和rlike</h5><ol><li><p>使用like运算选择类似的值</p></li><li><p>选择条件可以包含字符或数字</p></li><li><p>rlike子句是Hive中这个功能的一个扩展,其可以通过<font color="red">Java的正则表达式</font>这个更强大的语言来指定匹配条件</p></li><li><p><strong>案例实操</strong></p><p>(1) 查找以2开头薪水的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">like</span> <span class="string">&#x27;2%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>(2) 查找第二个数值为2的薪水的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">like</span> <span class="string">&#x27;_2%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>(3) 查找薪水中含有2的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">rlike</span> <span class="string">&#x27;[2]&#x27;</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-2-3-逻辑运算符（and-or-not）"><a href="#6-2-3-逻辑运算符（and-or-not）" class="headerlink" title="6.2.3 逻辑运算符（and/or/not）"></a>6.2.3 逻辑运算符（and/or/not）</h5><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><p>​    <strong>案例实操</strong></p><ol><li><p>查询薪水大于1000，部门是30</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">and</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询薪水大于1000，或者部门是304</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">or</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询除了20部门和30部门以外的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">in</span>(<span class="number">30</span>,<span class="number">200</span>);</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-3-分组"><a href="#6-3-分组" class="headerlink" title="6.3 分组"></a>6.3 分组</h4><h5 id="6-3-1-group-by语句"><a href="#6-3-1-group-by语句" class="headerlink" title="6.3.1 group by语句"></a>6.3.1 group by语句</h5><p>​        group by语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p><p>​    <strong>案例实操</strong></p><ol><li><p>计算emp表每个部门的平局工资</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno,<span class="keyword">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure></li><li><p>计算emp每个部门中每个岗位的最高薪水</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno,t.job,<span class="keyword">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno,t.job;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-3-2-Having语句"><a href="#6-3-2-Having语句" class="headerlink" title="6.3.2 Having语句"></a>6.3.2 Having语句</h5><ol><li><p>having与where不同点</p><p>（1）where后面不能写分组函数，而having后面可以使用分组函数。</p><p>（2）having只用于group by分组统计语句</p></li><li><p><strong>案例实操</strong></p><p>求每个部门的平均薪水大于2000的部门</p><p>（1）求每个部门的平均工资</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> deptno,<span class="keyword">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p>（2）求每个部门的平均薪水大于2000的部门</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> deptno,<span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal&gt;<span class="number">2000</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-4-join语句"><a href="#6-4-join语句" class="headerlink" title="6.4 join语句"></a>6.4 join语句</h4><h5 id="6-4-1-等值join"><a href="#6-4-1-等值join" class="headerlink" title="6.4.1 等值join"></a>6.4.1 等值join</h5><p>​    Hive支持通常的SQL join语句，但是<font color="red">只支持等值连接，不支持非等值连接</font></p><p>​    <strong>案例实操</strong></p><p>​    根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-2-表的别名"><a href="#6-4-2-表的别名" class="headerlink" title="6.4.2 表的别名"></a>6.4.2 表的别名</h5><ol><li><p>好处</p><p>（1）使用别名可以简化查询</p><p>（2）使用表名前缀可以提高执行效率</p></li><li><p>案例实操</p><p>合并员工表和部门表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-4-3-内连接"><a href="#6-4-3-内连接" class="headerlink" title="6.4.3 内连接"></a>6.4.3 内连接</h5><p>​    <font color="red">内连接</font>：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-4-左外连接"><a href="#6-4-4-左外连接" class="headerlink" title="6.4.4 左外连接"></a>6.4.4 左外连接</h5><p>​    <font color="red">左外连接</font>：join操作符左边表中符合where子句的所有记录将会被返回</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-5-右外连接"><a href="#6-4-5-右外连接" class="headerlink" title="6.4.5 右外连接"></a>6.4.5 右外连接</h5><p>​    <font color="red">右外连接</font>：join操作符右边表中符合where子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-6-满外连接"><a href="#6-4-6-满外连接" class="headerlink" title="6.4.6 满外连接"></a>6.4.6 满外连接</h5><p>​    <font color="red">满外连接</font>：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值得话，那么就使用null值替代。</p><h5 id="6-4-7-多表连接"><a href="#6-4-7-多表连接" class="headerlink" title="6.4.7 多表连接"></a>6.4.7 多表连接</h5><p>​    <font color="red">注意：连接n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</font></p><p>​    <strong>案例实操</strong></p><ol><li><p>​    创建位置表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> location(</span><br><span class="line">loc <span class="built_in">int</span>,</span><br><span class="line">loc_name <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>​    导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span><br></pre></td></tr></table></figure></li><li><p>​    多表连接查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename, d.dname, l.loc_name <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> d.deptno = e.deptno <span class="keyword">join</span> location l <span class="keyword">on</span> d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>​        大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p><p>​        注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</p><p>​        <font color="red">优化：当对3个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。</font></p></li></ol><h5 id="6-4-8-笛卡尔积（少用）"><a href="#6-4-8-笛卡尔积（少用）" class="headerlink" title="6.4.8 笛卡尔积（少用）"></a>6.4.8 笛卡尔积（少用）</h5><ol><li><p>笛卡尔积会在下面条件下产生</p><p>（1）省略连接条件</p><p>（2）连接条件无效</p><p>（3）所有表中得所有行互相连接</p></li><li><p>案例实操</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> empno,dname <span class="keyword">from</span> emp,dept;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-4-9-连接谓词中不支持or"><a href="#6-4-9-连接谓词中不支持or" class="headerlink" title="6.4.9 连接谓词中不支持or"></a>6.4.9 连接谓词中不支持or</h5><p>​        hive join目前不支持在on子句中使用谓词or</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno <span class="keyword">or</span> e.ename=d.ename;   错误的</span><br></pre></td></tr></table></figure><h4 id="6-5-排序"><a href="#6-5-排序" class="headerlink" title="6.5 排序"></a>6.5 排序</h4><h5 id="6-5-1-全局排序"><a href="#6-5-1-全局排序" class="headerlink" title="6.5.1 全局排序"></a>6.5.1 全局排序</h5><p><font color="red">order by：全局排序，只有一个reducer</font></p><ol><li><p>使用order by子句排序</p><p><font color="red">（1）asc（ascend）：升序（默认）</font></p><p><font color="red">（2）desc（descend）：降序</font></p></li><li><p>order by子句在select语句的结尾</p></li><li><p><strong>案例实操</strong></p><p>（1）查询员工信息按工资升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span><br></pre></td></tr></table></figure><p>（2）查询员工信息按工资降序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-5-2-按照别名排序"><a href="#6-5-2-按照别名排序" class="headerlink" title="6.5.2 按照别名排序"></a>6.5.2 按照别名排序</h5><p><strong>案例实操</strong></p><p>按照员工薪水的2倍排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename, sal*<span class="number">2</span> doublesal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> doublesal;</span><br></pre></td></tr></table></figure><h5 id="6-5-3-多个列排序"><a href="#6-5-3-多个列排序" class="headerlink" title="6.5.3 多个列排序"></a>6.5.3 多个列排序</h5><p><strong>案例实操</strong></p><p>按照部门和工资升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno,dal;</span><br></pre></td></tr></table></figure><h5 id="6-5-4-每个MapReduce内部排序（sort-by）"><a href="#6-5-4-每个MapReduce内部排序（sort-by）" class="headerlink" title="6.5.4 每个MapReduce内部排序（sort by）"></a>6.5.4 每个MapReduce内部排序（sort by）</h5><p>​        Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p><p>​        Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure></li><li><p>查看设置reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure></li><li><p>根据部门编号降序查看员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件夹中（按照部门编号降序排序）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">&#x27;/opt/module/datas/sortby-result&#x27;</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-5-5-分区排序（distribute-by）"><a href="#6-5-5-分区排序（distribute-by）" class="headerlink" title="6.5.5 分区排序（distribute by）"></a>6.5.5 分区排序（distribute by）</h5><p>​        Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。*<strong>*distribute by**</strong> 子句可以做这件事。*<strong>*distribute by**</strong>类似MR中partition（自定义分区），进行分区，结合sort by使用。 </p><p>​        对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p><strong>案例实操</strong></p><p>先按照部门编号分区，再按照员工编号降序排序</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>distrubute by 的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li><li><font color="red">Hive要求distribute by语句要写在sort by语句之前</font></li></ol><h5 id="6-5-6-cluster-by"><a href="#6-5-6-cluster-by" class="headerlink" title="6.5.6 cluster by"></a>6.5.6 cluster by</h5><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>1）以下两种写法等价</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p><h5 id="6-5-6-几个排序函数的区别（重要）"><a href="#6-5-6-几个排序函数的区别（重要）" class="headerlink" title="6.5.6 几个排序函数的区别（重要）"></a>6.5.6 几个排序函数的区别（重要）</h5><ol><li>order by：全局排序，reduce个数为1，设置多个也没用</li><li>sort by：区内排序，通常结合distribute by使用，reduce多个，会产生数据倾斜</li><li>cluster by：当分区字段和排序字段相同的时候，可以使用cluster by代替。</li></ol><h4 id="6-6-分桶及抽样查询"><a href="#6-6-分桶及抽样查询" class="headerlink" title="6.6 分桶及抽样查询"></a>6.6 分桶及抽样查询</h4><h5 id="6-6-1-分桶表数据存储"><a href="#6-6-1-分桶表数据存储" class="headerlink" title="6.6.1 分桶表数据存储"></a>6.6.1 分桶表数据存储</h5><p>​        1. 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p><p>​        2. 分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p><p>​        3. <font color="red">分区针对的是数据的存储路径；分桶针对的是数据文件。</font></p><p><strong>案例实操</strong></p><ol><li><p>先创建分桶表，通过直接导入数据文件的方式</p><p>（1）数据准备</p><p>​        准备student.txt文件</p><p>（2）创建分桶表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）查看表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted stu_buck;</span><br><span class="line">Num Buckets:4</span><br></pre></td></tr></table></figure><p>（4）导入数据到分桶表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br></pre></td></tr></table></figure><p>（5）查看创建的分桶表中是否分成4个桶，如下图</p><p>​        发现并未分桶，那是什么原因呢（load相当于是put，所以不会分桶）</p></li></ol><ol start="2"><li><p>创建分桶表时，数据通过子查询的方式导入</p><p>（1）先创建一个普通的stu表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（2）先普通的stu表中导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><p>（3）清空stu_buck表中数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck;</span><br></pre></td></tr></table></figure><p>（4）导入数据到分桶表，通过子查询的方式（走MR）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>（5）发现还是只有一个分桶</p><p>（6）需要设置一个属性</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>; <span class="comment">/*开启分桶功能*/</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">-1</span>; <span class="comment">/*-1表示reduce会根据桶的个数分配reduce个数*/</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/jurRfc7bzdG6IiP.png"></p><p>（7）查询分桶的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_buck;</span><br><span class="line">OK</span><br><span class="line">stu_buck.id     stu_buck.name</span><br><span class="line">1004    ss4</span><br><span class="line">1008    ss8</span><br><span class="line">1012    ss12</span><br><span class="line">1016    ss16</span><br><span class="line">1001    ss1</span><br><span class="line">1005    ss5</span><br><span class="line">1009    ss9</span><br><span class="line">1013    ss13</span><br><span class="line">1002    ss2</span><br><span class="line">1006    ss6</span><br><span class="line">1010    ss10</span><br><span class="line">1014    ss14</span><br><span class="line">1003    ss3</span><br><span class="line">1007    ss7</span><br><span class="line">1011    ss11</span><br><span class="line">1015    ss15</span><br></pre></td></tr></table></figure><p><font color="red">分桶规则：</font></p><p>根据结果可知：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p></li></ol><h5 id="6-6-2-分桶抽样查询"><a href="#6-6-2-分桶抽样查询" class="headerlink" title="6.6.2 分桶抽样查询"></a>6.6.2 分桶抽样查询</h5><p>​        对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p><p>​        查询表stu_buck中的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p>​        注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>​        y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>​        <font color="red">x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。</font>例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p>​        <font color="red">注意：x的值必须小于等于y的值，否则</font></p><p>​        FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p><h4 id="6-7-其他常用查询函数"><a href="#6-7-其他常用查询函数" class="headerlink" title="6.7 其他常用查询函数"></a>6.7 其他常用查询函数</h4><h5 id="6-7-1-空字段赋值"><a href="#6-7-1-空字段赋值" class="headerlink" title="6.7.1 空字段赋值"></a>6.7.1 空字段赋值</h5><ol><li><p>函数说明</p><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。</p></li><li><p>数据准备：采用员工表</p></li><li><p>查询：如果员工的comm为NULL，则用-1代替</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive(default) &gt; select comm,nvl(comm, -1) from emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    -1.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br></pre></td></tr></table></figure></li><li><p>查询：如果员工的comm为NULL，则用领导id代替</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> comm, nvl(comm, mgr) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    7902.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    NULL</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    7788.0</span><br><span class="line">NULL    7698.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    7782.0</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-2-case-when"><a href="#6-7-2-case-when" class="headerlink" title="6.7.2 case when"></a>6.7.2 case when</h5><ol><li><p>数据准备</p><table><thead><tr><th>name</th><th>dept_id</th><th>sex</th></tr></thead><tbody><tr><td>悟空</td><td>A</td><td>男</td></tr><tr><td>大海</td><td>A</td><td>男</td></tr><tr><td>宋宋</td><td>B</td><td>男</td></tr><tr><td>凤姐</td><td>A</td><td>女</td></tr><tr><td>婷姐</td><td>B</td><td>女</td></tr><tr><td>婷婷</td><td>B</td><td>女</td></tr></tbody></table></li><li><p>需求</p><p>求出不同部门男女各多少人，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A     2       1</span><br><span class="line">B     1       2</span><br></pre></td></tr></table></figure></li><li><p>创建本地emp_sex.txt，导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi emp_sex.txt</span><br><span class="line">悟空A男</span><br><span class="line">大海A男</span><br><span class="line">宋宋B男</span><br><span class="line">凤姐A女</span><br><span class="line">婷姐B女</span><br><span class="line">婷婷B女</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">dept_id <span class="keyword">string</span>, </span><br><span class="line">sex <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp_sex.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> dept_id,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;女&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span> emp_sex <span class="keyword">group</span> <span class="keyword">by</span> dept_id;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-3-行转列（对多个列进行合并）"><a href="#6-7-3-行转列（对多个列进行合并）" class="headerlink" title="6.7.3 行转列（对多个列进行合并）"></a>6.7.3 行转列（对多个列进行合并）</h5><ol><li><p>相关函数说明</p><p>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p></li><li><p>数据准备</p><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table></li><li><p>需求</p><p>把星座和血型一样的人归类到一起。结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure></li><li><p>创建本地constellation.txt，导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi constellation.txt</span><br><span class="line">孙悟空白羊座A</span><br><span class="line">大海     射手座A</span><br><span class="line">宋宋     白羊座B</span><br><span class="line">猪八戒    白羊座A</span><br><span class="line">凤姐     射手座A</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">constellation <span class="keyword">string</span>, </span><br><span class="line">blood_type <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/constellation.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">&#x27;|&#x27;</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">name</span>,</span><br><span class="line">        <span class="keyword">concat</span>(constellation, <span class="string">&quot;,&quot;</span>, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-4-列转行（把列拆分）"><a href="#6-7-4-列转行（把列拆分）" class="headerlink" title="6.7.4 列转行（把列拆分）"></a>6.7.4 列转行（把列拆分）</h5><ol><li><p>函数说明</p><p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p></li><li><p>数据准备</p><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table></li><li><p>需求</p><p>将电影分类中的数组数据展开，结果如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure></li><li><p>创建本地movie.txt，导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi movie.txt</span><br><span class="line">《疑犯追踪》悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》战争,动作,灾难</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">    movie <span class="keyword">string</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;,&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/movie.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-5-窗口函数（开窗函数）"><a href="#6-7-5-窗口函数（开窗函数）" class="headerlink" title="6.7.5 窗口函数（开窗函数）"></a>6.7.5 窗口函数（开窗函数）</h5><ol><li><p>相关函数说明</p><p>（1）OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。</p><p>（2）CURRENT ROW：当前行</p><p>（3）n PRECEDING：往前n行数据</p><p>（4）n FOLLOWING：往后n行数据</p><p>（5）UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>（6）LAG(col,n,default_val)：往前第n行数据</p><p>（7）LEAD(col,n, default_val)：往后第n行数据</p><p>（8）NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<font color="red">注意：n必须为int类型。</font></p></li><li><p>数据准备：name，orderdate，cost</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure></li><li><p>需求</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><p>（2）查询顾客的购买明细及月购买总额</p><p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p><p>（4）查询每个顾客上次的购买时间</p><p>（5）查询前20%时间的订单信息</p></li><li><p>创建本地business.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi business.txt</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">orderdate <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">cost</span> <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/business.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">count</span>(*) <span class="keyword">over</span> () </span><br><span class="line"><span class="keyword">from</span> business </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">&#x27;2017-04&#x27;</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>（2）查询顾客的购买明细及月购买总额</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,<span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) <span class="keyword">from</span></span><br><span class="line"> business;</span><br></pre></td></tr></table></figure><p>（3）上述的场景，将每个顾客的cost按照日期进行累加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行 </span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>rows必须跟在order by子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量。</p><p>（4）查看顾客上次的购买时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line">lag(orderdate,<span class="number">1</span>,<span class="string">&#x27;1900-01-01&#x27;</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1, lag(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2 </span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>（5）查询前20%时间的订单信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> sorted = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-6-rank"><a href="#6-7-6-rank" class="headerlink" title="6.7.6 rank"></a>6.7.6 rank</h5><ol><li><p>函数说明</p><p>（1）RANK() 排序相同时会重复，总数不会变</p><p>（2）DENSE_RANK() 排序相同时会重复，总数会减少</p><p>（3）ROW_NUMBER() 会根据顺序计算</p></li><li><p>数据准备</p><table><thead><tr><th>name</th><th>subject</th><th>score</th></tr></thead><tbody><tr><td>孙悟空</td><td>语文</td><td>87</td></tr><tr><td>孙悟空</td><td>数学</td><td>95</td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td></tr><tr><td>大海</td><td>语文</td><td>94</td></tr><tr><td>大海</td><td>数学</td><td>56</td></tr><tr><td>大海</td><td>英语</td><td>84</td></tr><tr><td>宋宋</td><td>语文</td><td>64</td></tr><tr><td>宋宋</td><td>数学</td><td>86</td></tr><tr><td>宋宋</td><td>英语</td><td>84</td></tr><tr><td>婷婷</td><td>语文</td><td>65</td></tr><tr><td>婷婷</td><td>数学</td><td>85</td></tr><tr><td>婷婷</td><td>英语</td><td>78</td></tr></tbody></table></li><li><p>需求</p><p>计算每门学科成绩排名</p></li><li><p>创建本地score.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi score.txt</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subject <span class="keyword">string</span>, </span><br><span class="line">score <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/score.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,</span><br><span class="line">subject,</span><br><span class="line">score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rp,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) drp,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rmp</span><br><span class="line"><span class="keyword">from</span> score;</span><br><span class="line"></span><br><span class="line">name    subject score   rp      drp     rmp</span><br><span class="line">孙悟空  数学    95      1       1       1</span><br><span class="line">宋宋    数学    86      2       2       2</span><br><span class="line">婷婷    数学    85      3       3       3</span><br><span class="line">大海    数学    56      4       4       4</span><br><span class="line">宋宋    英语    84      1       1       1</span><br><span class="line">大海    英语    84      1       1       2</span><br><span class="line">婷婷    英语    78      3       2       3</span><br><span class="line">孙悟空  英语    68      4       3       4</span><br><span class="line">大海    语文    94      1       1       1</span><br><span class="line">孙悟空  语文    87      2       2       2</span><br><span class="line">婷婷    语文    65      3       3       3</span><br><span class="line">宋宋    语文    64      4       4       4</span><br></pre></td></tr></table></figure><p><font color="red">扩展：求出每门学科前三名的学生</font></p></li></ol><h5 id="6-7-7-时间类"><a href="#6-7-7-时间类" class="headerlink" title="6.7.7 时间类"></a>6.7.7 时间类</h5><ol><li><p>date_format:格式化时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">&#x27;1987-5-23&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>date_add:时间跟天数相加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select date_add(&#x27;2019-06-05&#x27;,5);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">2019-06-10</span><br></pre></td></tr></table></figure></li><li><p>date_sub:时间跟天数相减</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select date_sub(&#x27;2019-06-05&#x27;,5);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">2019-05-31</span><br><span class="line">Time taken: 0.343 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li><li><p>datediff:两个时间相减</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select datediff(&#x27;2019-06-29&#x27;,&#x27;2019-07-05&#x27;);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">-6</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 / 第二章 Hive安装 / 第三章 Hive数据类型 /&lt;br&gt;第四章 DDL数据定义 / 第五章 DML数据操作 / 第六章 查询&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka入门</title>
    <link href="http://luo6656.github.io/2020/06/25/BigDataFrame/Kafka%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/06/25/BigDataFrame/Kafka%E5%85%A5%E9%97%A8/</id>
    <published>2020-06-24T16:00:00.000Z</published>
    <updated>2020-10-27T05:11:00.271Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 概述 / 第二章 快速入门 / 第三章 Kafka架构深入 / 第四章 Kafka API /</p><a id="more"></a><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h3 id="第一章-Kafka概述"><a href="#第一章-Kafka概述" class="headerlink" title="第一章 Kafka概述"></a>第一章 Kafka概述</h3><h4 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h4><p>​    Kafka是一个分布式的基于<font color="red">发布/订阅模式</font>的消息队列，主要应用于大数据实时处理领域。</p><h4 id="1-2-消息队列（Message-Queue）"><a href="#1-2-消息队列（Message-Queue）" class="headerlink" title="1.2 消息队列（Message Queue）"></a>1.2 消息队列（Message Queue）</h4><h5 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h5><p><img src="https://i.loli.net/2020/10/27/jGKtykBrihl8VCg.png"></p><p><img src="https://i.loli.net/2020/10/27/rYhFNLJfH8bjtD3.png"></p><p><strong>消息队列的优点</strong></p><p>（1）解耦</p><p>（2）可恢复性</p><p>（3）缓冲</p><p>（4）灵活性&amp;峰值处理能力</p><p>（5）异步通信</p><h5 id="1-2-2-消息队列的模式"><a href="#1-2-2-消息队列的模式" class="headerlink" title="1.2.2 消息队列的模式"></a>1.2.2 消息队列的模式</h5><ol><li><p>点对点模式（<font color="red">一对一</font>，消费者主动拉取数据，消息收到后消息清除）</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。</p><p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p><p><img src="https://i.loli.net/2020/10/27/wR8EXKGuqZWr9fV.png"></p></li></ol><ol start="2"><li><p>发布/订阅模式（<font color="red">一对多</font>，消费者消费数据之后不会清除消息）</p><p>消息生产者（发布）将消息发布到topic中，同时有多个消息消9费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。kafka是消费者主动拉取模式的，需要一直轮询，看topic中是否有新消息</p><p><img src="https://i.loli.net/2020/10/27/D7rPpGY1o9FjUQW.png"></p></li></ol><h4 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h4><p><img src="https://i.loli.net/2020/10/27/6nHYkWQar4Fx17q.png"></p><p><img src="https://i.loli.net/2020/10/27/CxTMcGgbJZQXtUe.png"></p><p>Producer：消息生产者，就是向kafka broker发消息的客户端；</p><p>Consumer：消息消费者，向kafka broker取出消息的客户端</p><p>Consumer Group（CG）：消费者组，由多个consumer组成。<font color="red">消费者组内每个消费者负责消费不同区分的数据，一个分区只能由一个组里面的一个消费者消费；消费者组之间互不影响。</font>所有的消费者都属于某个消费者组，即<font color="red">消费者组是逻辑上的一个订阅者</font> 。同一个消费者组里面的消费者不能同时消费相同的分区数据，其他的消费者组可以消费相同分区。消费者组就是为了提高消费能力。消费者组里的消费者个数大于分区数，回浪费资源。并发度最好就是消费者组的消费者个数=分区数。</p><p>Broker：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p><p>Topic：可以理解为一个队列，<font color="red">生产者和消费者面向的都是一个topic</font></p><p>Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，<font color="red">一个topic可以分为多个partition</font>，每个partition是一个有序的队列。</p><p>Replica：副本，为保证集群中的某个节点发生故障，<font color="red">该节点上的partition数据不丢失，且kafka仍能够继续工作，</font>kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p><p>Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p><p>Follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower（应该是leader吧）。</p><p>Zookeeper：管理Kafka集群，保存消费信息（offset偏移量）0.9之前。0.9版本之后为了效率就不保存在zk上了保存在磁盘上</p><h3 id="第二章-Kafka快速入门"><a href="#第二章-Kafka快速入门" class="headerlink" title="第二章 Kafka快速入门"></a>第二章 Kafka快速入门</h3><h4 id="2-1-安装部署"><a href="#2-1-安装部署" class="headerlink" title="2.1 安装部署"></a>2.1 安装部署</h4><h5 id="2-1-1-集群规划"><a href="#2-1-1-集群规划" class="headerlink" title="2.1.1 集群规划"></a>2.1.1 集群规划</h5><table><thead><tr><th>hadoop103</th><th>hadoop104</th><th>hadoop105</th></tr></thead><tbody><tr><td>zk</td><td>zk</td><td>zk</td></tr><tr><td>kafka</td><td>kafka</td><td>kafka</td></tr></tbody></table><h5 id="2-1-2-jar包下载"><a href="#2-1-2-jar包下载" class="headerlink" title="2.1.2 jar包下载"></a>2.1.2 jar包下载</h5><blockquote><p><a href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></p></blockquote><p><img src="https://i.loli.net/2020/10/27/JDKgMhUoRVQyNGp.png"></p><h5 id="2-1-3-集群部署"><a href="#2-1-3-集群部署" class="headerlink" title="2.1.3 集群部署"></a>2.1.3 集群部署</h5><ol><li><p>解压安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改解压后的文件名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv kafka_2.11-0.11.0.0/ kafka</span><br></pre></td></tr></table></figure></li><li><p>在/opt/module/kafka目录下创建logs文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure></li><li><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ cd config/</span><br><span class="line">[atguigu@hadoop102 config]$ vi server.properties</span><br></pre></td></tr></table></figure><p>输入以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta">#</span><span class="bash">设置可以删除topic</span></span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span><span class="bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta">#</span><span class="bash">用来处理磁盘IO的现成数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta">#</span><span class="bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta">#</span><span class="bash">kafka运行日志存放的路径,实际就是消息数据</span></span><br><span class="line">log.dirs=/opt/module/kafka/logs</span><br><span class="line"><span class="meta">#</span><span class="bash">topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta">#</span><span class="bash">用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta">#</span><span class="bash">segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta">#</span><span class="bash">配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=hadoop103:2181,hadoop104:2181,hadoop105:2181</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ sudo vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 module]$ source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>分发安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync kafka/</span><br></pre></td></tr></table></figure><p><font color="red">注意：分发之后记得配置其他机器的环境变量</font></p></li><li><p>分别在hadoop104和hadoop105上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p><p><font color="red">注意：broker.id不得重复</font></p></li><li><p>启动集群</p><p>依次在hadoop103、hadoop104、hadoop105节点上启动kafka。</p><p><font color="red">中间加一个-daemon使他成为守护进程，就不需要阻塞了</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon  config/server.properties</span><br><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon  config/server.properties</span><br></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>关闭集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-server-stop.sh stop</span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-server-stop.sh stop</span><br><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure></li></ol><ol start="10"><li><p>kafka群起脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for i in &#96;cat &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;slaves&#96;</span><br><span class="line">do</span><br><span class="line">echo &quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; $i &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot; </span><br><span class="line">ssh $i &#39;source &#x2F;etc&#x2F;profile&amp;&amp;&#x2F;opt&#x2F;module&#x2F;kafka_2.11-0.11.0.2&#x2F;bin&#x2F;kafka-server-start.sh -daemon &#x2F;opt&#x2F;module&#x2F;kafka_2.11-0.11.0.2&#x2F;config&#x2F;server.properties&#39;</span><br><span class="line">echo $?</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h4><ol><li><p>查看当前服务器中的所有topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--list</span><br></pre></td></tr></table></figure></li><li><p>创建topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--create </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--topic first</span><br><span class="line">--partitions 2</span><br><span class="line">--replication-factor 2</span><br></pre></td></tr></table></figure><p><font color="red">选项说明：</font></p><p>–topic 定义topic名</p><p>–replication-factor  定义副本数，<font color="red">副本为1那么就只有leader</font></p><p>–partitions  定义分区数</p><p><font color="red">注意：</font></p><p>副本数不能超过集群数，因为相同分区不能出现在同一个broker上。</p><p>分区数量可以超过集群，因为在一个broker上可以有多个分区。</p></li><li><p>删除topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--delete </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--topic first</span><br></pre></td></tr></table></figure><p>需要<font color="red">servier.properties中设置delete.topic.enable=true</font>否则只是标记删除。</p></li><li><p>查看某个Topic的详情</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--describe </span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--topic first</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>发送消息，生产者不需要使用zk</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh</span><br><span class="line">--topic first</span><br><span class="line">--broker-list hadoop102:9092 </span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">atguigu  atguigu</span></span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>消费消息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh </span><br><span class="line">--bootstrap-server hadoop102:9092 </span><br><span class="line">--from-beginning </span><br><span class="line">--topic first</span><br><span class="line"><span class="meta">#</span><span class="bash"> 新版本，断点是存在kafka中的。offset</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 旧版本,断点是存在zookeeper中的，所以调用zookeeper</span></span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh</span><br><span class="line">--topic first</span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--from-beginning </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>–from-beginning：会把主题中以往所有的数据都读取出来</p></li></ol><ol start="7"><li><p>修改分区数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$bin/kafka-topics.sh </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--alter </span><br><span class="line">--topic first </span><br><span class="line">--partitions 6</span><br></pre></td></tr></table></figure></li></ol><h3 id="第三章-Kafka架构深入"><a href="#第三章-Kafka架构深入" class="headerlink" title="第三章 Kafka架构深入"></a>第三章 Kafka架构深入</h3><h4 id="3-1-Kafka工作流程及文件存储机制"><a href="#3-1-Kafka工作流程及文件存储机制" class="headerlink" title="3.1 Kafka工作流程及文件存储机制"></a>3.1 Kafka工作流程及文件存储机制</h4><p><img src="https://i.loli.net/2020/10/27/e5PozwXQNnUsOKa.png"></p><p>Kafka中消息是以<font color="red">topic</font>进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p><p>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错回复，从上次的位置继续消费。</p><p><img src="https://i.loli.net/2020/10/27/L4kAVj9H5XuDstG.png"></p><p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了<strong>分片</strong>和<strong>索引</strong>机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure><p>index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。</p><p><img src="https://i.loli.net/2020/10/27/BN8QW9cGoCL4FOf.png"></p><p><font color="red">“.index”文件存储大量的索引信息，“.log”文件存储大量的数据</font>，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p><h4 id="3-2-Kafka生产者"><a href="#3-2-Kafka生产者" class="headerlink" title="3.2 Kafka生产者"></a>3.2 Kafka生产者</h4><h5 id="3-2-1-分区策略"><a href="#3-2-1-分区策略" class="headerlink" title="3.2.1 分区策略"></a>3.2.1 分区策略</h5><ol><li><p><strong>分区的原因</strong></p><p>（1）<font color="red">方便在集群中扩展</font>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p><p>（2）<font color="red">可以提高并发</font>，因为可以以Partition为单位读写了。</p></li></ol><ol start="2"><li><p><strong>分区的原则</strong></p><p>我们需要将producer发送的数据封装成一个<font color="red"><strong>ProducerRecord</strong></font>对象。</p><p><img src="https://i.loli.net/2020/10/27/O6aNlcfEwsuGFTA.png"></p><p>（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；</p><p>（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</p><p>（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin（轮询） 算法。</p></li></ol><h5 id="3-2-2-数据可靠性保证"><a href="#3-2-2-数据可靠性保证" class="headerlink" title="3.2.2 数据可靠性保证"></a>3.2.2 数据可靠性保证</h5><p><font color="red">为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</font></p><p><img src="https://i.loli.net/2020/10/27/gPXBCs2tM5KWqE8.png"></p><p><img src="https://i.loli.net/2020/10/27/yPjFMk2lRhT7aSu.png"></p><ol><li><p>副本数据同步策略</p><table><thead><tr><th>方案</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>半数以上完成同步，就发送ack</td><td>延迟低</td><td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr><tr><td>全部完成同步，才发送ack</td><td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td><td>延迟高</td></tr></tbody></table><p>Kafka选择了第二种方案，原因如下：</p><p>1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p><p>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p></li><li><p>ISR（同步队列，为了选出新的Leader）</p><p>采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p><p><font color="red">Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由*<strong>*replica.lag.time.max.ms**</strong>参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</font></p></li></ol><ol start="3"><li><p>ack应答机制</p><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p><p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p><p><strong><font color="red">acks参数配置：</font></strong></p><p>(1) 0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能<font color="red">丢失数据</font></p><p><img src="https://i.loli.net/2020/10/27/kMvyXUoQbd2ZAC9.png"></p><p><img src="https://i.loli.net/2020/10/27/2vFlIECj4wgM3Ok.png"></p><p>(2) 1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<font color="red">丢失数据</font></p><p><img src="https://i.loli.net/2020/10/27/kMvyXUoQbd2ZAC9.png"></p><p><img src="https://i.loli.net/2020/10/27/2vFlIECj4wgM3Ok.png"></p><p>(3) -1(all)：producer等待broker的ack，partition的leader和follower(ISR里面的)全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<font color="red">数据重复</font>。</p><p><img src="https://i.loli.net/2020/10/27/ja5weIKLB4RVsXk.png"></p><p><img src="https://i.loli.net/2020/10/27/jJhk89QpGm4q3A7.png"></p></li><li><p>故障处理细节</p><p><img src="https://i.loli.net/2020/10/27/T8uFe5sNpKrZMw7.png"></p><p>（1）follower故障</p><p>​        follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该，<font color="red">follower的LEO大于等于该Partition的HW</font>，即follower追上leader之后，就可以重新加入ISR了。</p><p>（2）leader故障</p><p>​        leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p><p><font color="red">注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</font></p></li></ol><h5 id="3-2-3-Exactly-Once语义（解决数据重复问题）"><a href="#3-2-3-Exactly-Once语义（解决数据重复问题）" class="headerlink" title="3.2.3 Exactly Once语义（解决数据重复问题）"></a>3.2.3 Exactly Once语义（解决数据重复问题）</h5><p>将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 <font color="red">At Least Once 语义</font>。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 <font color="red">At Most Once 语义。 </font></p><p>At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once 可以保证数据不重复，但是不能保证数据不丢失。<font color="red">但是，对于一些非常重要的信息，比如说 交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。</font>在 0.11 版 本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局 去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 </p><p>0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论 向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语 义，就构成了 Kafka 的 Exactly Once 语义。即：</p><p>​                       At Least Once + <font color="red">幂等性</font> = Exactly Once </p><p>要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker 只 会持久化一条。 </p><p>但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨 分区跨会话的 Exactly Once。 </p><p>使用时，只需将enable.idempotence属性设置为true，kafka自动将acks属性设为-1。</p><h4 id="3-3-Kafka消费者"><a href="#3-3-Kafka消费者" class="headerlink" title="3.3 Kafka消费者"></a>3.3 Kafka消费者</h4><h5 id="3-3-1-消费方式"><a href="#3-3-1-消费方式" class="headerlink" title="3.3.1 消费方式"></a>3.3.1 消费方式</h5><p>consumer采用pull（拉）模式从broker中读取数据。</p><p><font color="red">push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。</font>它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p><font color="red">pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。</font>针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</p><h5 id="3-3-2-分区分配策略"><a href="#3-3-2-分区分配策略" class="headerlink" title="3.3.2 分区分配策略"></a>3.3.2 分区分配策略</h5><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p><p>Kafka有两种分配策略，</p><p>一是roundrobin（轮询）：面向组，</p><p>一是range，面向主题</p><p><strong>（1）roundrobin（轮询）</strong></p><p><img src="https://i.loli.net/2020/10/27/AzubjqYyR5cNhXU.png"></p><p><img src="https://i.loli.net/2020/10/27/7yMa4OVPBswWgQY.png"></p><p><strong>（2）range（范围）</strong><font color="red">默认策略</font></p><p>​    <img src="https://i.loli.net/2020/10/27/AzubjqYyR5cNhXU.png"></p><p><img src="https://i.loli.net/2020/10/27/zVYlLwQhXFfdDix.png"></p><h5 id="3-3-3-offset的维护"><a href="#3-3-3-offset的维护" class="headerlink" title="3.3.3 offset的维护"></a>3.3.3 offset的维护</h5><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p><font color="red">Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</font></p><p>（1）保存在zk中时，是由组+topic+分区来保存的</p><p><img src="https://i.loli.net/2020/10/27/ZDjcShKlJ74qO5Q.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get &#x2F;consumers&#x2F;console-consumer-88502&#x2F;offsets&#x2F;bigdata&#x2F;0</span><br><span class="line">                     组名                       topic 分区</span><br></pre></td></tr></table></figure><p>（2）保存在本地</p><p>​        1.修改配置文件consumer.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exclude.internal.topic&#x3D;false &#x2F;&#x2F;可以让消费者消费内部主题</span><br></pre></td></tr></table></figure><p>​        2.读取offset</p><p>​            0.11.0.0之前的版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><p>​            0.11.0.0之后的版本（含）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh </span><br><span class="line">--topic __consumer_offsets</span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OddsetsMessageFormatter&quot;</span><br><span class="line">--consumer.config config/consumer.properties</span><br><span class="line">--from-beginning</span><br></pre></td></tr></table></figure><p>组+topic+分区唯一确定一个offset</p><h5 id="3-3-4-消费者组案例"><a href="#3-3-4-消费者组案例" class="headerlink" title="3.3.4 消费者组案例"></a>3.3.4 消费者组案例</h5><p><strong>需求</strong></p><p>​    测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费</p><p><strong>案例实操</strong></p><p>​    （1）在hadoop102、hadoop103上修改/opt/module/kafka/config/consumer.properties配置文件中的group.id属性为任意组名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 config]$ vi consumer.properties</span><br><span class="line">group.id=atguigu</span><br></pre></td></tr></table></figure><p>（2）在hadoop102、hadoop103上分别启动消费者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh </span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--topic first </span><br><span class="line">--consumer.config config/consumer.properties</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh </span><br><span class="line">--bootstrap-server hadoop102:9092 </span><br><span class="line">--topic first </span><br><span class="line">--consumer.config config/consumer.properties</span><br></pre></td></tr></table></figure><p>（3）在hadoop104上启动生产者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh </span><br><span class="line">--broker-list hadoop102:9092 --topic first</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br></pre></td></tr></table></figure><p>（4）查看hadoop102和Hadoop103的接收者。</p><p>​        同一时刻同一个组中只有一个消费者接收到数据</p><p>​        不同组可以同时消费</p><h4 id="3-4-Kafka高效读写数据"><a href="#3-4-Kafka高效读写数据" class="headerlink" title="3.4 Kafka高效读写数据"></a>3.4 Kafka高效读写数据</h4><p><strong>（1）顺序写磁盘</strong></p><p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p><p><strong>（2）零复制技术</strong></p><p>​    原始</p><p>​    <img src="https://i.loli.net/2020/10/27/vNZFec9DIRwf67M.png"></p><p>​    零拷贝</p><p>​    <img src="https://i.loli.net/2020/10/27/lISnkgHLW4cdjeA.png"></p><h4 id="3-5-Zookeeper在Kafka中的作用"><a href="#3-5-Zookeeper在Kafka中的作用" class="headerlink" title="3.5 Zookeeper在Kafka中的作用"></a>3.5 Zookeeper在Kafka中的作用</h4><p>Kafka集群中有一个broker会被选举为Controller，<font color="red">负责管理集群broker的上下线</font>，所有topic的<font color="red">分区副本分配</font>和<font color="red">leader选举</font>等工作。</p><p>Controller的管理工作都依赖于Zookeeper。</p><p>以下是partition的leader选举过程：</p><p><img src="https://i.loli.net/2020/10/27/aBtgI3Rc462uLeS.png"></p><h4 id="3-6-Kafka事务"><a href="#3-6-Kafka事务" class="headerlink" title="3.6 Kafka事务"></a>3.6 Kafka事务</h4><p>Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><h5 id="3-6-1-Producer事务"><a href="#3-6-1-Producer事务" class="headerlink" title="3.6.1 Producer事务"></a>3.6.1 Producer事务</h5><p>为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。这样当Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。</p><p> 为了管理 Transaction，Kafka 引入了一个新的组件 Transaction Coordinator。Producer 就 是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于 事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 </p><h5 id="3-6-2-Consumer事务"><a href="#3-6-2-Consumer事务" class="headerlink" title="3.6.2 Consumer事务"></a>3.6.2 Consumer事务</h5><p>上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对 较弱，尤其时无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访 问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后删除的情况。 </p><h3 id="第四章-Kafka-API"><a href="#第四章-Kafka-API" class="headerlink" title="第四章 Kafka API"></a>第四章 Kafka API</h3><h4 id="4-1-Producer-API"><a href="#4-1-Producer-API" class="headerlink" title="4.1 Producer API"></a>4.1 Producer API</h4><h5 id="4-1-1-消息发送流程"><a href="#4-1-1-消息发送流程" class="headerlink" title="4.1.1 消息发送流程"></a>4.1.1 消息发送流程</h5><p>Kafka的producer发送消息采用的是<font color="red">异步发送</font>的方式。在消息发送的过程中，涉及到了<font color="red">两个线程——main线程和Sender线程</font>，以及<font color="red">一个线程共享变量——RecordAccumulator</font>。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p><p><img src="https://i.loli.net/2020/10/27/Sg5CBiAxHpc4J1f.png"></p><p><font color="red">相关参数：</font></p><p>batch.size：只有数据积累到batch.size之后，sender才会发送数据</p><p>linger.ms：如果数据迟迟未到达batch.size，sender等待linger.time之后就会发送数据。</p><h5 id="4-1-2-异步发送API"><a href="#4-1-2-异步发送API" class="headerlink" title="4.1.2 异步发送API"></a>4.1.2 异步发送API</h5><ol><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>编写代码</p><p>需要用到的类：</p><p><strong>KafkaProducer：</strong>需要创建一个生产者对象，用来发送数据</p><p><strong>ProducerConfig：</strong>获取所需的一系列配置参数</p><p><strong>ProducerRecord：</strong>每条数据都要封装成一个ProducerRecord对象</p></li><li><p>不带回调函数的API</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        <span class="comment">// ack应答级别</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小，16k</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间，1ms</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小，32M</span></span><br><span class="line">        <span class="comment">// key的序列化类</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        <span class="comment">// value的序列化类</span></span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close(); <span class="comment">//资源要关</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>带回调函数的API</p><p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p><p><font color="red">注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//ProduceConfig</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu--&quot;</span>+i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span>    </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;success-&gt;&quot;</span> + metadata.offset());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-1-3-同步发送API"><a href="#4-1-3-同步发送API" class="headerlink" title="4.1.3 同步发送API"></a>4.1.3 同步发送API</h5><p>同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</p><p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需要在调用Futrue对象的get方法即可。</p><p>send线程进行时阻塞main线程实现同步</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i)).get();</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-2-Consumer-API"><a href="#4-2-Consumer-API" class="headerlink" title="4.2 Consumer API"></a>4.2 Consumer API</h4><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p><h5 id="4-2-1-手动提交offset"><a href="#4-2-1-手动提交offset" class="headerlink" title="4.2.1 手动提交offset"></a>4.2.1 手动提交offset</h5><ol><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>编写代码</p><p>需要用到的类</p><p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p><p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p><p><strong>ConsumerRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// ConsumerConfig</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//自动提交offset</span></span><br><span class="line">       </span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>代码分析</p><p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将**<font color="red">本次poll的一批数据最高的偏移量提交</font>**；不同点是，commitSync会失败重试，一直到提交成功（如果由于不可恢复原因导致，也会提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p></li><li><p>数据重复消费问题</p><p><img src="https://i.loli.net/2020/10/27/BJAinMYoOmVGvxP.png"></p></li></ol><h5 id="4-2-2-自动提交offset"><a href="#4-2-2-自动提交offset" class="headerlink" title="4.2.2 自动提交offset"></a>4.2.2 自动提交offset</h5><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。</p><p>自动提交offset的相关参数：</p><p>enable.auto.commit：是否开启自动提交offset功能</p><p>auto.commit.interval.ms：自动提交offset的时间间隔</p><p>以下为自动提交offset的代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-自定义Interceptor"><a href="#4-3-自定义Interceptor" class="headerlink" title="4.3 自定义Interceptor"></a>4.3 自定义Interceptor</h4><h5 id="4-3-1-拦截器原理"><a href="#4-3-1-拦截器原理" class="headerlink" title="4.3.1 拦截器原理"></a>4.3.1 拦截器原理</h5><p>​        Producer拦截器(interceptor)是在Kafka0.10版本中被引入的，主要用于实现clients端的定制化控制逻辑。</p><p>​        对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是<code>org.apache.kafka.clients.producer.ProducerInterceptor</code>，其定义的方法包括：</p><p>（1）configure(configs)</p><p>​        获取配置信息和初始化数据时调用。</p><p>（2）onSend(ProducerRecord)</p><p>​        该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。<font color="red">用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区</font>，否则会影响目标分区的计算。</p><p>（3）onAcknowledgement(RecordMetadata,Exception)</p><p>​        <font color="red">该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。</font>并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。</p><p>（4）close</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 概述 / 第二章 快速入门 / 第三章 Kafka架构深入 / 第四章 Kafka API /&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Flume入门</title>
    <link href="http://luo6656.github.io/2020/06/17/BigDataFrame/Flume%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/06/17/BigDataFrame/Flume%E5%85%A5%E9%97%A8/</id>
    <published>2020-06-16T16:00:00.000Z</published>
    <updated>2020-10-27T06:24:12.189Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 概述 / 第二章 快速入门 / 第三章 企业开发案例 / 第四章 监测控制 / 第五章 自定义Interceptor /<br>第六章 自定义Source / 第七章 自定义Sink / 第八章 企业真实面试题</p><a id="more"></a><h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h3 id="第一章-概述"><a href="#第一章-概述" class="headerlink" title="第一章 概述"></a>第一章 概述</h3><h4 id="1-1-Flume定义"><a href="#1-1-Flume定义" class="headerlink" title="1.1 Flume定义"></a>1.1 Flume定义</h4><p>Flume是Cloudera提供的一个高可用的，高可靠的，<font color="red">分布式的海量日志采集、聚合和传输的系统</font>。Flume基于流式架构，灵活简单。</p><p><img src="https://i.loli.net/2020/10/27/ADfw2GVRl5voePH.png"></p><h4 id="1-2-Flume的优点"><a href="#1-2-Flume的优点" class="headerlink" title="1.2 Flume的优点"></a>1.2 Flume的优点</h4><ol><li>可以和任意存储进程集成。</li><li>输入的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</li><li>flume中的事务基于channel，使用两个事务模型(sender+receiver),确保消息被可靠发送。</li></ol><p>Flume使用两个独立的事务分别负责从source到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p><h4 id="1-3-Flume组成架构"><a href="#1-3-Flume组成架构" class="headerlink" title="1.3 Flume组成架构"></a>1.3 Flume组成架构</h4><p><img src="https://i.loli.net/2020/10/27/1kzlx3aqAoQXPts.png"><br><img src="https://i.loli.net/2020/10/27/xQqZr2XVidByMsw.png"></p><p>Channel是被动的，只能被put和take</p><p>​    下面我们来详细介绍一下Flume架构中的组件</p><h5 id="1-3-1-Agent"><a href="#1-3-1-Agent" class="headerlink" title="1.3.1 Agent"></a>1.3.1 Agent</h5><p>​        Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p><p>​        Agent主要有3个部分组成，source，channel，sink</p><h5 id="1-3-2-Source"><a href="#1-3-2-Source" class="headerlink" title="1.3.2 Source"></a>1.3.2 Source</h5><p><font color="red">Source是负责接收数据到Flume Agent的组件。</font>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、<font color="red">exec</font>、jms、<font color="red">spooling directory</font>、netcat、sequence generator、syslog、http、legacy。</p><h5 id="1-3-3-Channel"><a href="#1-3-3-Channel" class="headerlink" title="1.3.3 Channel"></a>1.3.3 Channel</h5><p><font color="red">Channel是位于Source和Sink之间的缓冲区。</font>因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel</p><p><font color="red">Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。</font>如果需要关系数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p><p><font color="red">File Channel将所有事件写到磁盘。</font>因此在程序关闭或机器宕机的情况下不会丢失数据。</p><h5 id="1-3-4-Sink"><a href="#1-3-4-Sink" class="headerlink" title="1.3.4 Sink"></a>1.3.4 Sink</h5><p><font color="red">Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</font></p><p><font color="red">Sink是完全事务性的。</font>在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p><h5 id="1-3-5-Event"><a href="#1-3-5-Event" class="headerlink" title="1.3.5 Event"></a>1.3.5 Event</h5><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。Event由可选的header和载有数据的一个byte array构成。Header是容纳了key-value字符串对的HashMap</p><p><img src="https://i.loli.net/2020/10/27/q1BEiWyTMe5oQZf.png"></p><h4 id="1-4-Flume拓扑结构"><a href="#1-4-Flume拓扑结构" class="headerlink" title="1.4 Flume拓扑结构"></a>1.4 Flume拓扑结构</h4><p><img src="https://i.loli.net/2020/10/27/Gp8SyUnYzgBjNsQ.png"></p><p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量，flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p><p><img src="https://i.loli.net/2020/10/27/2hmk4axYNT3gRie.png"></p><p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送不同的目的地。</p><p><img src="https://i.loli.net/2020/10/27/zXwGm3ZHBsIWeKR.png"></p><p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p><p><img src="https://i.loli.net/2020/10/27/9265on7NXimGptc.png"></p><p>这种模式是我们最常见的，也是非常实用，日常web应用通常分布在上百个服务，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p><h4 id="1-5-Flume-Agent内部原理"><a href="#1-5-Flume-Agent内部原理" class="headerlink" title="1.5 Flume Agent内部原理"></a>1.5 Flume Agent内部原理</h4><p><img src="https://i.loli.net/2020/10/27/ALNnw4ZD3EUWci6.png"></p><h3 id="第二章-快速入门"><a href="#第二章-快速入门" class="headerlink" title="第二章 快速入门"></a>第二章 快速入门</h3><h4 id="2-1-Flume安装地址"><a href="#2-1-Flume安装地址" class="headerlink" title="2.1 Flume安装地址"></a>2.1 Flume安装地址</h4><ol><li><p>Flume官网地址</p><blockquote><p><a href="http://flume.apache.org/">http://flume.apache.org/</a></p></blockquote></li><li><p>文档查看地址</p><blockquote><p><a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p></blockquote></li><li><p>下载地址</p><blockquote><p><a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></p></blockquote></li></ol><h4 id="2-2-安装部署"><a href="#2-2-安装部署" class="headerlink" title="2.2 安装部署"></a>2.2 安装部署</h4><ol><li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-flume-1.7.0-bin的名称为flume</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li><li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[atguigu@hadoop102 conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure></li></ol><h3 id="第三章-企业开发案例"><a href="#第三章-企业开发案例" class="headerlink" title="第三章 企业开发案例"></a>第三章 企业开发案例</h3><h4 id="3-1-监控端口数据官方案例（netcat）"><a href="#3-1-监控端口数据官方案例（netcat）" class="headerlink" title="3.1 监控端口数据官方案例（netcat）"></a>3.1 监控端口数据官方案例（netcat）</h4><ol><li><p>案例需求</p><p>（1）首先启动Flume任务，监控本机44444端口，服务器</p><p>（2）然后通过netcat工具向本机44444端口发送消息，客户端。</p><p>（3）最后Flume将监听的数据实时显示在控制台。</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/KQRVy8miANSlcOn.png"></p></li><li><p>实现步骤</p><p>（1）安装netcat工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure><p>（2）判断44444端口是否被占用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure><p>功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 </p><p>基本语法：netstat [选项]</p><p>选项参数：</p><p>​    -t或–tcp：显示TCP传输协议的连线状况； </p><p>​    -u或–udp：显示UDP传输协议的连线状况；</p><p>​    -n或–numeric：直接使用ip地址，而不通过域名服务器； </p><p>​    -l或–listening：显示监控中的服务器的Socket； </p><p>​    -p或–programs：显示正在使用Socket的程序识别码（PID）和程序名称；</p></li></ol><p>   （3）创建Flume Agent配置文件flume-netcat-logger.conf</p><p>   ​        a.在flume目录下创建job文件夹并进入job文件夹</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir job</span><br><span class="line">[atguigu@hadoop102 flume]$ cd job/</span><br></pre></td></tr></table></figure><p>   ​        b.在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-netcat-logger.conf</span><br></pre></td></tr></table></figure><p>   ​        c.在flume-netcat-logger.conf文件中添加如下</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><p>   <font color="red">注：配置文件来源于官方手册</font><a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p><p>   <img src="https://i.loli.net/2020/10/27/w5gbOoZCneYFyRE.png"></p><p>   ​        <font color="red">注意：最后一行channel，说明一个sink只能绑定一个channel，但是一个channel可以绑定多个sink</font></p><p>   （4）先开启flume监听端口</p><p>   ​        第一种写法：</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>   ​        第二种写法（简写）：</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>   ​        参数说明（<font color="red">–XX表示参数，空格后面表示参数值</font>）：</p><p>   ​    –conf conf/  ：表示配置文件存储在conf/目录</p><p>   ​    –name a1    ：表示给agent起名为a1</p><p>   ​    –conf-file job/flume-netcat.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p><p>   ​    -Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p><p>   （5）使用netcat工具向本机的44444端口发送内容</p>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure><p>   （6）在Flume监听页面观察接收数据情况</p><p>   <img src="https://i.loli.net/2020/10/27/pVhIxjlGvEPsKa5.png"></p><p>   <font color="red">nc hadoop102 44444 ,flume能否接收到</font></p><h4 id="3-2-实时读取本地文件到HDFS案例（exec）"><a href="#3-2-实时读取本地文件到HDFS案例（exec）" class="headerlink" title="3.2 实时读取本地文件到HDFS案例（exec）"></a>3.2 实时读取本地文件到HDFS案例（exec）</h4><ol><li><p>案例需求：实时监控Hive日志，并上传到HDFS中</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/VMhIEBTSuDaGzrl.png"></p></li></ol><ol start="3"><li><p>实现步骤</p><p>（1）Flume要想将数据输出到HDFS，必须持有Hadoop相关jar包</p><p>​        将</p><p>​            commons-configuration-1.6.jar、</p><p>​            hadoop-auth-2.7.2.jar、</p><p>​            hadoop-common-2.7.2.jar、</p><p>​            hadoop-hdfs-2.7.2.jar、</p><p>​            commons-io-2.4.jar、 </p><p>​            htrace-core-3.1.0-incubating.jar</p><p>​        拷贝到/opt/module/flume/lib文件夹下。</p><p>（2）创建flume-file-hdfs.conf文件</p><p>​    创建文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>​    注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources &#x3D; r2</span><br><span class="line">a2.sinks &#x3D; k2</span><br><span class="line">a2.channels &#x3D; c2</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a2.sources.r2.type &#x3D; exec</span><br><span class="line">a2.sources.r2.command &#x3D; tail -F &#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs&#x2F;hive.log</span><br><span class="line">a2.sources.r2.shell &#x3D; &#x2F;bin&#x2F;bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type &#x3D; hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F;flume&#x2F;%Y%m%d&#x2F;%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix &#x3D; logs-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.round &#x3D; true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue &#x3D; 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit &#x3D; hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.batchSize &#x3D; 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval &#x3D; 60</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollSize &#x3D; 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k2.hdfs.rollCount &#x3D; 0</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type &#x3D; memory</span><br><span class="line">a2.channels.c2.capacity &#x3D; 1000</span><br><span class="line">a2.channels.c2.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels &#x3D; c2</span><br><span class="line">a2.sinks.k2.channel &#x3D; c2</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>对于所有与时间相关的转义序列，Event Header中必须存在以”timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p><p><font color="red">a3.sinks.k3.hdfs.useLocalTimeStamp=true</font></p><p><img src="https://i.loli.net/2020/10/27/vQKeJ659tTcCqsL.png"></p><p>（3）执行监控配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>（4）开启Hadoop和Hive并操作Hive产生日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure><p>（5）在HDFS上查看文件</p><p>​    <img src="https://i.loli.net/2020/10/27/tYWwDTFCxQy7bM1.png"></p></li></ol><h4 id="3-3-实时读取目录文件到HDFS案例（spooldir）"><a href="#3-3-实时读取目录文件到HDFS案例（spooldir）" class="headerlink" title="3.3 实时读取目录文件到HDFS案例（spooldir）"></a>3.3 实时读取目录文件到HDFS案例（spooldir）</h4><ol><li><p>案例需求：使用Flume监听整个目录的文件</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/XzdJSZQjA2aBlwg.png"></p></li><li><p>实现步骤</p><p>（1）创建配置文件flume-dir-hdfs.conf</p><p>​        a.创建一个文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>​        b.打开文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>​        c.添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash">忽略所有以.tmp结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/t1FcnYeX6jvJOIx.png"></p><p>（2）启动监控文件夹命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p><font color="red">说明：在使用Spooling Directory Source时</font></p><p>a. 不要在监控目录中创建并持续修改文件(不能监控动态变化的文件)</p><p>b. 上传完成的文件会以.COMPLETED结尾（默认）</p><p>c. 被监控文件夹每500毫秒扫描一次文件变动</p><p>（3）向upload文件夹中添加文件</p><p>​        在/opt/module/flume目录下创建upload文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir upload</span><br></pre></td></tr></table></figure><p>​        向upload文件夹中添加文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.txt</span><br><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.tmp</span><br><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.log</span><br></pre></td></tr></table></figure><p>（4）查看HDFS上的数据</p><p><img src="https://i.loli.net/2020/10/27/hBistWC9Gu5aFx6.png"></p><p>（5）等待1s，再次查询upload文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.tmp</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.txt.COMPLETED</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-3-实时监控目录下的多个追加文件（tailDir）"><a href="#3-3-实时监控目录下的多个追加文件（tailDir）" class="headerlink" title="3.3 实时监控目录下的多个追加文件（tailDir）"></a>3.3 实时监控目录下的多个追加文件（tailDir）</h4><p>​        Exec source适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir中source能保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而Taildir Source既能实现断点续传，又可以保证数据不丢失，还能够进行实时监控。</p><p><strong>案例需求</strong></p><p>使用Flume监听整个目录的实时追加文件，并上传至HDFS</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/iebAFwkufNgUISD.png"></p><p><strong>实现步骤</strong></p><p>创建配置文件taildir-flume-hdfs.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r1.filegroups &#x3D; f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;files&#x2F;file1.txt</span><br><span class="line">a1.sources.r1.filegroups.f2 &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;files&#x2F;file2.txt</span><br><span class="line">a1.sources.r1.positionFile &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;position&#x2F;position.json</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动监控命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flume-ng agent -c conf&#x2F; -n a1 -f job&#x2F;files-flume-logger.conf -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure><h4 id="3-4-单数据源多出口案例（选择器）"><a href="#3-4-单数据源多出口案例（选择器）" class="headerlink" title="3.4 单数据源多出口案例（选择器）"></a>3.4 单数据源多出口案例（选择器）</h4><p><img src="https://i.loli.net/2020/10/27/8fqLvsIX12VHimW.png"></p><p><strong>案例需求</strong></p><p>​    使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到LocalFileSystem。</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/GlJQz5OHbW3I7FV.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group1文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ cd group1/</span><br></pre></td></tr></table></figure><p>在/opt/module/datas/目录下创建flume3文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ mkdir flume3</span><br></pre></td></tr></table></figure></li><li><p>创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-file-flume.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-file-flume.conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将数据流复制给所有channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink端的avro是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p><font color="red">注：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架</font></p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li></ol><ol start="3"><li><p>创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-flume-hdfs.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-flume-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>端的avro是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-flume-dir.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-flume-dir.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>启动Hadoop和Hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>检查HDFS上数据</p><p><img src="https://i.loli.net/2020/10/27/CWbqTAVaHB6ye7n.png"></p></li></ol><ol start="8"><li><p>检查/opt/module/datas/flume3目录中数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume3]$ ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 5942 5月  22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-5-单数据源多出口案例（Sink组）（负载均衡和故障转移）"><a href="#3-5-单数据源多出口案例（Sink组）（负载均衡和故障转移）" class="headerlink" title="3.5 单数据源多出口案例（Sink组）（负载均衡和故障转移）"></a>3.5 单数据源多出口案例（Sink组）（负载均衡和故障转移）</h4><p>​    单Source、Channel多Sink（负载均衡），如图</p><p>​    <img src="https://i.loli.net/2020/10/27/d4SW7w2nmeo83x5.png"></p><p><strong>案例需求</strong></p><p>​    使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/YngpWLfw7h5IDUj.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group2文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ cd group2&#x2F;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建flume-netcat-flume</p><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-netcat-flume.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-netcat-flume.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">/*这里是负载均衡，故障转移的type是failover*/</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p><font color="red">注</font>：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li></ol><ol start="3"><li><p>创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-flume-console1.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-flume-console1.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume-flume-console2.conf</p><p>配置上机Flume输出的Source，输出是到本地控制台</p><p>创建配置文件并打开</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-flume-console2.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-flume-console2.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>使用netcat工具向本机的4444端口发送内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 44444</span></span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li>查看Flume2及Flume3的控制台打印日志</li></ol><h4 id="3-6-多数据源汇总案例"><a href="#3-6-多数据源汇总案例" class="headerlink" title="3.6 多数据源汇总案例"></a>3.6 多数据源汇总案例</h4><p>多Source汇总数据到单Flume如图</p><p><img src="https://i.loli.net/2020/10/27/Cc5oJmHfTSNPyrA.png"></p><p><strong>案例需求</strong></p><p>​    hadoop103上的Flume-1监控文件/opt/module/group.log</p><p>​    hadoop102上的Flume-2监控某一个端口的数据流</p><p>​    Flume-1与Flume-2将数据发送给hadoop104上的Flume-3，Flume-3将最终数据打印到控制台。</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/DWFKlntXPGQjqA5.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>分发Flume</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync flume</span><br></pre></td></tr></table></figure><p>在hadoop102、hadoop103以及hadoop104的/opt/module/flume/job目录下创建一个group3文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ mkdir group3</span><br><span class="line">[atguigu@hadoop103 job]$ mkdir group3</span><br><span class="line">[atguigu@hadoop104 job]$ mkdir group3</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建flume1-logger-flume.conf</p><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume</p><p>在hadoop103上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 group3]$ touch flume1-logger-flume.conf</span><br><span class="line">[atguigu@hadoop103 group3]$ vim flume1-logger-flume.conf </span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>创建flume2-netcat-flume.conf</p><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在hadoop102上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group3]$ touch flume2-netcat-flume.conf</span><br><span class="line">[atguigu@hadoop102 group3]$ vim flume2-netcat-flume.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = hadoop104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume3-flume-logger.conf</p><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在hadoop104上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 group3]$ touch flume3-flume-logger.conf</span><br><span class="line">[atguigu@hadoop104 group3]$ vim flume3-flume-logger.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>在hadoop103上向/opt/module目录下的group.log追加内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 module]$ echo &#x27;hello&#x27; &gt; group.log</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>在hadoop102上向44444端口发送数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ telnet hadoop102 44444</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>检查hadoop104上数据</p><p><img src="G:\截图\flume\3_6_2.png"></p></li></ol><h3 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h3><h4 id="4-1-Ganglia的安装与部署"><a href="#4-1-Ganglia的安装与部署" class="headerlink" title="4.1 Ganglia的安装与部署"></a>4.1 Ganglia的安装与部署</h4><ol><li><p>安装httpd服务与php</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install httpd php</span><br></pre></td></tr></table></figure></li><li><p>安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure></li><li><p>安装ganglia</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install ganglia-gmetad </span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install ganglia-web</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum install -y ganglia-gmond</span><br></pre></td></tr></table></figure><p>Ganglia由gmond、gmetad和gweb三部分组成。</p><p><font color="red">gmond（Ganglia Monitoring Daemon）</font>是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等。</p><p><font color="red">gmetad（Ganglia Meta Daemon）</font>整合所有信息，并将其以RRD格式存储至磁盘的服务。</p><p><font color="red">gweb（Ganglia Web）</font>Ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。</p></li><li><p>修改配置文件/etc/httpd/conf.d/ganglia.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure><p><font color="red">修改为红颜色的配置：</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Ganglia monitoring system php web frontend</span><br><span class="line">Alias &#x2F;ganglia &#x2F;usr&#x2F;share&#x2F;ganglia</span><br><span class="line">&lt;Location &#x2F;ganglia&gt;</span><br><span class="line">  Order deny,allow</span><br><span class="line">  #Deny from all</span><br><span class="line">  Allow from all</span><br><span class="line">  # Allow from 127.0.0.1</span><br><span class="line">  # Allow from ::1</span><br><span class="line">  # Allow from .example.com</span><br><span class="line">&lt;&#x2F;Location&gt;</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>修改配置文件/etc/ganglia/gmetad.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/ganglia/gmetad.conf</span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_source &quot;hadoop102&quot; 192.168.1.102</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>修改配置文件/etc/ganglia/gmond.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim &#x2F;etc&#x2F;ganglia&#x2F;gmond.conf </span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">  name = &quot;hadoop102&quot;</span><br><span class="line">  owner = &quot;unspecified&quot;</span><br><span class="line">  latlong = &quot;unspecified&quot;</span><br><span class="line">  url = &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">bind_hostname = yes <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line">                       # This option tells gmond to use a source address</span><br><span class="line">                       # that resolves to the machine&#x27;s hostname.  Without</span><br><span class="line">                       # this, the metrics may appear to come from any</span><br><span class="line">                       # interface and the DNS names associated with</span><br><span class="line">                       # those IPs will be used to create the RRDs.</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  host = 192.168.1.102</span><br><span class="line">  port = 8649</span><br><span class="line">  ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  port = 8649</span><br><span class="line">  bind = 192.168.1.102</span><br><span class="line">  retry_bind = true</span><br><span class="line"><span class="meta">  #</span><span class="bash"> Size of the UDP buffer. If you are handling lots of metrics you really</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> buffer = 10485760</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>修改配置文件/etc/selinux/config</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo setenforce 0</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>启动ganglia</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo service httpd start</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo service gmetad start</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo service gmond start</span><br></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>打开网页浏览ganglia页面</p><p><a href="http://192.168.1.102/ganglia">http://192.168.1.102/ganglia</a></p><p><font color="red">提示</font>：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo chmod -R 777 &#x2F;var&#x2F;lib&#x2F;ganglia</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-2-操作Flume测试监控"><a href="#4-2-操作Flume测试监控" class="headerlink" title="4.2 操作Flume测试监控"></a>4.2 操作Flume测试监控</h4><ol><li><p>修改/opt/module/flume/conf目录下的flume-env.sh配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS&#x3D;&quot;-Dflume.monitoring.type&#x3D;ganglia</span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.1.102:8649</span><br><span class="line">-Xms100m</span><br><span class="line">-Xmx200m&quot;</span><br></pre></td></tr></table></figure></li><li><p>启动Flume任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin&#x2F;flume-ng agent \</span><br><span class="line">--conf conf&#x2F; \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf-file job&#x2F;flume-netcat-logger.conf \</span><br><span class="line">-Dflume.root.logger&#x3D;&#x3D;INFO,console \</span><br><span class="line">-Dflume.monitoring.type&#x3D;ganglia \</span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.1.102:8649</span><br></pre></td></tr></table></figure></li><li><p>发送数据观察ganglia监测图</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ nc localhost 44444</span><br></pre></td></tr></table></figure><p><strong>样式如图：</strong></p><p><img src="https://i.loli.net/2020/10/27/dBG5X83zocSWuOI.png"></p><p><strong>图例说明：</strong></p><table><thead><tr><th>字段（图表名称）</th><th>字段含义</th></tr></thead><tbody><tr><td>EventPutAttemptCount</td><td>source尝试写入channel的事件总数量</td></tr><tr><td>EventPutSuccessCount</td><td>成功写入channel且提交的事件总数量</td></tr><tr><td>EventTakeAttemptCount</td><td>sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。</td></tr><tr><td>EventTakeSuccessCount</td><td>sink成功读取的事件的总数量</td></tr><tr><td>StartTime</td><td>channel启动的时间（毫秒）</td></tr><tr><td>StopTime</td><td>channel停止的时间（毫秒）</td></tr><tr><td>ChannelSize</td><td>目前channel中事件的总数量</td></tr><tr><td>ChannelFillPercentage</td><td>channel占用百分比</td></tr><tr><td>ChannelCapacity</td><td>channel的容量</td></tr></tbody></table></li></ol><h3 id="第五章-自定义Interceptor"><a href="#第五章-自定义Interceptor" class="headerlink" title="第五章 自定义Interceptor"></a>第五章 自定义Interceptor</h3><h3 id="第六章-自定义Source"><a href="#第六章-自定义Source" class="headerlink" title="第六章 自定义Source"></a>第六章 自定义Source</h3><h4 id="6-1-介绍"><a href="#6-1-介绍" class="headerlink" title="6.1 介绍"></a>6.1 介绍</h4><p><font color="red">Source是负责接收数据到Flume Agent的组件。</font>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。</p><p>官方也提供了自定义source的接口：</p><p><a href="#source">https://flume.apache.org/FlumeDeveloperGuide.html#source</a>根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p><p>实现相应方法：</p><p><code>getBackOffSleepIncrement()</code>//暂不用</p><p><code>getMaxBackOffSleepInterval()</code>//暂不用</p><p><code>configure(Context context)</code>//初始化context（读取配置文件内容）</p><p><code>process()</code>//获取数据封装成event并写入channel，<font color="red">这个方法将被循环调用。</font></p><p>使用场景：读取MySQL数据或者其他文件系统。</p><h4 id="6-2-需求"><a href="#6-2-需求" class="headerlink" title="6.2 需求"></a>6.2 需求</h4><p>使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。</p><p><img src="https://i.loli.net/2020/10/27/HkaSKRlrtq4hbLo.png"></p><h4 id="6-3-分析"><a href="#6-3-分析" class="headerlink" title="6.3 分析"></a>6.3 分析</h4><p><img src="https://i.loli.net/2020/10/27/92KqrxYAdecPwDs.png"></p><h4 id="6-4-编码"><a href="#6-4-编码" class="headerlink" title="6.4 编码"></a>6.4 编码</h4><ol><li><p>导入pom依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>写java代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义配置文件将来要读取的字段</span></span><br><span class="line">    <span class="keyword">private</span> Long delay;</span><br><span class="line">    <span class="keyword">private</span> String field;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化配置信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        delay = context.getLong(<span class="string">&quot;delay&quot;</span>);</span><br><span class="line">        field = context.getString(<span class="string">&quot;field&quot;</span>, <span class="string">&quot;Hello!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建事件头信息</span></span><br><span class="line">            HashMap&lt;String, String&gt; hearderMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="comment">//创建事件</span></span><br><span class="line">            SimpleEvent event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">            <span class="comment">//循环封装事件</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                <span class="comment">//给事件设置头信息</span></span><br><span class="line">                event.setHeaders(hearderMap);</span><br><span class="line">                <span class="comment">//给事件设置内容</span></span><br><span class="line">                event.setBody((field + i).getBytes());</span><br><span class="line">                <span class="comment">//将事件写入channel</span></span><br><span class="line">                getChannelProcessor().processEvent(event);</span><br><span class="line">                Thread.sleep(delay);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-5-测试"><a href="#6-5-测试" class="headerlink" title="6.5 测试"></a>6.5 测试</h4><ol><li><p>打包</p><p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume)下。</p></li><li><p>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; com.atguigu.MySource</span><br><span class="line">a1.sources.r1.delay &#x3D; 1000</span><br><span class="line">#a1.sources.r1.field &#x3D; atguigu</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>开启任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ pwd</span><br><span class="line">/opt/module/flume</span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysource.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><p><img src="https://i.loli.net/2020/10/27/1uVWsDKybXSUk3B.png"></p></li></ol><h3 id="第七章-自定义Sink"><a href="#第七章-自定义Sink" class="headerlink" title="第七章 自定义Sink"></a>第七章 自定义Sink</h3><h4 id="7-1-介绍"><a href="#7-1-介绍" class="headerlink" title="7.1 介绍"></a>7.1 介绍</h4><p><font color="red">Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent.</font></p><p>Sink是完全事务性地。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写入到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。</p><p><font color="red">官方也提供了自定义source的接口:</font></p><p><a href="#sink">https://flume.apache.org/FlumeDeveloperGuide.html#sink</a>根据官方说明自定义MySink需要继承AbstractSink类并实现Configurable接口。</p><p>实现相应方法：</p><p><code>configure(Context context)</code>//初始化context（读取配置文件内容）</p><p><code>process()</code>//从Channel读取获取数据（event），这个方法将被循环调用。</p><p>使用场景：读取Channel数据写入MySQL或者其他文件系统。</p><h4 id="7-2-需求"><a href="#7-2-需求" class="headerlink" title="7.2 需求"></a>7.2 需求</h4><p>使用flume接收数据，并在Sink端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume任务配置文件中配置。</p><p>流程分析</p><p><img src="https://i.loli.net/2020/10/27/e4PE23m7rh9nKyg.png"></p><h4 id="7-3-编码"><a href="#7-3-编码" class="headerlink" title="7.3 编码"></a>7.3 编码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Logger对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(AbstractSink.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前Sink绑定的Channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取Channel中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> String(event.getBody()) + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefix&quot;</span>, <span class="string">&quot;hello:&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">&quot;suffix&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="7-4-测试"><a href="#7-4-测试" class="headerlink" title="7.4 测试"></a>7.4 测试</h4><ol><li><p>打包</p><p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p></li><li><p>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; com.atguigu.MySink</span><br><span class="line">#a1.sinks.k1.prefix &#x3D; atguigu:</span><br><span class="line">a1.sinks.k1.suffix &#x3D; :atguigu</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></li><li><p>开启任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ pwd</span><br><span class="line">/opt/module/flume</span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">[atguigu@hadoop102 ~]$ nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br><span class="line">atguigu</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><p><img src="https://i.loli.net/2020/10/27/ILe48pEPrfyzbus.png"></p></li></ol><h3 id="第八章-企业真实面试题（重点）"><a href="#第八章-企业真实面试题（重点）" class="headerlink" title="第八章 企业真实面试题（重点）"></a>第八章 企业真实面试题（重点）</h3><h4 id="8-1-你是如何实现Flume数据传输的监控的"><a href="#8-1-你是如何实现Flume数据传输的监控的" class="headerlink" title="8.1 你是如何实现Flume数据传输的监控的"></a>8.1 你是如何实现Flume数据传输的监控的</h4><p>​    使用第三方框架Ganglia实时监控Flume</p><h4 id="8-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？"><a href="#8-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？" class="headerlink" title="8.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？"></a>8.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？</h4><p><strong>1.作用</strong></p><p>（1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</p><p>（2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中</p><p>（3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</p><p>2.我们公司采用的Source类型为：</p><p>（1）监控后台日志：exec</p><p>（2）监控后台产生日志的端口：netcat</p><p><font color="red">exec spooldir</font></p><h4 id="8-3-Flume的Channel-Selectors"><a href="#8-3-Flume的Channel-Selectors" class="headerlink" title="8.3 Flume的Channel Selectors"></a>8.3 Flume的Channel Selectors</h4><p><img src="https://i.loli.net/2020/10/27/b3ym6Jj5s7rlxTq.png"></p><h4 id="8-4-Flume参数调优"><a href="#8-4-Flume参数调优" class="headerlink" title="8.4 Flume参数调优"></a>8.4 Flume参数调优</h4><ol><li><strong>Source</strong></li></ol><p>增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</p><p>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</p><ol start="2"><li><strong>Channel</strong> </li></ol><p>type 选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。</p><p>使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</p><p>Capacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</p><ol start="3"><li><strong>Sink</strong> </li></ol><p>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</p><p>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</p><h4 id="8-5-Flume的事务机制"><a href="#8-5-Flume的事务机制" class="headerlink" title="8.5 Flume的事务机制"></a>8.5 Flume的事务机制</h4><p>Flume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。</p><h4 id="8-6-Flume采集数据会丢失吗"><a href="#8-6-Flume采集数据会丢失吗" class="headerlink" title="8.6 Flume采集数据会丢失吗"></a>8.6 Flume采集数据会丢失吗</h4><p>不会，Channel存储可以存储在File中，数据传输自身有事务。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 概述 / 第二章 快速入门 / 第三章 企业开发案例 / 第四章 监测控制 / 第五章 自定义Interceptor /&lt;br&gt;第六章 自定义Source / 第七章 自定义Sink / 第八章 企业真实面试题&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Shell</title>
    <link href="http://luo6656.github.io/2020/01/03/Linux/Shell/"/>
    <id>http://luo6656.github.io/2020/01/03/Linux/Shell/</id>
    <published>2020-01-02T16:00:00.000Z</published>
    <updated>2020-10-27T05:37:37.675Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 Shell概述 / 第二章 Shell解析器 /<br>第三章 Shell脚本入门 / 第四章 Shell中的变量/<br>第五章 运算符 / 第六章 条件判断/<br>第七章 流程控制（重点） / 第八章 read读取控制台输入/<br>第九章 函数 / 第十章 Shell工具（重点）/<br>第十一章 企业真实面试题/</p><a id="more"></a><h1 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h1><h3 id="一、Shell概述"><a href="#一、Shell概述" class="headerlink" title="一、Shell概述"></a>一、Shell概述</h3><p>Shell 是一个<font color="red">命令行解释器</font>, 它接收应用程序/用户命令，然后调用操作系统内核。</p><p>Shell 还是一个功能强大的编程语言，易编写，易调试，灵活性强</p><h3 id="二、Shell解析器"><a href="#二、Shell解析器" class="headerlink" title="二、Shell解析器"></a>二、Shell解析器</h3><p>（1）Linux提供的Shell解析器有：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ cat /etc/shells </span><br><span class="line"></span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/bin/dash</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure><p>（2）bash和sh的关系</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 bin]$ ll | grep bash</span><br><span class="line"></span><br><span class="line">-rwxr-xr-x. 1 root root 941880 5月  11 2016 bash</span><br><span class="line"></span><br><span class="line">lrwxrwxrwx. 1 root root    4 5月  27 2017 sh -&gt; bash</span><br></pre></td></tr></table></figure><p>（3）Centos默认的解析器是bash</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ echo $SHELL</span><br><span class="line"></span><br><span class="line">&#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><h3 id="三、Shell脚本入门"><a href="#三、Shell脚本入门" class="headerlink" title="三、Shell脚本入门"></a>三、Shell脚本入门</h3><p>1．脚本格式</p><p>脚本以#!/bin/bash开头（指定解析器）</p><p>2．第一个Shell脚本：helloworld</p><p>（1）需求：创建一个Shell脚本，输出helloworld</p><p>（2）案例实操：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch helloworld.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vi helloworld.sh</span><br><span class="line"></span><br><span class="line">在helloworld.sh中输入如下内容</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">echo &quot;helloworld&quot;</span><br></pre></td></tr></table></figure><p>（3）脚本的常用执行方式</p><p>第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）</p><p>​    sh+脚本的相对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ sh helloworld.sh </span><br><span class="line"></span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure><p>​    sh+脚本的绝对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ sh &#x2F;home&#x2F;atguigu&#x2F;datas&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">helloworld</span><br></pre></td></tr></table></figure><p>​    bash+脚本的相对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ bash helloworld.sh </span><br><span class="line"></span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure><p>​    bash+脚本的绝对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ bash &#x2F;home&#x2F;atguigu&#x2F;datas&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure><p>第二种：采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p><p>（a）首先要赋予helloworld.sh 脚本的+x权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ chmod 777 helloworld.sh</span><br></pre></td></tr></table></figure><p>（b）执行脚本</p><p>相对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure><p>绝对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ &#x2F;home&#x2F;atguigu&#x2F;datas&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure><p>注意：<font color="red">第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。</font></p><p>3．第二个Shell脚本：多命令处理</p><p>（1）需求： </p><p>在/home/atguigu/目录下创建一个banzhang.txt,在banzhang.txt文件中增加“I love cls”。</p><p>（2）案例实操：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch batch.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vi batch.sh</span><br></pre></td></tr></table></figure><p>在batch.sh中输入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">cd &#x2F;home&#x2F;atguigu</span><br><span class="line"></span><br><span class="line">touch cls.txt</span><br><span class="line"></span><br><span class="line">echo &quot;I love cls&quot; &gt;&gt;cls.txt</span><br></pre></td></tr></table></figure><h3 id="四、Shell中的变量"><a href="#四、Shell中的变量" class="headerlink" title="四、Shell中的变量"></a>四、Shell中的变量</h3><h4 id="4-1-系统变量"><a href="#4-1-系统变量" class="headerlink" title="4.1 系统变量"></a>4.1 系统变量</h4><ol><li>常用系统变量</li></ol><p>$HOME、$PWD、$SHELL、$USER等</p><p>​    2．案例实操</p><p>（1）查看系统变量的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ echo $HOME</span><br><span class="line"></span><br><span class="line">&#x2F;home&#x2F;atguigu</span><br></pre></td></tr></table></figure><p>（2）显示当前Shell中所有变量：set</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ set</span><br><span class="line"></span><br><span class="line">BASH&#x3D;&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">BASH_ALIASES&#x3D;()</span><br><span class="line"></span><br><span class="line">BASH_ARGC&#x3D;()</span><br><span class="line"></span><br><span class="line">BASH_ARGV&#x3D;()</span><br></pre></td></tr></table></figure><h4 id="4-2-自定义变量"><a href="#4-2-自定义变量" class="headerlink" title="4.2 自定义变量"></a>4.2 自定义变量</h4><p>1．基本语法</p><p>（1）定义变量：变量=值 </p><p>（2）撤销变量：unset 变量</p><p>（3）声明静态变量：readonly变量，注意：不能unset</p><p>2．变量定义规则</p><p>​    （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。</p><p>​    （2）等号两侧不能有空格</p><p>​    （3）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。</p><p>​    （4）变量的值如果有空格，需要使用双引号或单引号括起来。</p><p>3．案例实操</p><p>​    （1）定义变量A</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ A&#x3D;5</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $A</span><br><span class="line"></span><br><span class="line">5</span><br></pre></td></tr></table></figure><p>​    （2）给变量A重新赋值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ A&#x3D;8</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $A</span><br><span class="line"></span><br><span class="line">8</span><br></pre></td></tr></table></figure><p>​    （3）撤销变量A</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ unset A</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $A</span><br></pre></td></tr></table></figure><p>​    （4）声明静态的变量B=2，不能unset</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ readonly B&#x3D;2</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $B</span><br><span class="line"></span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ B&#x3D;9</span><br><span class="line"></span><br><span class="line">-bash: B: readonly variable</span><br></pre></td></tr></table></figure><p>​    （5）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ C&#x3D;1+2</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 ~]$ echo $C</span><br><span class="line"></span><br><span class="line">1+2</span><br></pre></td></tr></table></figure><p>（6）变量的值如果有空格，需要使用双引号或单引号括起来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ D&#x3D;I love banzhang</span><br><span class="line"></span><br><span class="line">-bash: world: command not found</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 ~]$ D&#x3D;&quot;I love banzhang&quot;</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 ~]$ echo $A</span><br><span class="line"></span><br><span class="line">I love banzhang</span><br></pre></td></tr></table></figure><p>​    （7）可把变量提升为全局环境变量，可供其他Shell程序使用</p><p>export 变量名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ vim helloworld.sh </span><br><span class="line"></span><br><span class="line">在helloworld.sh文件中增加echo $B</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">echo &quot;helloworld&quot;</span><br><span class="line"></span><br><span class="line">echo $B</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure><p>发现并没有打印输出变量B的值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ export B</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">helloworld</span><br><span class="line"></span><br><span class="line">2</span><br></pre></td></tr></table></figure><h4 id="4-3-特殊变量：-n"><a href="#4-3-特殊变量：-n" class="headerlink" title="4.3 特殊变量：$n"></a>4.3 特殊变量：$n</h4><p>1．基本语法</p><p>​    $n    （功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}）</p><p>2．案例实操</p><p>（1）输出该脚本文件名称、输入参数1和输入参数2 的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch parameter.sh </span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim parameter.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">echo &quot;$0  $1  $2&quot;</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 parameter.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;parameter.sh cls  xz</span><br><span class="line"></span><br><span class="line">.&#x2F;parameter.sh  cls  xz</span><br></pre></td></tr></table></figure><h4 id="4-4-特殊变量："><a href="#4-4-特殊变量：" class="headerlink" title="4.4 特殊变量：$#"></a>4.4 特殊变量：$#</h4><p>1．基本语法</p><p>​    $#    （功能描述：获取所有输入参数个数，常用于循环）。</p><p>2．案例实操</p><p>（1）获取输入参数的个数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ vim parameter.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">echo &quot;$0  $1  $2&quot;</span><br><span class="line">echo $#</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 parameter.sh</span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;parameter.sh cls  xz</span><br><span class="line"></span><br><span class="line">parameter.sh cls xz </span><br><span class="line"></span><br><span class="line">2</span><br></pre></td></tr></table></figure><h4 id="4-5-特殊变量：-、"><a href="#4-5-特殊变量：-、" class="headerlink" title="4.5 特殊变量：$*、$@"></a>4.5 特殊变量：$*、$@</h4><p>1．基本语法</p><ul><li>​    $*    （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）</li><li>​    $@    （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）</li></ul><p>2．案例实操</p><p>（1）打印输入的所有参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ vim parameter.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">echo &quot;$0  $1  $2&quot;</span><br><span class="line">echo $#</span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ bash parameter.sh 1 2 3</span><br><span class="line"></span><br><span class="line">parameter.sh  1  2</span><br><span class="line"></span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">1 2 3</span><br><span class="line"></span><br><span class="line">1 2 3</span><br></pre></td></tr></table></figure><h4 id="4-6-特殊变量："><a href="#4-6-特殊变量：" class="headerlink" title="4.6 特殊变量：$?"></a>4.6 特殊变量：$?</h4><p>1．基本语法</p><p>$？    （功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。）</p><p>2．案例实操</p><p>​    （1）判断helloworld.sh脚本是否正确执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;helloworld.sh </span><br><span class="line"></span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $?</span><br><span class="line"></span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="五、运算符"><a href="#五、运算符" class="headerlink" title="五、运算符"></a>五、运算符</h3><p>1．基本语法</p><p>（1）“$((运算式))”或“$[运算式]”</p><p>（2）expr  + , - , *,  /,  %   加，减，乘，除，取余</p><p>注意：expr运算符间要有空格</p><p>2．案例实操： </p><p>（1）计算3+2的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ expr 2 + 3</span><br><span class="line"></span><br><span class="line">5</span><br></pre></td></tr></table></figure><p>（2）计算3-2的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ expr 3 - 2 </span><br><span class="line"></span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>（3）计算（2+3）X4的值</p><p>​    （a）expr一步完成计算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ expr &#39;expr 2 + 3&#39; \* 4</span><br><span class="line"></span><br><span class="line">20</span><br></pre></td></tr></table></figure><p>（b）采用$[运算式]方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]# S&#x3D;$[(2+3)*4]</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]# echo $S</span><br></pre></td></tr></table></figure><h3 id="六、条件判断"><a href="#六、条件判断" class="headerlink" title="六、条件判断"></a>六、条件判断</h3><p>​    1．基本语法</p><p>[ condition ]（注意condition前后要有空格）</p><p>注意：条件非空即为true，[ atguigu ]返回true，[] 返回false。</p><ol start="2"><li>常用判断条件</li></ol><p>（1）两个整数之间比较</p><ul><li>= 字符串比较</li><li>-lt 小于（less than）            </li><li>-le 小于等于（less equal）</li><li>-eq 等于（equal）                </li><li>-gt 大于（greater than）</li><li>-ge 大于等于（greater equal）    </li><li>-ne 不等于（Not equal）</li></ul><p>（2）按照文件权限进行判断</p><ul><li>-r 有读的权限（read）            </li><li>-w 有写的权限（write）</li><li>-x 有执行的权限（execute）</li></ul><p>（3）按照文件类型进行判断</p><ul><li>-f 文件存在并且是一个常规的文件（file）</li><li>-e 文件存在（existence）        </li><li>-d 文件存在并是一个目录（directory）</li></ul><p>3．案例实操</p><p>​    （1）23是否大于等于22</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ [ 23 -ge 22 ]</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $?</span><br><span class="line"></span><br><span class="line">0</span><br></pre></td></tr></table></figure><p>​    （2）helloworld.sh是否具有写权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ [ -w helloworld.sh ]</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $?</span><br><span class="line"></span><br><span class="line">0</span><br></pre></td></tr></table></figure><p>​    （3）/home/atguigu/cls.txt目录中的文件是否存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ [ -e &#x2F;home&#x2F;atguigu&#x2F;cls.txt ]</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ echo $?</span><br><span class="line"></span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>（4）多条件判断（&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ [ condition ] &amp;&amp; echo OK || echo notok</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ [ condition ] &amp;&amp; [ ] || echo notok</span><br><span class="line">notok</span><br></pre></td></tr></table></figure><h3 id="七、流程控制（重点）"><a href="#七、流程控制（重点）" class="headerlink" title="七、流程控制（重点）"></a>七、流程控制（重点）</h3><h4 id="7-1-if判断"><a href="#7-1-if判断" class="headerlink" title="7.1 if判断"></a>7.1 if判断</h4><p>1．基本语法</p><p>if [ 条件判断式 ];then </p><p> 程序 </p><p>fi </p><p>或者 </p><p>if [ 条件判断式 ] </p><p> then </p><p>  程序 </p><p>elif [ 条件判断式 ]</p><p>​    then</p><p>​        程序</p><p>else</p><p>​    程序</p><p>fi</p><p>​    注意事项：</p><p>（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格</p><p>（2）if后要有空格</p><p>2．案例实操</p><p>（1）输入一个数字，如果是1，则输出banzhang zhen shuai，如果是2，则输出cls zhen mei，如果是其它，什么也不输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch if.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim if.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">if [ $1 -eq &quot;1&quot; ]</span><br><span class="line"></span><br><span class="line">then</span><br><span class="line">    echo &quot;banzhang zhen shuai&quot;</span><br><span class="line">elif [ $1 -eq &quot;2&quot; ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;cls zhen mei&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ chmod 777 if.sh </span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;if.sh 1</span><br><span class="line"></span><br><span class="line">banzhang zhen shuai</span><br></pre></td></tr></table></figure><h4 id="7-2-case语句"><a href="#7-2-case语句" class="headerlink" title="7.2 case语句"></a>7.2 case语句</h4><p>1．基本语法</p><p>case $变量名 in </p><p> “值1”） </p><p>  如果变量的值等于值1，则执行程序1 </p><p>  ;; </p><p> “值2”） </p><p>  如果变量的值等于值2，则执行程序2 </p><p>  ;; </p><p> …省略其他分支… </p><p> *） </p><p>  如果变量的值都不是以上的值，则执行此程序 </p><p>  ;; </p><p>esac</p><p>注意事项：</p><ul><li><ol><li>case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。</li></ol></li><li><ol start="2"><li>双分号“*<strong>*;;**</strong>”表示命令序列结束，相当于java中的break。</li></ol></li><li><ol start="3"><li>最后的“*）”表示默认模式，相当于java中的default。</li></ol></li></ul><p>2．案例实操</p><p>（1）输入一个数字，如果是1，则输出banzhang，如果是2，则输出cls，如果是其它，输出renyao。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch case.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim case.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">case $1 in</span><br><span class="line">&quot;1&quot;)</span><br><span class="line">    echo &quot;banzhang&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;2&quot;)</span><br><span class="line">    echo &quot;cls&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;renyao&quot;</span><br><span class="line">;;</span><br><span class="line"></span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 case.sh</span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;case.sh 1</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="7-3-for循环"><a href="#7-3-for循环" class="headerlink" title="7.3 for循环"></a>7.3 for循环</h4><p>1．基本语法1</p><p>​    for (( 初始值;循环控制条件;变量变化 )) </p><p> do </p><p>  程序 </p><p> done</p><p>2．案例实操</p><p>（1）从1加到100</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch for1.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim for1.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">s&#x3D;0</span><br><span class="line">for((i&#x3D;0;i&lt;&#x3D;100;i++))</span><br><span class="line">do</span><br><span class="line">    s&#x3D;$[$s+$i]</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo $s</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 for1.sh </span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;for1.sh </span><br><span class="line"></span><br><span class="line">“5050”</span><br></pre></td></tr></table></figure><p>3．基本语法2</p><p>for 变量 in 值1 值2 值3… </p><p> do </p><p>  程序 </p><p> done</p><p>4．案例实操</p><p>​    （1）打印所有输入参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch for2.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim for2.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#打印数字</span><br><span class="line"></span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">   echo &quot;ban zhang love $i &quot;</span><br><span class="line"></span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 for2.sh </span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ bash for2.sh cls xz bd</span><br><span class="line"></span><br><span class="line">ban zhang love cls</span><br><span class="line"></span><br><span class="line">ban zhang love xz</span><br><span class="line"></span><br><span class="line">ban zhang love bd</span><br></pre></td></tr></table></figure><p>（2）比较$*和$@区别</p><ul><li>$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。</li><li>当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ vim for.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash </span><br><span class="line">for i in &quot;$*&quot; </span><br><span class="line">#$*中的所有参数看成是一个整体，所以这个for循环只会循环一次 </span><br><span class="line"></span><br><span class="line">do </span><br><span class="line">     echo &quot;ban zhang love $i&quot;</span><br><span class="line">done </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">for j in &quot;$@&quot; </span><br><span class="line"></span><br><span class="line">#$@中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次 </span><br><span class="line"></span><br><span class="line">do </span><br><span class="line">     echo &quot;ban zhang love $j&quot; </span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 for.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ bash for.sh cls xz bd</span><br><span class="line"></span><br><span class="line">ban zhang love cls xz bd</span><br><span class="line"></span><br><span class="line">ban zhang love cls</span><br><span class="line"></span><br><span class="line">ban zhang love xz</span><br><span class="line"></span><br><span class="line">ban zhang love bd</span><br></pre></td></tr></table></figure><h4 id="7-4-while循环"><a href="#7-4-while循环" class="headerlink" title="7.4 while循环"></a>7.4 while循环</h4><p>1．基本语法</p><p>while [ 条件判断式 ] </p><p> do </p><p>  程序</p><p> done</p><p>2．案例实操</p><p>​    （1）从1加到100</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch while.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim while.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">s&#x3D;0</span><br><span class="line">i&#x3D;1</span><br><span class="line">while [ $i -le 100 ]</span><br><span class="line">do</span><br><span class="line">    s&#x3D;$[$s+$i]</span><br><span class="line">    i&#x3D;$[$i+1]</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo $s</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 while.sh </span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;while.sh </span><br><span class="line"></span><br><span class="line">5050</span><br></pre></td></tr></table></figure><h3 id="八、read读取控制台输入"><a href="#八、read读取控制台输入" class="headerlink" title="八、read读取控制台输入"></a>八、read读取控制台输入</h3><p>1．基本语法</p><p>​    read(选项)(参数)</p><p>​    选项：</p><p>-p：指定读取值时的提示符；</p><p>-t：指定读取值时等待的时间（秒）。</p><p>参数</p><p>​    变量：指定读取值的变量名</p><p>2．案例实操</p><p>​    （1）提示7秒内，读取控制台输入的名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch read.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim read.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">read -t 7 -p &quot;Enter your name in 7 seconds &quot; NAME</span><br><span class="line"></span><br><span class="line">echo $NAME</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;read.sh </span><br><span class="line"></span><br><span class="line">Enter your name in 7 seconds xiaoze</span><br><span class="line"></span><br><span class="line">xiaoze</span><br></pre></td></tr></table></figure><h3 id="九、函数"><a href="#九、函数" class="headerlink" title="九、函数"></a>九、函数</h3><h4 id="9-1-系统函数"><a href="#9-1-系统函数" class="headerlink" title="9.1 系统函数"></a>9.1 系统函数</h4><p>1．basename基本语法</p><p>basename [string / pathname] [suffix]  （功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。</p><p>选项：</p><p>suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。</p><p>2．案例实操</p><p>（1）截取该/home/atguigu/banzhang.txt路径的文件名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ basename &#x2F;home&#x2F;atguigu&#x2F;banzhang.txt </span><br><span class="line"></span><br><span class="line">banzhang.txt</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ basename &#x2F;home&#x2F;atguigu&#x2F;banzhang.txt .txt</span><br><span class="line"></span><br><span class="line">banzhang</span><br></pre></td></tr></table></figure><ol start="3"><li>   dirname基本语法</li></ol><p>​    dirname 文件绝对路径        （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分））</p><p>4．案例实操</p><p>（1）获取banzhang.txt文件的路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ dirname &#x2F;home&#x2F;atguigu&#x2F;banzhang.txt </span><br><span class="line"></span><br><span class="line">&#x2F;home&#x2F;atguigu</span><br></pre></td></tr></table></figure><h4 id="9-2-自定义函数"><a href="#9-2-自定义函数" class="headerlink" title="9.2 自定义函数"></a>9.2 自定义函数</h4><p>1．基本语法</p><p>[ function ] funname[()]</p><p>{</p><p>​    Action;</p><p>​    [return int;]</p><p>}</p><p>funname</p><p>2．经验技巧</p><p>​    （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。</p><p>​    （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255)</p><p>3．案例实操</p><p>​    （1）计算两个输入参数的和</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch fun.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim fun.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">function sum()</span><br><span class="line">&#123;</span><br><span class="line">  s&#x3D;0</span><br><span class="line">  s&#x3D;$[ $1 + $2 ]</span><br><span class="line">  echo &quot;$s&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">read -p &quot;Please input the number1: &quot; n1;</span><br><span class="line">read -p &quot;Please input the number2: &quot; n2;</span><br><span class="line">sum $n1 $n2; </span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ chmod 777 fun.sh</span><br><span class="line">[atguigu@hadoop101 datas]$ .&#x2F;fun.sh </span><br><span class="line"></span><br><span class="line">Please input the number1: 2</span><br><span class="line">Please input the number2: 5</span><br><span class="line"></span><br><span class="line">7</span><br></pre></td></tr></table></figure><h3 id="十、Shell工具（重点）"><a href="#十、Shell工具（重点）" class="headerlink" title="十、Shell工具（重点）"></a>十、Shell工具（重点）</h3><h4 id="10-1-cut"><a href="#10-1-cut" class="headerlink" title="10.1 cut"></a>10.1 cut</h4><p>​        cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。</p><p>1.基本用法</p><p>cut [选项参数]  filename</p><p>说明：默认分隔符是制表符</p><p>2.选项参数说明</p><p>表1-55</p><table><thead><tr><th>选项参数</th><th>功能</th></tr></thead><tbody><tr><td>-f</td><td>列号，提取第几列</td></tr><tr><td>-d</td><td>分隔符，按照指定分隔符分割列</td></tr><tr><td>-c</td><td>指定具体的字符</td></tr></tbody></table><p>3.案例实操</p><p>（0）数据准备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ touch cut.txt</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 datas]$ vim cut.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><p>（1）切割cut.txt第一列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ cut -d &quot; &quot; -f 1 cut.txt </span><br><span class="line"></span><br><span class="line">dong</span><br><span class="line">guan</span><br><span class="line">wo</span><br><span class="line">lai</span><br><span class="line">le</span><br></pre></td></tr></table></figure><p>（2）切割cut.txt第二、三列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ cut -d &quot; &quot; -f 2,3 cut.txt </span><br><span class="line"></span><br><span class="line">shen</span><br><span class="line">zhen</span><br><span class="line">wo</span><br><span class="line">lai</span><br><span class="line">le</span><br></pre></td></tr></table></figure><p>（3）在cut.txt文件中切割出guan</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ cat cut.txt | grep &quot;guan&quot; | cut -d &quot; &quot; -f 1</span><br><span class="line">guan</span><br></pre></td></tr></table></figure><p>（4）选取系统PATH变量值，第2个“：”开始后的所有路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ echo $PATH</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;sbin:&#x2F;home&#x2F;atguigu&#x2F;bin</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 datas]$ echo $PATH | cut -d: -f 2-</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;sbin:&#x2F;home&#x2F;atguigu&#x2F;bin</span><br></pre></td></tr></table></figure><p>（5）切割ifconfig 后打印的IP地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 datas]$ ifconfig eth0 | grep &quot;inet addr&quot; | cut -d: -f 2 | cut -d&quot; &quot; -f1</span><br><span class="line"></span><br><span class="line">192.168.1.102</span><br></pre></td></tr></table></figure><h4 id="10-2-sed"><a href="#10-2-sed" class="headerlink" title="10.2 sed"></a>10.2 sed</h4><p>​        sed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。</p><ol><li>基本用法</li></ol><p>sed [选项参数]  ‘command’  filename</p><ol start="2"><li>选项参数说明</li></ol><table><thead><tr><th>选项参数</th><th>功能</th></tr></thead><tbody><tr><td>-e</td><td>直接在指令列模式上进行sed的动作编辑。</td></tr><tr><td>-i</td><td>直接编辑文件</td></tr></tbody></table><ol start="3"><li>命令功能描述</li></ol><table><thead><tr><th>命令</th><th>功能描述</th></tr></thead><tbody><tr><td><strong>a</strong></td><td>新增，a的后面可以接字串，在下一行出现</td></tr><tr><td>d</td><td>删除</td></tr><tr><td>s</td><td>查找并替换</td></tr></tbody></table><ol start="4"><li>案例实操</li></ol><p>（0）数据准备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch sed.txt</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 datas]$ vim sed.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><p>（1）将“mei nv”这个单词插入到sed.txt第二行下，打印。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ sed &#39;2a mei nv&#39; sed.txt </span><br><span class="line"></span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">mei nv</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ cat sed.txt </span><br><span class="line"></span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><p>注意：文件并没有改变</p><p>（2）删除sed.txt文件所有包含wo的行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ sed &#39;&#x2F;wo&#x2F;d&#39; sed.txt</span><br><span class="line"></span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><p>（3）将sed.txt文件中wo替换为ni</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ sed &#39;s&#x2F;wo&#x2F;ni&#x2F;g&#39; sed.txt </span><br><span class="line"></span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">ni  ni</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><p>注意：‘g’表示global，全部替换</p><p>（4）将sed.txt文件中的第二行删除并将wo替换为ni</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ sed -e &#39;2d&#39; -e &#39;s&#x2F;wo&#x2F;ni&#x2F;g&#39; sed.txt </span><br><span class="line"></span><br><span class="line">dong shen</span><br><span class="line">ni  ni</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure><h4 id="10-3-awk"><a href="#10-3-awk" class="headerlink" title="10.3 awk"></a>10.3 awk</h4><p>一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。</p><ol><li>基本用法</li></ol><p>awk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename</p><p>pattern：表示AWK在数据中查找的内容，就是匹配模式</p><p>action：在找到匹配内容时所执行的一系列命令</p><ol start="2"><li>选项参数说明</li></ol><table><thead><tr><th>选项参数</th><th>功能</th></tr></thead><tbody><tr><td>-F</td><td>指定输入文件折分隔符</td></tr><tr><td>-v</td><td>赋值一个用户定义变量</td></tr></tbody></table><ol start="3"><li>案例实操</li></ol><p>（0）数据准备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ sudo cp &#x2F;etc&#x2F;passwd .&#x2F;</span><br></pre></td></tr></table></figure><p>（1）搜索passwd文件以root关键字开头的所有行，并输出该行的第7列。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk -F: &#39;&#x2F;^root&#x2F;&#123;print $7&#125;&#39; passwd </span><br><span class="line"></span><br><span class="line">&#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><p>（2）搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk -F: &#39;&#x2F;^root&#x2F;&#123;print $1&quot;,&quot;$7&#125;&#39; passwd </span><br><span class="line"></span><br><span class="line">root,&#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><p>注意：只有匹配了pattern的行才会执行action</p><p>（3）只显示/etc/passwd的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在最后一行添加”dahaige，/bin/zuishuai”。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk -F : &#39;BEGIN&#123;print &quot;user, shell&quot;&#125; &#123;print $1&quot;,&quot;$7&#125; END&#123;print &quot;dahaige,&#x2F;bin&#x2F;zuishuai&quot;&#125;&#39; passwd</span><br><span class="line"></span><br><span class="line">user, shell</span><br><span class="line"></span><br><span class="line">root,&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">bin,&#x2F;sbin&#x2F;nologin</span><br><span class="line"></span><br><span class="line">atguigu,&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">dahaige,&#x2F;bin&#x2F;zuishuai</span><br></pre></td></tr></table></figure><p>注意：BEGIN 在所有数据读取行之前执行；END 在所有数据执行之后执行。</p><p>（4）将passwd文件中的用户id增加数值1并输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk -v i&#x3D;1 -F: &#39;&#123;print $3+i&#125;&#39; passwd</span><br><span class="line"></span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td></tr></table></figure><ol start="4"><li>awk的内置变量</li></ol><table><thead><tr><th>变量</th><th>说明</th></tr></thead><tbody><tr><td>FILENAME</td><td>文件名</td></tr><tr><td>NR</td><td>已读的记录数</td></tr><tr><td>NF</td><td>浏览记录的域的个数（切割后，列的个数）</td></tr></tbody></table><ol start="5"><li>案例实操</li></ol><p>（1）统计passwd文件名，每行的行号，每行的列数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk -F: &#39;&#123;print &quot;filename:&quot;  FILENAME &quot;, linenumber:&quot; NR  &quot;,columns:&quot; NF&#125;&#39; passwd </span><br><span class="line"></span><br><span class="line">filename:passwd, linenumber:1,columns:7</span><br><span class="line"></span><br><span class="line">filename:passwd, linenumber:2,columns:7</span><br><span class="line"></span><br><span class="line">filename:passwd, linenumber:3,columns:7</span><br></pre></td></tr></table></figure><p>​     （2）切割IP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ ifconfig eth0 | grep &quot;inet addr&quot; | awk -F: &#39;&#123;print $2&#125;&#39; | awk -F &quot; &quot; &#39;&#123;print $1&#125;&#39; </span><br><span class="line"></span><br><span class="line">192.168.1.102</span><br></pre></td></tr></table></figure><p>​     （3）查询sed.txt中空行所在的行号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk &#39;&#x2F;^$&#x2F;&#123;print NR&#125;&#39; sed.txt </span><br><span class="line"></span><br><span class="line">5</span><br></pre></td></tr></table></figure><h4 id="10-4-sort"><a href="#10-4-sort" class="headerlink" title="10.4 sort"></a>10.4 sort</h4><p>sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。</p><ol><li>基本语法</li></ol><p>sort(选项)(参数)</p><table><thead><tr><th>选项</th><th>说明</th></tr></thead><tbody><tr><td>-n</td><td>依照数值的大小排序</td></tr><tr><td>-r</td><td>以相反的顺序来排序</td></tr><tr><td>-t</td><td>设置排序时所用的分隔字符</td></tr><tr><td>-k</td><td>指定需要排序的列</td></tr></tbody></table><p>参数：指定待排序的文件列表</p><ol start="2"><li>案例实操</li></ol><p>（0）数据准备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch sort.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 datas]$ vim sort.sh </span><br><span class="line"></span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">xz:50:2.3</span><br><span class="line">cls:10:3.5</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure><p>（1）按照“：”分割后的第三列倒序排序。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ sort -t : -nrk 3  sort.sh </span><br><span class="line"></span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">cls:10:3.5</span><br><span class="line">xz:50:2.3</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure><h3 id="十一、企业真实面试题"><a href="#十一、企业真实面试题" class="headerlink" title="十一、企业真实面试题"></a>十一、企业真实面试题</h3><h4 id="11-1-京东"><a href="#11-1-京东" class="headerlink" title="11.1 京东"></a>11.1 京东</h4><p>问题1：使用Linux命令查询file1中空行所在的行号</p><p>答案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ awk &#39;&#x2F;^$&#x2F;&#123;print NR&#125;&#39; sed.txt </span><br><span class="line"></span><br><span class="line">5</span><br></pre></td></tr></table></figure><p>问题2：有文件chengji.txt内容如下:</p><p>张三 40</p><p>李四 50</p><p>王五 60</p><p>使用Linux命令计算第二列的和并输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ cat chengji.txt | awk -F &quot; &quot; &#39;&#123;sum+&#x3D;$2&#125; END&#123;print sum&#125;&#39;</span><br><span class="line"></span><br><span class="line">150</span><br></pre></td></tr></table></figure><h4 id="11-2-搜狐-amp-和讯网"><a href="#11-2-搜狐-amp-和讯网" class="headerlink" title="11.2 搜狐&amp;和讯网"></a>11.2 搜狐&amp;和讯网</h4><p>问题1：Shell脚本里如何检查一个文件是否存在？如果不存在该如何处理？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">if [ -f file.txt ]; then</span><br><span class="line">  echo &quot;文件存在!&quot;</span><br><span class="line">else</span><br><span class="line">  echo &quot;文件不存在!&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h4 id="11-3-新浪"><a href="#11-3-新浪" class="headerlink" title="11.3 新浪"></a>11.3 新浪</h4><p>问题1：用shell写一个脚本，对文本中无序的一列数字排序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS6-2 ~]# cat test.txt</span><br><span class="line">9</span><br><span class="line">8</span><br><span class="line">7</span><br><span class="line">6</span><br><span class="line">5</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">10</span><br><span class="line">1</span><br><span class="line">[root@CentOS6-2 ~]# sort -n test.txt|awk &#39;&#123;a+&#x3D;$0;print $0&#125;END&#123;print &quot;SUM&#x3D;&quot;a&#125;&#39;</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">SUM&#x3D;55</span><br></pre></td></tr></table></figure><h4 id="11-4-金和网络"><a href="#11-4-金和网络" class="headerlink" title="11.4 金和网络"></a>11.4 金和网络</h4><p>问题1：请用shell脚本写出查找当前文件夹（/home）下所有的文本文件内容中包含有字符”shen”的文件名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ grep -r &quot;shen&quot; &#x2F;home | cut -d &quot;:&quot; -f 1</span><br><span class="line">&#x2F;home&#x2F;atguigu&#x2F;datas&#x2F;sed.txt</span><br><span class="line">&#x2F;home&#x2F;atguigu&#x2F;datas&#x2F;cut.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 Shell概述 / 第二章 Shell解析器 /&lt;br&gt;第三章 Shell脚本入门 / 第四章 Shell中的变量/&lt;br&gt;第五章 运算符 / 第六章 条件判断/&lt;br&gt;第七章 流程控制（重点） / 第八章 read读取控制台输入/&lt;br&gt;第九章 函数 / 第十章 Shell工具（重点）/&lt;br&gt;第十一章 企业真实面试题/&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="http://luo6656.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="http://luo6656.github.io/2019/12/06/Linux/Linux/"/>
    <id>http://luo6656.github.io/2019/12/06/Linux/Linux/</id>
    <published>2019-12-05T16:00:00.000Z</published>
    <updated>2020-10-27T05:26:36.444Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 Linux文件与目录结构 / 第二章 VI/VIM编辑器 /<br>第三章 网络配置和系统管理操作/ 第四章 远程登陆(XShell)/<br>第五章 常用基本命令 / 第六章 软件包管理/<br>第七章 面试题/</p><a id="more"></a><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h3 id="一、Linux文件与目录结构"><a href="#一、Linux文件与目录结构" class="headerlink" title="一、Linux文件与目录结构"></a>一、Linux文件与目录结构</h3><h4 id="1-1-Linux-文件"><a href="#1-1-Linux-文件" class="headerlink" title="1.1 Linux 文件"></a>1.1 Linux 文件</h4><p>Linux系统中一切皆文件</p><h4 id="1-2-Linux目录结构"><a href="#1-2-Linux目录结构" class="headerlink" title="1.2 Linux目录结构"></a>1.2 Linux目录结构</h4><p>注意：前面加/就是绝对路径。前面加./就是从当前开始</p><p><img src="https://i.loli.net/2020/10/27/OgykC8SpD1FsJef.png"></p><ul><li><strong>/bin</strong>:是Binary的缩写，这个目录存放着最经常使用的命令</li><li>/sbin:s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序</li><li><strong>/home</strong>:存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般改目录以用户的账号命名的。</li><li><strong>/root</strong>:该目录为系统管理员，也称作超级权限者的用户主目录</li><li>/lib:系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库</li><li>/lost+found:这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。</li><li><strong>/etc</strong>:所有的系统管理所需要的配置文件和子目录</li><li><strong>/user</strong>:这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于Windows下的program files目录</li><li><strong>/boot</strong>:这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放在这</li><li>/proc:这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统的信息</li><li>/srv:service缩写，该目录存放一些服务启动之后所需要提取的数据</li><li>/sys:这是linux2.6内核的一个很大的变化。改目录下安装了2.6内核中新出现的一个文件系统sysfs</li><li>/tmp:这个目录是用来存放一些临时文件的</li><li>/dev:类似于windows的设备管理器，把所有的硬件用文件的形式存储</li><li><strong>/media</strong>:linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux会把识别的设备挂载到这个目录下。</li><li><strong>/mnt</strong>:系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将外部的存储挂载在/mnt/上，然后进入该目录就可以查看里的内容了</li><li><strong>/opt</strong>:这是给主机额外安装软件所摆放的目录。比如你安装一个mysql数据库则就可以放到这个目录下。默认是空的。</li><li><strong>/var</strong>:这个目录中存放着在不断扩充着的东西，我们习惯将这些经常被修改的目录放在这个目录下。包括各种日志文件。</li><li>/selinux:SELinux是一个安全子系统，他能控制程序只能访问特定文件</li></ul><h3 id="二、VI-VIM编辑器"><a href="#二、VI-VIM编辑器" class="headerlink" title="二、VI/VIM编辑器"></a>二、VI/VIM编辑器</h3><h4 id="2-1-概念"><a href="#2-1-概念" class="headerlink" title="2.1 概念"></a>2.1 概念</h4><ul><li>VI是Unix操作系统和类Unix操作系统中最通用的文本编辑器</li><li>VIM编辑器是从VI发展出来的一个性能更强大的文本编辑器。可以主动的以字体颜色辨别语法的正确性，方便程序设计。VIM与VI编辑器完全兼容。</li></ul><h4 id="2-2-一般模式"><a href="#2-2-一般模式" class="headerlink" title="2.2 一般模式"></a>2.2 一般模式</h4><p>​        以vi打开一个档案就直接进入一般模式了（这是*<strong>*默认的模式**</strong>）。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、贴上』来处理你的文件数据。<font color="red">一般模式的意思就是只能通过命令来做一些东西，并不能写一些东西</font></p><p><img src="https://i.loli.net/2020/10/27/Qt2wiDB4dAJ5SFZ.png" alt="1584342878092"></p><h4 id="2-3-编辑模式"><a href="#2-3-编辑模式" class="headerlink" title="2.3 编辑模式"></a>2.3 编辑模式</h4><p>​        在一般模式中可以进行删除、复制、粘贴等的动作，但是无法编辑文件内容！要等到你按下『i, I, o, O, a, A, r, R』等任何一个字母之后才会进入编辑模式。</p><p>​        注意了！通常在Linux中，按下这些按键时，在画面的左下方会出现『INSERT或 REPLACE』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。</p><p><img src="https://i.loli.net/2020/10/27/yhW6SYZg4f98bsJ.png" alt="1584343750273"></p><h4 id="2-4-指令模式"><a href="#2-4-指令模式" class="headerlink" title="2.4 指令模式"></a>2.4 指令模式</h4><p><strong><em>\</em>在一般模式当中**</strong>，输入『 <strong><em>\</em>: / ?**</strong>』3个中的任何一个按钮，就可以将光标移动到最底下那一行。</p><p>在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等动作是在此模式中达成的！</p><p><img src="https://i.loli.net/2020/10/27/qGdSYIrLU2HsiDm.png" alt="1584343826290"></p><h3 id="三、网络配置和系统管理操作"><a href="#三、网络配置和系统管理操作" class="headerlink" title="三、网络配置和系统管理操作"></a>三、网络配置和系统管理操作</h3><h4 id="3-1-查看网络IP和网关"><a href="#3-1-查看网络IP和网关" class="headerlink" title="3.1 查看网络IP和网关"></a>3.1 查看网络IP和网关</h4><p>1．查看虚拟网络编辑器，如图1-95所示</p><p><img src="https://i.loli.net/2020/10/27/kTavs6Px8ltNGqd.png" alt="img"> </p><p>图1-95  查看虚拟网络编辑器</p><p>2．修改ip地址，如图1-96所示</p><p><img src="https://i.loli.net/2020/10/27/W1jUYyZmbfDHFNt.png" alt="img"> </p><p>图1-96 修改ip地址</p><p>3．查看网关，如图1-97所示</p><p><img src="https://i.loli.net/2020/10/27/QTuHLoFM2EDbeiq.png" alt="img"> </p><p>图1-97 查看网关</p><ol start="4"><li>   查看windows环境的中VMnet8网络配置，如图1-98所示</li></ol><p><img src="https://i.loli.net/2020/10/27/ou4GNXdKlDHz37F.png" alt="img"> </p><p>图1-98  windows中VMnet8网络配置</p><h4 id="3-2-配置网络ip地址"><a href="#3-2-配置网络ip地址" class="headerlink" title="3.2 配置网络ip地址"></a>3.2 配置网络ip地址</h4><h5 id="3-2-1-ifconfig-配置网络接口"><a href="#3-2-1-ifconfig-配置网络接口" class="headerlink" title="3.2.1 ifconfig 配置网络接口"></a>3.2.1 ifconfig 配置网络接口</h5><p>ifconfig :network interfaces configuring网络接口配置</p><p> 1．基本语法</p><p>ifconfig        （功能描述：显示所有网络接口的配置信息）</p><ol start="2"><li>案例实操</li></ol><p>​    （1）查看当前网络ip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]# ifconfig</span><br></pre></td></tr></table></figure><h5 id="3-2-2-ping-测试主机之间网络连通性"><a href="#3-2-2-ping-测试主机之间网络连通性" class="headerlink" title="3.2.2 ping 测试主机之间网络连通性"></a>3.2.2 ping 测试主机之间网络连通性</h5><ol><li>基本语法</li></ol><p>​    ping 目的主机    （功能描述：测试当前服务器是否可以连接目的主机）</p><ol start="2"><li>案例实操</li></ol><p>​    （1）测试当前服务器是否可以连接百度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]# ping www.baidu.com</span><br></pre></td></tr></table></figure><h5 id="3-2-3-修改ip地址"><a href="#3-2-3-修改ip地址" class="headerlink" title="3.2.3 修改ip地址"></a>3.2.3 修改ip地址</h5><ol><li>修改IP地址</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]#vim &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/oRLyPiCXzdnTuFY.png" alt="img"> </p><p>以下标红的项必须修改，有值的按照下面的值修改，没有该项的要增加。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">DEVICE&#x3D;eth0         #接口名（设备,网卡）</span><br><span class="line"></span><br><span class="line">HWADDR&#x3D;00:0C:2x:6x:0x:xx  #MAC地址 </span><br><span class="line"></span><br><span class="line">TYPE&#x3D;Ethernet        #网络类型（通常是Ethemet）</span><br><span class="line"></span><br><span class="line">UUID&#x3D;926a57ba-92c6-4231-bacb-f27e5e6a9f44  #随机id</span><br><span class="line"></span><br><span class="line">#系统启动的时候网络接口是否有效（yes&#x2F;no）</span><br><span class="line">ONBOOT&#x3D;yes         </span><br><span class="line"></span><br><span class="line"># IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）</span><br><span class="line">BOOTPROTO&#x3D;static   </span><br><span class="line"></span><br><span class="line">#IP地址</span><br><span class="line">IPADDR&#x3D;192.168.1.100  </span><br><span class="line"></span><br><span class="line">#网关 </span><br><span class="line">GATEWAY&#x3D;192.168.1.2   </span><br><span class="line"></span><br><span class="line">#域名解析器</span><br><span class="line">DNS1&#x3D;114.114.114.114</span><br><span class="line">DNS2&#x3D;8.8.8.8</span><br></pre></td></tr></table></figure><p>​    修改后</p><p><img src="https://i.loli.net/2020/10/27/2wq5bARW1PZHekJ.png" alt="img"> </p><p>：wq  保存退出</p><ol start="2"><li>执行service network restart，重启网络</li></ol><p><img src="https://i.loli.net/2020/10/27/gipKkrHtI3dBxTA.png" alt="img"> </p><ol start="3"><li>如果报错，reboot，重启虚拟机</li></ol><h4 id="3-3-配置主机名"><a href="#3-3-配置主机名" class="headerlink" title="3.3 配置主机名"></a>3.3 配置主机名</h4><h5 id="5-3-1-hostname-显示和设置系统的主机名称"><a href="#5-3-1-hostname-显示和设置系统的主机名称" class="headerlink" title="5.3.1 hostname 显示和设置系统的主机名称"></a>5.3.1 hostname 显示和设置系统的主机名称</h5><ol><li>基本语法</li></ol><p>hostname        （功能描述：查看当前服务器的主机名称）</p><ol start="2"><li>案例实操</li></ol><p>​    （1）查看当前服务器主机名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]# hostname</span><br></pre></td></tr></table></figure><h5 id="5-3-2-修改主机名称"><a href="#5-3-2-修改主机名称" class="headerlink" title="5.3.2 修改主机名称"></a>5.3.2 修改主机名称</h5><ol><li>修改linux的主机映射文件（hosts文件）</li></ol><p>（1）进入Linux系统查看本机的主机名。通过hostname命令查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]# hostname</span><br><span class="line">hadoop100</span><br></pre></td></tr></table></figure><p>（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]# vi &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br></pre></td></tr></table></figure><p>文件中内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING&#x3D;yes</span><br><span class="line"></span><br><span class="line">NETWORKING_IPV6&#x3D;no</span><br><span class="line"></span><br><span class="line">HOSTNAME&#x3D; hadoop100</span><br></pre></td></tr></table></figure><p><font color="red">注意：主机名称不要有“_”下划线</font></p><p>（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop100。</p><p>（4）保存退出。</p><p>（5）打开/etc/hosts</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]# vim &#x2F;etc&#x2F;hosts</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.100 hadoop100</span><br><span class="line"></span><br><span class="line">192.168.1.101 hadoop101</span><br><span class="line"></span><br><span class="line">192.168.1.102 hadoop102</span><br><span class="line"></span><br><span class="line">192.168.1.103 hadoop103</span><br><span class="line"></span><br><span class="line">192.168.1.104 hadoop104</span><br><span class="line"></span><br><span class="line">192.168.1.105 hadoop105</span><br><span class="line"></span><br><span class="line">192.168.1.106 hadoop106</span><br><span class="line"></span><br><span class="line">192.168.1.107 hadoop107</span><br><span class="line"></span><br><span class="line">192.168.1.108 hadoop108</span><br></pre></td></tr></table></figure><p>（6）并重启设备，重启后，查看主机名，已经修改成功</p><ol start="2"><li>修改window7的主机映射文件（hosts文件）</li></ol><p>​    （1）进入C:\Windows\System32\drivers\etc路径</p><p>​    （2）打开hosts文件并添加如下内容</p><p>​    192.168.1.100 hadoop100</p><p>​    192.168.1.101 hadoop101</p><p>​    192.168.1.102 hadoop102</p><p>​    192.168.1.103 hadoop103</p><p>​    192.168.1.104 hadoop104</p><p>​    192.168.1.105 hadoop105</p><p>​    192.168.1.106 hadoop106</p><p>​    192.168.1.107 hadoop107</p><p>​    192.168.1.108 hadoop108</p><ol start="3"><li><p>修改window10的主机映射文件（hosts文件）</p><p>（1）进入C:\Windows\System32\drivers\etc路径</p><p>（2）拷贝hosts文件到桌面</p><p>（3）打开桌面hosts文件并添加如下内容</p><p>192.168.1.100 hadoop100</p><p>192.168.1.101 hadoop101</p><p>192.168.1.102 hadoop102</p><p>192.168.1.103 hadoop103</p><p>192.168.1.104 hadoop104</p><p>192.168.1.105 hadoop105</p><p>192.168.1.106 hadoop106</p><p>192.168.1.107 hadoop107</p><p>192.168.1.108 hadoop108</p></li></ol><p>（4）将桌面hosts文件覆盖C:\Windows\System32\drivers\etc路径hosts文件</p><h4 id="3-4-关闭防火墙"><a href="#3-4-关闭防火墙" class="headerlink" title="3.4 关闭防火墙"></a>3.4 关闭防火墙</h4><h5 id="3-4-1-service-后台服务管理"><a href="#3-4-1-service-后台服务管理" class="headerlink" title="3.4.1 service 后台服务管理"></a>3.4.1 service 后台服务管理</h5><ol><li>基本语法</li></ol><p>​    service  服务名 start            （功能描述：开启服务）</p><p>​    service  服务名 stop            （功能描述：关闭服务）</p><p>​    service  服务名 restart            （功能描述：重新启动服务）</p><p>​    service  服务名 status            （功能描述：查看服务状态）</p><ol start="2"><li>经验技巧</li></ol><p>​    查看服务的方法：/etc/init.d/服务名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 init.d]# pwd</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;init.d</span><br><span class="line"></span><br><span class="line">[root@hadoop100 init.d]# ls -al</span><br></pre></td></tr></table></figure><ol start="3"><li>案例实操</li></ol><p>（1）查看网络服务的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]#service network status</span><br></pre></td></tr></table></figure><p>（2）停止网络服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]#service network stop</span><br></pre></td></tr></table></figure><p>（3）启动网络服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]#service network start</span><br></pre></td></tr></table></figure><p>（4）重启网络服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]#service network restart</span><br></pre></td></tr></table></figure><p>（5）查看系统中所有的后台服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 桌面]#service --status-all</span><br></pre></td></tr></table></figure><h5 id="3-4-2-chkconfig-设置后台服务的自启配置"><a href="#3-4-2-chkconfig-设置后台服务的自启配置" class="headerlink" title="3.4.2 chkconfig 设置后台服务的自启配置"></a><font color="red">3.4.2 chkconfig 设置后台服务的自启配置</font></h5><ol><li>基本语法</li></ol><p>chkconfig               （功能描述：查看所有服务器自启配置）</p><p>chkconfig 服务名 off  （功能描述：关掉指定服务的自动启动）</p><p>chkconfig 服务名 on  （功能描述：开启指定服务的自动启动）</p><p>chkconfig 服务名 –list    （功能描述：查看服务开机启动状态）</p><ol start="2"><li>案例实操</li></ol><p>（1）关闭iptables服务的自动启动</p><p>[root@hadoop100 桌面]#chkconfig iptables off</p><p>（2）开启iptables服务的自动启动</p><p>[root@hadoop100 桌面]#chkconfig iptables on</p><h5 id="3-4-3-进程运行级别"><a href="#3-4-3-进程运行级别" class="headerlink" title="3.4.3 进程运行级别"></a>3.4.3 进程运行级别</h5><p>Linux进程运行级别，如图1-102所示</p><p><img src="https://i.loli.net/2020/10/27/qwGhD8yMj3ESx16.png" alt="img"></p><h5 id="3-4-4-关闭防火墙"><a href="#3-4-4-关闭防火墙" class="headerlink" title="3.4.4 关闭防火墙"></a>3.4.4 关闭防火墙</h5><ol><li><p>临时关闭防火墙</p><p>（1）查看防火墙状态</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100桌面]# service iptables status</span><br></pre></td></tr></table></figure><p>​    （2）临时关闭防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100桌面]# service iptables stop</span><br></pre></td></tr></table></figure><ol start="2"><li> 开机启动时关闭防火墙</li></ol><p>​    （1）查看防火墙开机启动状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100桌面]#chkconfig iptables --list</span><br></pre></td></tr></table></figure><p>​    （2）设置开机时关闭防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100桌面]#chkconfig iptables off</span><br></pre></td></tr></table></figure><h4 id="3-5-关机重启命令"><a href="#3-5-关机重启命令" class="headerlink" title="3.5 关机重启命令"></a>3.5 关机重启命令</h4><p>​        在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机。</p><p>​        <strong>正确的关机流程为</strong>：sync &gt; shutdown &gt; reboot &gt; halt</p><ol><li>基本语法</li></ol><p>​    （1）sync          （功能描述：将数据由内存同步到硬盘中）</p><p>​    （2）halt             （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）</p><p>​    （3）reboot             （功能描述：就是重启，等同于 shutdown -r now）</p><p>​    （4）shutdown [选项] 时间    </p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-h</td><td>-h=halt关机</td></tr><tr><td>-r</td><td>-r=reboot重启</td></tr></tbody></table><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>now</td><td>立刻关机</td></tr><tr><td>时间</td><td>等待多久后关机（时间单位是*<strong>*分钟**</strong>）。</td></tr></tbody></table><ol start="2"><li><p>经验技巧</p><p>​    Linux系统中为了提高磁盘的读写效率，对磁盘采取了 “预读迟写”操作方式。当用户保存文件时，Linux核心并不一定立即将保存数据写入物理磁盘中，而是将数据保存在缓冲区中，等缓冲区满时再写入磁盘，这种方式可以极大的提高磁盘写入数据的效率。但是，也带来了安全隐患，如果数据还未写入磁盘时，系统掉电或者其他严重问题出现，则将导致数据丢失。使用sync指令可以立即将缓冲区的数据写入磁盘。</p></li></ol><p>​    3．案例实操</p><p>​    （1）将数据由内存同步到硬盘中</p><p>​        [root@hadoop100桌面]#sync  </p><p>​    （2）重启</p><p>​        [root@hadoop100桌面]# reboot </p><p>​    （3）关机</p><p>​        [root@hadoop100桌面]#halt </p><p>​    （4）计算机将在1分钟后关机，并且会显示在登录用户的当前屏幕中</p><p>​        [root@hadoop100桌面]#shutdown -h 1 ‘This server will shutdown after 1 mins’</p><p>​    （5）立马关机（等同于 halt）</p><p>​        [root@hadoop100桌面]# shutdown -h now </p><p>​    （6）系统立马重启（等同于 reboot）</p><p>​        [root@hadoop100桌面]# shutdown -r now</p><h4 id="3-6找回root密码"><a href="#3-6找回root密码" class="headerlink" title="3.6找回root密码"></a>3.6找回root密码</h4><p>[详见root密码破解[1.0版].pdf]: F:\BaiduNetdiskDownload\005_Linux\2.资料\04_笔记中对应的详细资料</p><h4 id="3-7-克隆虚拟机"><a href="#3-7-克隆虚拟机" class="headerlink" title="3.7 克隆虚拟机"></a>3.7 克隆虚拟机</h4><p>1．关闭要被克隆的虚拟机</p><p>2．找到克隆选项，如图1-112所示 </p><p><img src="https://i.loli.net/2020/10/27/j2AdePnUvs7WONa.png" alt="img"> </p><p>3．欢迎页面，如图1-113所示</p><p><img src="https://i.loli.net/2020/10/27/jN7DvEkitCcLBsg.png" alt="img"> </p><p>4．克隆虚拟机，如图1-114所示</p><p><img src="https://i.loli.net/2020/10/27/O13sf9inJ2dPGwh.png" alt="img"> </p><p>5．设置创建完整克隆，如图1-115所示</p><p><img src="https://i.loli.net/2020/10/27/4aWMmG5HyLshvdl.png" alt="img"> </p><p>6．设置克隆的虚拟机名称和存储位置，如图1-116所示</p><p><img src="https://i.loli.net/2020/10/27/dXrAbsKalBjOtv1.png" alt="img"> </p><p>7．等待正在克隆，如题1-117所示</p><p><img src="https://i.loli.net/2020/10/27/JkEypscgxZrI9Wf.png" alt="img"> </p><p>8．点击关闭，完成克隆，如图1-118所示</p><p><img src="https://i.loli.net/2020/10/27/nRZ8XNDrO2YSGC7.png" alt="img"> </p><p>9．修改克隆后虚拟机的ip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 &#x2F;]#vim &#x2F;etc&#x2F;udev&#x2F;rules.d&#x2F;70-persistent-net.rules</span><br></pre></td></tr></table></figure><p>进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址，如图1-119所示</p><p><img src="https://i.loli.net/2020/10/27/YJqXo9PwBvCl5V7.png" alt="img"> </p><p>10．修改IP地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 &#x2F;]#vim &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0</span><br></pre></td></tr></table></figure><p>（1）把复制的物理ip地址更新</p><p>HWADDR=00:0C:2x:6x:0x:xx  #MAC地址 </p><p>（2）修改成你想要的ip</p><p>IPADDR=192.168.1.101    #IP地址</p><p>11．修改主机名称</p><p>详见5.3。</p><p>12．重新启动服务器</p><h3 id="四、远程登陆"><a href="#四、远程登陆" class="headerlink" title="四、远程登陆"></a>四、远程登陆</h3><h4 id="4-1-安装XShell"><a href="#4-1-安装XShell" class="headerlink" title="4.1 安装XShell"></a>4.1 安装XShell</h4><h3 id="五、常用基本命令"><a href="#五、常用基本命令" class="headerlink" title="五、常用基本命令"></a>五、常用基本命令</h3><h4 id="5-1-帮助命令"><a href="#5-1-帮助命令" class="headerlink" title="5.1 帮助命令"></a>5.1 帮助命令</h4><h5 id="5-1-1-man-获得帮助信息"><a href="#5-1-1-man-获得帮助信息" class="headerlink" title="5.1.1 man 获得帮助信息"></a>5.1.1 man 获得帮助信息</h5><ol><li>基本语法</li></ol><p>​    man [命令或配置文件]        （功能描述：获得帮助信息）</p><p>​    2．显示说明</p><table><thead><tr><th>信息</th><th>功能</th></tr></thead><tbody><tr><td>NAME</td><td>命令的名称和单行描述</td></tr><tr><td>SYNOPSIS</td><td>怎样使用命令</td></tr><tr><td>DESCRIPTION</td><td>命令功能的深入讨论</td></tr><tr><td>EXAMPLES</td><td>怎样使用命令的例子</td></tr><tr><td>SEE ALSO</td><td>相关主题（通常是手册页）</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）查看ls命令的帮助信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# man ls</span><br></pre></td></tr></table></figure><h5 id="5-1-2-help-获得shell内置命令的帮助信息"><a href="#5-1-2-help-获得shell内置命令的帮助信息" class="headerlink" title="5.1.2 help 获得shell内置命令的帮助信息"></a>5.1.2 help 获得shell内置命令的帮助信息</h5><p>1．基本语法</p><p>​    help 命令    （功能描述：获得shell内置命令的帮助信息）</p><p>2．案例实操</p><p>​    （1）查看cd命令的帮助信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# help cd</span><br></pre></td></tr></table></figure><h5 id="5-1-3-常用快捷键"><a href="#5-1-3-常用快捷键" class="headerlink" title="5.1.3 常用快捷键"></a>5.1.3 常用快捷键</h5><table><thead><tr><th>常用快捷键</th><th>功能</th></tr></thead><tbody><tr><td>ctrl + c</td><td>停止进程</td></tr><tr><td>ctrl+l</td><td>清屏；彻底清屏是：reset</td></tr><tr><td>ctrl + q</td><td>退出</td></tr><tr><td><strong>善于用tab键</strong></td><td>提示(更重要的是可以防止敲错)</td></tr><tr><td><strong>上下键</strong></td><td>查找执行过的命令</td></tr><tr><td>ctrl +alt</td><td>linux和Windows之间切换</td></tr></tbody></table><h4 id="5-2-文件目录类"><a href="#5-2-文件目录类" class="headerlink" title="5.2  文件目录类"></a>5.2  文件目录类</h4><h5 id="5-2-1-pwd-显示当前工作目录的绝对路径"><a href="#5-2-1-pwd-显示当前工作目录的绝对路径" class="headerlink" title="5.2.1 pwd 显示当前工作目录的绝对路径"></a>5.2.1 pwd 显示当前工作目录的绝对路径</h5><p>pwd:print working directory 打印工作目录</p><p>1．基本语法</p><p>pwd        （功能描述：显示当前工作目录的绝对路径）</p><p>2．案例实操</p><p>​    （1）显示当前工作目录的绝对路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# pwd</span><br><span class="line">&#x2F;root</span><br></pre></td></tr></table></figure><h5 id="5-2-2-ls-列出目录的内容"><a href="#5-2-2-ls-列出目录的内容" class="headerlink" title="5.2.2 ls 列出目录的内容"></a>5.2.2 ls 列出目录的内容</h5><p>ls:list 列出目录内容</p><p>1．基本语法</p><p>ls [选项] [目录或是文件]</p><p>2．选项说明</p><p>​    表1-8 选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-a</td><td>全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用)</td></tr><tr><td>-l</td><td>长数据串列出，包含文件的属性与权限等等数据；(常用)，就是ll</td></tr></tbody></table><p>3．显示说明</p><p>每行列出的信息依次是： <font color="red">文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字 </font></p><p>4．案例实操</p><p>​    （1）查看当前目录的所有内容信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ ls -al</span><br><span class="line"></span><br><span class="line">总用量 44</span><br><span class="line"></span><br><span class="line">drwx------. 5 atguigu atguigu 4096 5月  27 15:15 .</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 3 root   root   4096 5月  27 14:03 ..</span><br><span class="line"></span><br><span class="line">drwxrwxrwx. 2 root   root   4096 5月  27 14:14 hello</span><br><span class="line"></span><br><span class="line">-rwxrw-r--. 1 atguigu atguigu  34 5月  27 14:20 test.txt</span><br></pre></td></tr></table></figure><h5 id="5-2-3-cd-切换目录"><a href="#5-2-3-cd-切换目录" class="headerlink" title="5.2.3 cd 切换目录"></a>5.2.3 cd 切换目录</h5><p>cd:Change Directory切换路径</p><p>1．基本语法</p><p>​    cd  [参数]</p><p>2．参数说明</p><p>表1-9 参数说明</p><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>cd 绝对路径</td><td>切换路径</td></tr><tr><td>cd 相对路径</td><td>切换路径</td></tr><tr><td>cd ~或者cd</td><td>回到自己的家目录</td></tr><tr><td>cd -</td><td>回到上一次所在目录</td></tr><tr><td>cd ..</td><td>回到当前目录的上一级目录</td></tr><tr><td>cd -P</td><td>跳转到实际物理路径，而非快捷方式路径</td></tr></tbody></table><p>3．案例实操</p><p>（1）使用绝对路径切换到root目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cd &#x2F;root&#x2F;</span><br></pre></td></tr></table></figure><p>（2）使用相对路径切换到“公共的”目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cd 公共的&#x2F;</span><br></pre></td></tr></table></figure><p>（3）表示回到自己的家目录，亦即是 /root 这个目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 公共的]# cd ~</span><br></pre></td></tr></table></figure><p>（4）cd- 回到上一次所在目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cd -</span><br></pre></td></tr></table></figure><p>（5）表示回到当前目录的上一级目录，亦即是 “/root/公共的”的上一级目录的意思；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 公共的]# cd ..</span><br></pre></td></tr></table></figure><h5 id="5-2-4-mkdir-创建一个新的目录"><a href="#5-2-4-mkdir-创建一个新的目录" class="headerlink" title="5.2.4 mkdir 创建一个新的目录"></a>5.2.4 mkdir 创建一个新的目录</h5><p>mkdir:Make directory 建立目录</p><p>1．基本语法</p><p>​    mkdir [选项] 要创建的目录</p><p>2．选项说明</p><p>表1-10 选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-p</td><td>创建多层目录</td></tr></tbody></table><p>3．案例实操</p><p>（1）创建一个目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# mkdir xiyou</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# mkdir xiyou&#x2F;mingjie</span><br></pre></td></tr></table></figure><p>（2）创建一个多级目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# mkdir -p xiyou&#x2F;dssz&#x2F;meihouwang</span><br></pre></td></tr></table></figure><h5 id="5-2-5-rmdir-删除一个空的目录"><a href="#5-2-5-rmdir-删除一个空的目录" class="headerlink" title="5.2.5 rmdir 删除一个空的目录"></a>5.2.5 rmdir 删除一个空的目录</h5><p>rmdir:Remove directory 移动目录</p><p>1．基本语法：</p><p>​    rmdir 要删除的*<strong>*空目录**</strong></p><p>2．案例实操</p><p>​    （1）删除一个空的文件夹（只能删除空的文件夹，有内容是不能删的）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# rmdir xiyou&#x2F;dssz&#x2F;meihouwang</span><br></pre></td></tr></table></figure><h5 id="5-2-6-touch-创建空文件（重要）"><a href="#5-2-6-touch-创建空文件（重要）" class="headerlink" title="5.2.6 touch 创建空文件（重要）"></a>5.2.6 touch 创建空文件（重要）</h5><p>1．基本语法</p><p>​    touch 文件名称</p><p>2．案例实操</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# touch xiyou&#x2F;dssz&#x2F;sunwukong.txt</span><br></pre></td></tr></table></figure><h5 id="5-2-7-cp-复制文件或目录"><a href="#5-2-7-cp-复制文件或目录" class="headerlink" title="5.2.7 cp 复制文件或目录"></a>5.2.7 cp 复制文件或目录</h5><p>1．基本语法</p><p>cp [选项] source dest                 （功能描述：复制source文件到dest）</p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>递归复制整个文件夹</td></tr></tbody></table><p>3．参数说明</p><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>source</td><td>源文件</td></tr><tr><td>dest</td><td>目标文件</td></tr></tbody></table><p>4．经验技巧</p><p>​    强制覆盖不提示的方法：\cp</p><p>5．案例实操</p><p>（1）复制文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cp xiyou&#x2F;dssz&#x2F;suwukong.txt xiyou&#x2F;mingjie&#x2F;</span><br></pre></td></tr></table></figure><p>（2）递归复制整个文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cp -r xiyou&#x2F;dssz&#x2F; .&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-2-8-rm-移除文件或目录"><a href="#5-2-8-rm-移除文件或目录" class="headerlink" title="5.2.8 rm 移除文件或目录"></a>5.2.8 rm 移除文件或目录</h5><p>1．基本语法</p><p>rm [选项] deleteFile            （功能描述：递归删除目录中所有内容）</p><p>2．选项说明</p><p>表1-13 选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>递归删除目录中所有内容（文件夹中的内容全部删除）</td></tr><tr><td>-f</td><td>强制执行删除操作，而不提示用于进行确认。</td></tr><tr><td>-v</td><td>显示指令的详细执行过程</td></tr></tbody></table><p>\3. 案例实操</p><p>（1）删除目录中的内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# rm xiyou&#x2F;mingjie&#x2F;sunwukong.txt</span><br></pre></td></tr></table></figure><p>（2）递归删除目录中所有内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# rm -rf dssz&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-2-9-mv-移动文件与目录或重命名"><a href="#5-2-9-mv-移动文件与目录或重命名" class="headerlink" title="5.2.9 mv 移动文件与目录或重命名"></a>5.2.9 mv 移动文件与目录或重命名</h5><p>1．基本语法</p><p>​    （1）mv oldNameFile newNameFile    （功能描述：重命名）</p><p>​    （2）mv /temp/movefile /targetFolder    （功能描述：移动文件）</p><p>2．案例实操</p><p>（1）重命名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# mv xiyou&#x2F;dssz&#x2F;suwukong.txt xiyou&#x2F;dssz&#x2F;houge.txt</span><br></pre></td></tr></table></figure><p>（2）移动文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# mv xiyou&#x2F;dssz&#x2F;houge.txt .&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-2-10-cat-查看文件内容（重要）"><a href="#5-2-10-cat-查看文件内容（重要）" class="headerlink" title="5.2.10 cat 查看文件内容（重要）"></a>5.2.10 cat 查看文件内容（重要）</h5><p>查看文件内容，从第一行开始显示。</p><p>1．基本语法</p><p>​    cat  [选项] 要查看的文件</p><p>2．选项说明</p><p>表1-14</p><table><thead><tr><th>选项</th><th>功能描述</th></tr></thead><tbody><tr><td>-n</td><td>显示所有行的行号，包括空行。</td></tr></tbody></table><p>3．经验技巧</p><p>一般查看比较小的文件，*<strong>*一屏幕能显示全的**</strong>。</p><p>4．案例实操</p><p>​    （1）查看文件内容并显示行号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ cat -n houge.txt </span><br></pre></td></tr></table></figure><h5 id="5-2-11-more-文件内容分屏查看器"><a href="#5-2-11-more-文件内容分屏查看器" class="headerlink" title="5.2.11 more 文件内容分屏查看器"></a>5.2.11 more 文件内容分屏查看器</h5><p>more指令是一个基于VI编辑器的文本过滤器，它以全屏幕的方式按页显示文本文件的内容。more指令中内置了若干快捷键，详见操作说明。</p><p>1．基本语法</p><p>​    more 要查看的文件</p><p>2．操作说明</p><p>表1-15 操作说明</p><table><thead><tr><th>操作</th><th>功能说明</th></tr></thead><tbody><tr><td>空白键 (space)</td><td>代表向下翻一页；</td></tr><tr><td>Enter</td><td>代表向下翻『一行』；</td></tr><tr><td>q</td><td>代表立刻离开 more ，不再显示该文件内容。</td></tr><tr><td>Ctrl+F</td><td>向下滚动一屏</td></tr><tr><td>Ctrl+B</td><td>返回上一屏</td></tr><tr><td>=</td><td>输出当前行的行号</td></tr><tr><td>:f</td><td>输出文件名和当前行的行号</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）采用more查看文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# more smartd.conf</span><br></pre></td></tr></table></figure><h5 id="5-2-12-less-分屏显示文件内容（重要）"><a href="#5-2-12-less-分屏显示文件内容（重要）" class="headerlink" title="5.2.12 less 分屏显示文件内容（重要）"></a>5.2.12 less 分屏显示文件内容（重要）</h5><p>​    less指令用来分屏查看文件内容，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示*<strong>*大型文件具有较高的效率**</strong>。</p><p>1．基本语法</p><p>​    less 要查看的文件</p><p>2．操作说明</p><p>表1-16 操作说明</p><table><thead><tr><th>操作</th><th>功能说明</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻动一页；</td></tr><tr><td>[pagedown]</td><td>向下翻动一页</td></tr><tr><td>[pageup]</td><td>向上翻动一页；</td></tr><tr><td>/字串</td><td>向下搜寻『字串』的功能；n：向下查找；N：向上查找；</td></tr><tr><td>?字串</td><td>向上搜寻『字串』的功能；n：向上查找；N：向下查找；</td></tr><tr><td>q</td><td>离开 less 这个程序；</td></tr></tbody></table><p>4．案例实操</p><p>​    （1）采用less查看文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# less smartd.conf</span><br></pre></td></tr></table></figure><h5 id="5-2-13-echo"><a href="#5-2-13-echo" class="headerlink" title="5.2.13 echo"></a>5.2.13 echo</h5><p>echo输出内容到控制台</p><ol><li>基本语法</li></ol><p>​        echo [选项] [输出内容]</p><p>选项： </p><p> -e：  支持反斜线控制的字符转换</p><table><thead><tr><th>控制字符</th><th>作用</th></tr></thead><tbody><tr><td>\</td><td>输出\本身</td></tr><tr><td>\n</td><td>换行符</td></tr><tr><td>\t</td><td>制表符，也就是Tab键</td></tr></tbody></table><ol start="2"><li>案例实操</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 ~]$ echo &quot;hello\tworld&quot;</span><br><span class="line"></span><br><span class="line">hello\tworld</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop101 ~]$ echo -e &quot;hello\tworld&quot;</span><br><span class="line"></span><br><span class="line">helloworld</span><br></pre></td></tr></table></figure><h5 id="5-2-14-head-显示文件头部内容"><a href="#5-2-14-head-显示文件头部内容" class="headerlink" title="5.2.14 head 显示文件头部内容"></a>5.2.14 head 显示文件头部内容</h5><p>head用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。</p><ol><li>基本语法</li></ol><p>head 文件       （功能描述：查看文件头10行内容）</p><p>head -n 5 文件    （功能描述：查看文件头5行内容，5可以是任意行数）</p><p>​    2．选项说明</p><p>表1-18</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-n &lt;行数&gt;</td><td>指定显示头部内容的行数</td></tr></tbody></table><p>​    3．案例实操</p><p>​    （1）查看文件的头2行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# head -n 2 smartd.conf</span><br></pre></td></tr></table></figure><h5 id="5-2-15-tail-输出文件尾部内容"><a href="#5-2-15-tail-输出文件尾部内容" class="headerlink" title="5.2.15 tail 输出文件尾部内容"></a>5.2.15 tail 输出文件尾部内容</h5><p>tail用于输出文件中尾部的内容，默认情况下tail指令显示文件的后10行内容。</p><ol><li>   基本语法</li></ol><p>（1）tail  文件             （功能描述：查看文件后10行内容）</p><p>（2）tail  -n 5 文件         （功能描述：查看文件后5行内容，5可以是任意行数）</p><p><font color="red">（3）tail  -f  文件        （功能描述：实时追踪该文档的所有更新）</font></p><p>2．    选项说明</p><p>表1-19</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-n&lt;行数&gt;</td><td>输出文件尾部n行内容</td></tr><tr><td>-f</td><td>显示文件最新追加的内容，监视文件变化</td></tr></tbody></table><p>3．案例实操</p><p>（1）查看文件头1行内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# tail -n 1 smartd.conf </span><br></pre></td></tr></table></figure><p>（2）实时追踪该档的所有更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# tail -f houge.txt</span><br></pre></td></tr></table></figure><h5 id="5-2-16-gt-覆盖和-gt-gt-追加"><a href="#5-2-16-gt-覆盖和-gt-gt-追加" class="headerlink" title="5.2.16&gt;覆盖和&gt;&gt;追加"></a>5.2.16&gt;覆盖和&gt;&gt;追加</h5><p>1．基本语法</p><p>（1）ll &gt;文件        （功能描述：列表的内容写入文件a.txt中（*<strong>*覆盖写**</strong>））</p><p>（2）ll &gt;&gt;文件        （功能描述：列表的内容*<strong>*追加**</strong>到文件aa.txt的末尾）</p><p>（3）cat 文件1 &gt; 文件2    （功能描述：将文件1的内容覆盖到文件2）</p><p>（4）echo “内容” &gt;&gt; 文件</p><p>2．案例实操</p><p>（1）将ls查看信息写入到文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ls -l&gt;houge.txt</span><br></pre></td></tr></table></figure><p>（2）将ls查看信息追加到文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ls -l&gt;&gt;houge.txt</span><br></pre></td></tr></table></figure><p>（3）采用echo将hello单词追加到文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# echo hello&gt;&gt;houge.txt</span><br></pre></td></tr></table></figure><h5 id="5-2-17-ln-软链接"><a href="#5-2-17-ln-软链接" class="headerlink" title="5.2.17 ln 软链接"></a>5.2.17 ln 软链接</h5><p>软链接也成为符号链接，类似于windows里的快捷方式，有自己的数据块，主要存放了链接其他文件的路径。</p><p>1．基本语法</p><p>ln -s [原文件或目录] [软链接名]        （功能描述：给原文件创建一个软链接）</p><p>2．经验技巧</p><p>删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/</p><p>查询：通过ll就可以查看，列表属性第1位是l，尾部会有位置指向。</p><p>3．案例实操</p><p>​    （1）创建软连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# mv houge.txt xiyou&#x2F;dssz&#x2F;</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ln -s xiyou&#x2F;dssz&#x2F;houge.txt .&#x2F;houzi</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ll</span><br><span class="line"></span><br><span class="line">lrwxrwxrwx. 1 root   root    20 6月  17 12:56 houzi -&gt; xiyou&#x2F;dssz&#x2F;houge.txt</span><br></pre></td></tr></table></figure><p>​    （2）删除软连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# rm -rf houzi</span><br></pre></td></tr></table></figure><p>​    （3）进入软连接实际物理路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ln -s xiyou&#x2F;dssz&#x2F; .&#x2F;dssz</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# cd -P dssz&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-2-18-history-查看已经执行过历史命令"><a href="#5-2-18-history-查看已经执行过历史命令" class="headerlink" title="5.2.18 history 查看已经执行过历史命令"></a>5.2.18 history 查看已经执行过历史命令</h5><p>1．基本语法</p><p>​    history                        （功能描述：查看已经执行过历史命令）</p><p>2．案例实操</p><p>​    （1）查看已经执行过的历史命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 test1]# history</span><br></pre></td></tr></table></figure><h4 id="5-3时间日期类"><a href="#5-3时间日期类" class="headerlink" title="5.3时间日期类"></a>5.3时间日期类</h4><p>1．基本语法</p><p>date [OPTION]… [+FORMAT]</p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-d&lt;时间字符串&gt;</td><td>显示指定的“时间字符串”表示的时间，而非当前时间</td></tr><tr><td>-s&lt;日期时间&gt;</td><td>设置系统日期时间</td></tr></tbody></table><p>3．参数说明</p><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>&lt;+日期时间格式&gt;</td><td>指定显示时使用的日期时间格式</td></tr></tbody></table><h5 id="5-3-1-date-显示当前时间"><a href="#5-3-1-date-显示当前时间" class="headerlink" title="5.3.1 date 显示当前时间"></a>5.3.1 date 显示当前时间</h5><p>1．基本语法</p><p>​    （1）date                                （功能描述：显示当前时间）</p><p>​    （2）date +%Y                            （功能描述：显示当前年份）</p><p>​    （3）date +%m                            （功能描述：显示当前月份）</p><p>​    （4）date +%d                            （功能描述：显示当前是哪一天）</p><p>​    （5）date “+%Y-%m-%d %H:%M:%S”        （功能描述：显示年月日时分秒）</p><p>2．案例实操</p><p>（1）显示当前时间信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# date</span><br><span class="line"></span><br><span class="line">2017年 06月 19日 星期一 20:53:30 CST</span><br></pre></td></tr></table></figure><p>（2）显示当前时间年月日</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# date +%Y%m%d</span><br><span class="line"></span><br><span class="line">20170619</span><br></pre></td></tr></table></figure><p>（3）显示当前时间年月日时分秒</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# date &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line"></span><br><span class="line">2017-06-19 20:54:58</span><br></pre></td></tr></table></figure><h5 id="5-3-2-date显示非当前时间"><a href="#5-3-2-date显示非当前时间" class="headerlink" title="5.3.2 date显示非当前时间"></a>5.3.2 date显示非当前时间</h5><p>1．基本语法</p><p>（1）date -d ‘1 days ago’            （功能描述：显示前一天时间）</p><p>（2）date -d ‘-1 days ago’            （功能描述：显示明天时间）</p><p>2．案例实操</p><p>（1）显示前一天</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# date -d &#39;1 days ago&#39;</span><br><span class="line"></span><br><span class="line">2017年 06月 18日 星期日 21:07:22 CST</span><br></pre></td></tr></table></figure><p>（2）显示明天时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#date -d &#39;-1 days ago&#39;</span><br><span class="line"></span><br><span class="line">2017年 06月 20日 星期日 21:07:22 CST</span><br></pre></td></tr></table></figure><h5 id="5-3-3-date-设置系统时间"><a href="#5-3-3-date-设置系统时间" class="headerlink" title="5.3.3 date 设置系统时间"></a>5.3.3 date 设置系统时间</h5><p>1．基本语法</p><p>​    date -s 字符串时间</p><p>2．案例实操</p><p>​    （1）设置系统当前时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# date -s &quot;2017-06-19 20:52:18&quot;</span><br></pre></td></tr></table></figure><h5 id="5-3-4-cal查看日历"><a href="#5-3-4-cal查看日历" class="headerlink" title="5.3.4 cal查看日历"></a>5.3.4 cal查看日历</h5><p>1．基本语法</p><p>cal [选项]            （功能描述：不加选项，显示本月日历）</p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>具体某一年</td><td>显示这一年的日历</td></tr></tbody></table><p>3．案例实操</p><p>（1）查看当前月的日历</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cal</span><br></pre></td></tr></table></figure><p>（2）查看2017年的日历</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cal 2017</span><br></pre></td></tr></table></figure><h4 id="5-4-用户管理命令"><a href="#5-4-用户管理命令" class="headerlink" title="5.4 用户管理命令"></a>5.4 用户管理命令</h4><h5 id="5-4-1-useradd添加新用户"><a href="#5-4-1-useradd添加新用户" class="headerlink" title="5.4.1 useradd添加新用户"></a>5.4.1 useradd添加新用户</h5><p>1．基本语法</p><p>​    useradd 用户名            （功能描述：添加新用户）</p><p>​    useradd -g 组名 用户名    （功能描述：添加新用户到某个组）</p><p>2．案例实操</p><p>​    （1）添加一个用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# useradd zhangsan</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#ll &#x2F;home&#x2F;</span><br><span class="line">drwx------. 4 zhangsan zhangsan 4096 3月  17 01:27 zhangsan</span><br></pre></td></tr></table></figure><h5 id="5-4-2-passwd设置用户密码"><a href="#5-4-2-passwd设置用户密码" class="headerlink" title="5.4.2 passwd设置用户密码"></a>5.4.2 passwd设置用户密码</h5><p>1．基本语法</p><p>​    passwd 用户名    （功能描述：设置用户密码）</p><p>2．案例实操</p><p>​    （1）设置用户的密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# passwd zhangsan</span><br><span class="line">更改用户 zhangsan 的密码 。</span><br><span class="line">新的 密码：</span><br><span class="line">无效的密码： 过于简单化&#x2F;系统化</span><br><span class="line">重新输入新的 密码：</span><br><span class="line">passwd： 所有的身份验证令牌已经成功更新。</span><br></pre></td></tr></table></figure><h5 id="5-4-3-id查看用户是否存在"><a href="#5-4-3-id查看用户是否存在" class="headerlink" title="5.4.3 id查看用户是否存在"></a>5.4.3 id查看用户是否存在</h5><p>1．基本语法</p><p>​    id 用户名</p><p>2．案例实操</p><p>​    （1）查看用户是否存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# id zhangsan</span><br><span class="line">uid&#x3D;500(zhangsan) gid&#x3D;500(zhangsan) 组&#x3D;500(zhangsan)</span><br></pre></td></tr></table></figure><h5 id="5-4-4-cat-etc-passwd查看创建了哪些用户"><a href="#5-4-4-cat-etc-passwd查看创建了哪些用户" class="headerlink" title="5.4.4 cat/etc/passwd查看创建了哪些用户"></a>5.4.4 cat/etc/passwd查看创建了哪些用户</h5><p>1）基本语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cat  &#x2F;etc&#x2F;passwd</span><br></pre></td></tr></table></figure><h5 id="5-4-5-su-切换用户"><a href="#5-4-5-su-切换用户" class="headerlink" title="5.4.5 su    切换用户"></a>5.4.5 su    切换用户</h5><p>su: swith user 切换用户</p><p>1．基本语法</p><p>su 用户名称  （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量）</p><p>su - 用户名称        （功能描述：切换到用户并获得该用户的环境变量及执行权限）</p><p>2．案例实操</p><p>​    （1）切换用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#su tangseng</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#echo $PATH</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;root&#x2F;bin</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#exit</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#su - tangseng</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#echo $PATH</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;qt-3.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;sbin:&#x2F;home&#x2F;tangseng&#x2F;bin</span><br></pre></td></tr></table></figure><h5 id="5-4-6-userdel删除用户"><a href="#5-4-6-userdel删除用户" class="headerlink" title="5.4.6 userdel删除用户"></a>5.4.6 userdel删除用户</h5><p>1．基本语法</p><p>​    （1）userdel  用户名        （功能描述：删除用户但保存用户主目录）</p><p>（2）userdel -r 用户名        （功能描述：用户和用户主目录，都删除）</p><p>2．选项说明</p><p>表1-23</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>删除用户的同时，删除与用户相关的所有文件。</td></tr></tbody></table><p>3．案例实操</p><p>（1）删除用户但保存用户主目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#userdel tangseng</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#ll &#x2F;home&#x2F;</span><br></pre></td></tr></table></figure><p>（2）删除用户和用户主目录，都删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#useradd zhubajie</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#ll &#x2F;home&#x2F;</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#userdel -r zhubajie</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#ll &#x2F;home&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-4-7-who查看登陆用户信息"><a href="#5-4-7-who查看登陆用户信息" class="headerlink" title="5.4.7 who查看登陆用户信息"></a>5.4.7 who查看登陆用户信息</h5><p>1．基本语法</p><p>​    （1）whoami            （功能描述：显示自身用户名称）</p><p>（2）who am i        （功能描述：显示*<strong>*登录用户**</strong>的用户名，就是显示用哪个用户来登陆的）</p><p>2．案例实操</p><p>​    （1）显示自身用户名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 opt]# whoami</span><br></pre></td></tr></table></figure><p>（2）显示登录用户的用户名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 opt]# who am i</span><br></pre></td></tr></table></figure><h5 id="5-4-8-sudo-设置普通用户具有root权限"><a href="#5-4-8-sudo-设置普通用户具有root权限" class="headerlink" title="5.4.8 sudo    设置普通用户具有root权限"></a>5.4.8 sudo    设置普通用户具有root权限</h5><p>1．添加atguigu用户，并对其设置密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#useradd atguigu</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#passwd atguigu</span><br></pre></td></tr></table></figure><p>2．修改配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#vi &#x2F;etc&#x2F;sudoers</span><br></pre></td></tr></table></figure><p>修改 /etc/sudoers 文件，找到下面一行(91行)，在root下面添加一行，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line"></span><br><span class="line">root   ALL&#x3D;(ALL)   ALL</span><br><span class="line"></span><br><span class="line">atguigu  ALL&#x3D;(ALL)   ALL</span><br></pre></td></tr></table></figure><p>或者配置成采用sudo命令时，不需要输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line"></span><br><span class="line">root    ALL&#x3D;(ALL)   ALL</span><br><span class="line"></span><br><span class="line">atguigu  ALL&#x3D;(ALL)   NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>修改完毕，现在可以用atguigu帐号登录，然后用命令 sudo ，即可获得root权限进行操作。</p><p>3．案例实操</p><p>​    （1）用普通用户在/opt目录下创建一个文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 opt]$ sudo mkdir module</span><br><span class="line"></span><br><span class="line">[root@hadoop101 opt]# chown atguigu:atguigu module&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-4-9-usermod-修改用户"><a href="#5-4-9-usermod-修改用户" class="headerlink" title="5.4.9 usermod    修改用户"></a>5.4.9 usermod    修改用户</h5><p>1．基本语法</p><p>usermod -g 用户组 用户名</p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-g</td><td>修改用户的初始登录组，给定的组必须存在</td></tr></tbody></table><p>3．案例实操</p><p>（1）将用户加入到用户组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# id zhangsan</span><br><span class="line">uid&#x3D;500(zhangsan) gid&#x3D;500(zhangsan) 组&#x3D;500(zhangsan)</span><br><span class="line"></span><br><span class="line">[root@hadoop100 ~]# usermod -g root zhangsan</span><br><span class="line">[root@hadoop100 ~]# id zhangsan</span><br><span class="line"></span><br><span class="line">uid&#x3D;500(zhangsan) gid&#x3D;0(root) 组&#x3D;0(root)</span><br></pre></td></tr></table></figure><h4 id="5-5-用户组管理命令"><a href="#5-5-用户组管理命令" class="headerlink" title="5.5 用户组管理命令"></a>5.5 用户组管理命令</h4><p>​        每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，</p><p>​        如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。</p><p>​        用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。</p><h5 id="5-5-1-groupadd-新增组"><a href="#5-5-1-groupadd-新增组" class="headerlink" title="5.5.1 groupadd 新增组"></a>5.5.1 groupadd 新增组</h5><p>1．基本语法</p><p>​        groupadd 组名</p><p>2．案例实操</p><p>​    （1）添加一个xitianqujing组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 opt]#groupadd xitianqujing</span><br></pre></td></tr></table></figure><h5 id="5-5-2-groupdel删除组"><a href="#5-5-2-groupdel删除组" class="headerlink" title="5.5.2 groupdel删除组"></a>5.5.2 groupdel删除组</h5><p>1．基本语法</p><p>groupdel 组名</p><p>2．案例实操</p><p>​    （1）删除xitianqujing组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 opt]# groupdel xitianqujing</span><br></pre></td></tr></table></figure><h5 id="5-5-3-groupmod修改组"><a href="#5-5-3-groupmod修改组" class="headerlink" title="5.5.3 groupmod修改组"></a>5.5.3 groupmod修改组</h5><p>1．基本语法</p><p>groupmod -n 新组名 老组名</p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能描述</th></tr></thead><tbody><tr><td>-n&lt;新组名&gt;</td><td>指定工作组的新组名</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）修改atguigu组名称为atguigu1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#groupadd xitianqujing</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# groupmod -n xitian xitianqujing</span><br></pre></td></tr></table></figure><h5 id="5-5-4-cat-etc-group查看创建了哪些组"><a href="#5-5-4-cat-etc-group查看创建了哪些组" class="headerlink" title="5.5.4 cat    /etc/group查看创建了哪些组"></a>5.5.4 cat    /etc/group查看创建了哪些组</h5><p>1．基本操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 atguigu]# cat  &#x2F;etc&#x2F;group</span><br></pre></td></tr></table></figure><h4 id="5-6-文件权限"><a href="#5-6-文件权限" class="headerlink" title="5.6 文件权限"></a>5.6 文件权限</h4><h5 id="5-6-1-文件属性"><a href="#5-6-1-文件属性" class="headerlink" title="5.6.1 文件属性"></a>5.6.1 文件属性</h5><p>​        Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属的用户和组。</p><p>1．从左到右的10个字符表示，如图1-154所示：</p><p><img src="https://i.loli.net/2020/10/27/QAcxWXSP9hBrq6b.png" alt="img"> </p><p>图1-154 文件属性</p><p>如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示:</p><p>（1）0首位表示类型，在Linux中第一个字符代表这个文件是目录、文件或链接文件等等</p><ul><li>​         -代表文件</li><li>​         d 代表目录</li><li>​         l 链接文档(link file)；</li></ul><p>（2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User</p><p>（3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group</p><p>（4）第7-9位确定其他用户拥有该文件的权限 —Other</p><p>2．rxw作用文件和目录的不同解释</p><p>（1）作用到文件：</p><ul><li>[ r ]代表可读(read): 可以读取，查看</li><li>[ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.</li><li>[ x ]代表可执行(execute):可以被系统执行</li></ul><p>（2）作用到目录：</p><ul><li>[ r ]代表可读(read): 可以读取，ls查看目录内容</li><li>[ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录</li><li>[ x ]代表可执行(execute):可以进入该目录</li></ul><p>3．案例实操</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ll</span><br><span class="line"></span><br><span class="line">总用量 104</span><br><span class="line"></span><br><span class="line">-rw-------. 1 root root  1248 1月  8 17:36 anaconda-ks.cfg</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 2 root root  4096 1月  12 14:02 dssz</span><br><span class="line"></span><br><span class="line">lrwxrwxrwx. 1 root root   20 1月  12 14:32 houzi -&gt; xiyou&#x2F;dssz&#x2F;houge.tx</span><br></pre></td></tr></table></figure><p>文件基本属性介绍，如图1-155所示：</p><p><img src="https://i.loli.net/2020/10/27/To75FZOAjtDlqks.png" alt="img"> </p><p>图1-155 文件基本属性介绍</p><p>（1）如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法</p><p>ln [原文件] [目标文件]     </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ln xiyou&#x2F;dssz&#x2F;houge.txt .&#x2F;hg.txt</span><br></pre></td></tr></table></figure><p>（2）如果查看的是文件夹：链接数指的是子文件夹个数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ls -al xiyou&#x2F;</span><br><span class="line"></span><br><span class="line">总用量 16</span><br><span class="line"></span><br><span class="line">drwxr-xr-x.  4 root root 4096 1月  12 14:00 .</span><br><span class="line"></span><br><span class="line">dr-xr-x---. 29 root root 4096 1月  12 14:32 ..</span><br><span class="line"></span><br><span class="line">drwxr-xr-x.  2 root root 4096 1月  12 14:30 dssz</span><br><span class="line"></span><br><span class="line">drwxr-xr-x.  2 root root 4096 1月  12 14:04 mingjie</span><br></pre></td></tr></table></figure><h5 id="5-6-2-chmod-改变权限"><a href="#5-6-2-chmod-改变权限" class="headerlink" title="5.6.2 chmod 改变权限"></a>5.6.2 chmod 改变权限</h5><p>1．基本语法</p><p><img src="https://i.loli.net/2020/10/27/v8FlEon32QyHWMN.png" alt="img"> </p><p>​    第一种方式变更权限</p><p>​        chmod  [{ugoa}{+-=}{rwx}] 文件或目录</p><p>​    第二种方式变更权限</p><p>​        chmod  [mode=421 ]  [文件或目录]</p><p>2．经验技巧</p><p>​    u:所有者  g:所有组  o:其他人  a:所有人(u、g、o的总和)</p><p>​    r=4 w=2 x=1     rwx=4+2+1=7</p><p>3．案例实操</p><p>​    （1）修改文件使其所属主用户具有执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# cp xiyou&#x2F;dssz&#x2F;houge.txt .&#x2F;</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# chmod u+x houge.txt</span><br></pre></td></tr></table></figure><p>​    （2）修改文件使其所属组用户具有执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# chmod g+x houge.txt</span><br></pre></td></tr></table></figure><p>​    （3）修改文件所属主用户执行权限,并使其他用户具有执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# chmod u-x,o+x houge.txt</span><br></pre></td></tr></table></figure><p>​    （4）采用数字的方式，设置文件所有者、所属组、其他用户都具有可读可写可执行权限。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# chmod 777 houge.txt</span><br></pre></td></tr></table></figure><p>​    （5）修改整个文件夹里面的所有文件的所有者、所属组、其他用户都具有可读可写可执行权限。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# chmod -R 777 xiyou&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-6-3-chown-改变所有者"><a href="#5-6-3-chown-改变所有者" class="headerlink" title="5.6.3 chown 改变所有者"></a>5.6.3 chown 改变所有者</h5><p>1．基本语法</p><p>chown [选项] [最终用户] [文件或目录]        （功能描述：改变文件或者目录的所有者）</p><p>2．选项说明</p><p>表1-26</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-R</td><td>递归操作</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）修改文件所有者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# chown atguigu houge.txt </span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ls -al</span><br><span class="line"></span><br><span class="line">-rwxrwxrwx. 1 atguigu root 551 5月  23 13:02 houge.txt</span><br></pre></td></tr></table></figure><p>（2）递归改变文件所有者和所有组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 xiyou]# ll</span><br><span class="line"></span><br><span class="line">drwxrwxrwx. 2 root root 4096 9月  3 21:20 xiyou</span><br><span class="line"></span><br><span class="line">[root@hadoop101 xiyou]# chown -R atguigu:atguigu xiyou&#x2F;</span><br><span class="line"></span><br><span class="line">[root@hadoop101 xiyou]# ll</span><br><span class="line"></span><br><span class="line">drwxrwxrwx. 2 atguigu atguigu 4096 9月  3 21:20 xiyou</span><br></pre></td></tr></table></figure><h5 id="5-6-4-chgrp-改变所属组"><a href="#5-6-4-chgrp-改变所属组" class="headerlink" title="5.6.4 chgrp 改变所属组"></a>5.6.4 chgrp 改变所属组</h5><p>1．基本语法</p><p>​        chgrp [最终用户组] [文件或目录]    （功能描述：改变文件或者目录的所属组）</p><p>2．案例实操</p><p>​    （1）修改文件的所属组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# chgrp root houge.txt</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ls -al</span><br><span class="line"></span><br><span class="line">-rwxrwxrwx. 1 atguigu root 551 5月  23 13:02 houge.txt</span><br></pre></td></tr></table></figure><h4 id="5-7-搜索查找类"><a href="#5-7-搜索查找类" class="headerlink" title="5.7 搜索查找类"></a>5.7 搜索查找类</h4><h5 id="5-7-1-find查找文件或者目录"><a href="#5-7-1-find查找文件或者目录" class="headerlink" title="5.7.1 find查找文件或者目录"></a>5.7.1 find查找文件或者目录</h5><p>find指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。</p><p>1．基本语法</p><p>​    find [搜索范围] [选项]</p><p>2．选项说明</p><p>表1-27</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-name&lt;查询方式&gt;</td><td>按照指定的文件名查找模式查找文件</td></tr><tr><td>-user&lt;用户名&gt;</td><td>查找属于指定用户名所有文件</td></tr><tr><td>-size&lt;文件大小&gt;</td><td>按照指定的文件大小查找文件。</td></tr></tbody></table><p>3．案例实操</p><p>（1）按文件名：根据名称查找/目录下的filename.txt文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# find xiyou&#x2F; -name “*.txt”</span><br></pre></td></tr></table></figure><p>（2）按拥有者：查找/opt目录下，用户名称为-user的文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# find xiyou&#x2F; -user atguigu</span><br></pre></td></tr></table></figure><p>​    （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于  -n小于  n等于）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]find &#x2F;home -size +204800</span><br></pre></td></tr></table></figure><h5 id="5-7-2-grep过滤查找及”-”管道符"><a href="#5-7-2-grep过滤查找及”-”管道符" class="headerlink" title="5.7.2 grep过滤查找及”|”管道符"></a>5.7.2 <font color="red">grep过滤查找及”|”管道符</font></h5><p>管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理</p><p>1．基本语法</p><p>grep 选项 查找内容 源文件</p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-n</td><td>显示匹配行及行号。</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）查找某文件在第几行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ls | grep -n test</span><br></pre></td></tr></table></figure><h5 id="5-7-3-which查找命令"><a href="#5-7-3-which查找命令" class="headerlink" title="5.7.3 which查找命令"></a>5.7.3 which查找命令</h5><p>查找命令在那个目录下</p><p>1．基本语法</p><p>which 命令</p><p>2．案例实操</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[luo@hadoop100 shelltest]$ which python</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;python</span><br></pre></td></tr></table></figure><h4 id="5-8-压缩和解压缩"><a href="#5-8-压缩和解压缩" class="headerlink" title="5.8 压缩和解压缩"></a>5.8 压缩和解压缩</h4><h5 id="5-8-1-gzip-gunzip-压缩"><a href="#5-8-1-gzip-gunzip-压缩" class="headerlink" title="5.8.1 gzip/gunzip 压缩"></a>5.8.1 gzip/gunzip 压缩</h5><p>1．基本语法</p><p>gzip 文件        （功能描述：压缩文件，只能将文件压缩为*.gz文件）</p><p>gunzip 文件.gz    （功能描述：解压缩文件命令）</p><p>2．经验技巧</p><p>（1）<strong>只能压缩文件</strong>不能压缩目录</p><p>（2）<strong>不保留原来的文件</strong></p><p>3．案例实操</p><p>（1）gzip压缩</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line"></span><br><span class="line">test.java</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# gzip houge.txt</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ls</span><br></pre></td></tr></table></figure><p>houge.txt.gz</p><p>（2）gunzip解压缩文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# gunzip houge.txt.gz </span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line"></span><br><span class="line">houge.txt</span><br></pre></td></tr></table></figure><h5 id="5-8-2-zip-unzip-压缩"><a href="#5-8-2-zip-unzip-压缩" class="headerlink" title="5.8.2 zip/unzip 压缩"></a>5.8.2 zip/unzip 压缩</h5><p>1．基本语法</p><p>zip  [选项] XXX.zip  将要压缩的内容         （功能描述：压缩文件和目录的命令）</p><p>unzip [选项] XXX.zip                        （功能描述：解压缩文件）</p><p>2．选项说明</p><p>表1-29</p><table><thead><tr><th>zip选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>压缩目录</td></tr></tbody></table><p>表1-30</p><table><thead><tr><th>unzip选项</th><th>功能</th></tr></thead><tbody><tr><td>-d&lt;目录&gt;</td><td>指定解压后文件的存放目录</td></tr></tbody></table><p>3．经验技巧</p><p>zip 压缩命令在window/linux都通用，<strong>可以压缩目录且保留源文件</strong>。</p><p>4．案例实操</p><p>（1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 opt]# touch bailongma.txt</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# zip houma.zip houge.txt bailongma.txt </span><br><span class="line"></span><br><span class="line"> adding: houge.txt (stored 0%)</span><br><span class="line"></span><br><span class="line"> adding: bailongma.txt (stored 0%)</span><br><span class="line"></span><br><span class="line">[root@hadoop101 opt]# ls</span><br><span class="line"></span><br><span class="line">houge.txtbailongma.txthouma.zip </span><br></pre></td></tr></table></figure><p>（2）解压 mypackage.zip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# unzip houma.zip </span><br><span class="line"></span><br><span class="line">Archive:  houma.zip</span><br><span class="line"></span><br><span class="line"> extracting: houge.txt        </span><br><span class="line"></span><br><span class="line"> extracting: bailongma.txt    </span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ls</span><br><span class="line"></span><br><span class="line">houge.txtbailongma.txthouma.zip </span><br></pre></td></tr></table></figure><p>（3）解压mypackage.zip到指定目录-d</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# unzip houma.zip -d &#x2F;opt</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ls &#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure><h5 id="5-8-3-tar-打包"><a href="#5-8-3-tar-打包" class="headerlink" title="5.8.3 tar 打包"></a>5.8.3 <font color="red">tar 打包</font></h5><p>1．基本语法</p><p>tar  [选项]  XXX.tar.gz  将要打包进去的内容        （功能描述：打包目录，压缩后的文件格式.tar.gz）</p><p>zcvf:压缩</p><p>zxvf:解压缩</p><p>2．选项说明</p><p>表1-31</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-z</td><td>打包同时压缩</td></tr><tr><td><strong>-c</strong></td><td><strong>产生.tar打包文件</strong></td></tr><tr><td>-v</td><td>显示详细信息</td></tr><tr><td>-f</td><td>指定压缩后的文件名</td></tr><tr><td><strong>-x</strong></td><td><strong>解包.tar文件</strong></td></tr></tbody></table><p>3．案例实操</p><p>（1）压缩多个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 opt]# tar -zcvf houma.tar.gz houge.txt bailongma.txt </span><br><span class="line"></span><br><span class="line">houge.txt</span><br><span class="line"></span><br><span class="line">bailongma.txt</span><br><span class="line"></span><br><span class="line">[root@hadoop101 opt]# ls</span><br><span class="line"></span><br><span class="line">houma.tar.gz houge.txt bailongma.txt </span><br></pre></td></tr></table></figure><p>（2）压缩目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# tar -zcvf xiyou.tar.gz xiyou&#x2F;</span><br><span class="line"></span><br><span class="line">xiyou&#x2F;</span><br><span class="line"></span><br><span class="line">xiyou&#x2F;mingjie&#x2F;</span><br><span class="line"></span><br><span class="line">xiyou&#x2F;dssz&#x2F;</span><br><span class="line"></span><br><span class="line">xiyou&#x2F;dssz&#x2F;houge.txt</span><br></pre></td></tr></table></figure><p>（3）解压到当前目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# tar -zxvf houma.tar.gz</span><br></pre></td></tr></table></figure><p>（4）解压到指定目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# tar -zxvf xiyou.tar.gz -C &#x2F;opt</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ll &#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure><h4 id="5-9-磁盘分区类"><a href="#5-9-磁盘分区类" class="headerlink" title="5.9 磁盘分区类"></a>5.9 磁盘分区类</h4><h5 id="5-9-1-df-查看磁盘空间使用情况"><a href="#5-9-1-df-查看磁盘空间使用情况" class="headerlink" title="5.9.1 df 查看磁盘空间使用情况"></a>5.9.1 df 查看磁盘空间使用情况</h5><p>df: disk free 空余硬盘</p><p>1．基本语法</p><p>​    df  选项    （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）</p><p>2．选项说明</p><p>表1-32</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-h</td><td>以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）查看磁盘使用情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# df -h</span><br><span class="line"></span><br><span class="line">Filesystem    Size  Used Avail Use% Mounted on</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sda2     15G  3.5G  11G  26% &#x2F;</span><br><span class="line"></span><br><span class="line">tmpfs      939M  224K  939M  1% &#x2F;dev&#x2F;shm</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sda1    190M  39M  142M  22% &#x2F;boot</span><br></pre></td></tr></table></figure><h5 id="5-9-2-fdisk-查看分区"><a href="#5-9-2-fdisk-查看分区" class="headerlink" title="5.9.2 fdisk 查看分区"></a>5.9.2 fdisk 查看分区</h5><p>1．基本语法</p><p>​    fdisk -l            （功能描述：查看磁盘分区详情）</p><p>2．选项说明</p><p>表1-33</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-l</td><td>显示所有硬盘的分区列表</td></tr></tbody></table><p>3．经验技巧</p><p>该命令必须在root用户下才能使用</p><p>4．功能说明</p><p>​    （1）Linux分区</p><ul><li>Device：分区序列</li><li>Boot：引导</li><li>Start：从X磁柱开始</li><li>End：到Y磁柱结束</li><li>Blocks：容量</li><li>Id：分区类型ID</li><li>System：分区类型</li></ul><p>（2）Win7分区</p><p><img src="C:\Users\luo\AppData\Local\Temp\ksohtml15952\wps4.jpg" alt="img"> </p><p>5．案例实操</p><p>​    （1）查看系统分区情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 &#x2F;]# fdisk -l</span><br><span class="line"></span><br><span class="line">Disk &#x2F;dev&#x2F;sda: 21.5 GB, 21474836480 bytes</span><br><span class="line"></span><br><span class="line">255 heads, 63 sectors&#x2F;track, 2610 cylinders</span><br><span class="line"></span><br><span class="line">Units &#x3D; cylinders of 16065 * 512 &#x3D; 8225280 bytes</span><br><span class="line"></span><br><span class="line">Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes</span><br><span class="line"></span><br><span class="line">I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes</span><br><span class="line"></span><br><span class="line">Disk identifier: 0x0005e654</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  Device Boot    Start     End    Blocks  Id  System</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sda1  *      1      26    204800  83  Linux</span><br><span class="line"></span><br><span class="line">Partition 1 does not end on cylinder boundary.</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sda2        26     1332   10485760  83  Linux</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sda3       1332     1593   2097152  82  Linux swap &#x2F; Solaris</span><br></pre></td></tr></table></figure><h5 id="5-9-3-mount-umount挂载-卸载"><a href="#5-9-3-mount-umount挂载-卸载" class="headerlink" title="5.9.3 mount/umount挂载/卸载"></a>5.9.3 mount/umount挂载/卸载</h5><p>​        对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。</p><p>​        Linux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。</p><p>1．挂载前准备（必须要有光盘或者已经连接镜像文件），如图1-158，1-159所示</p><p><img src="C:\Users\luo\AppData\Local\Temp\ksohtml15952\wps5.jpg" alt="img"> </p><p>图1-158</p><p><img src="C:\Users\luo\AppData\Local\Temp\ksohtml15952\wps6.jpg" alt="img"> </p><p>图1-159 挂载镜像文件</p><p>2．基本语法</p><p>mount [-t vfstype] [-o options] device dir    （功能描述：挂载设备）</p><p>umount 设备文件名或挂载点            （功能描述：卸载设备）</p><p>3．参数说明</p><p>表1-34</p><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>-t vfstype</td><td>指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：光盘或光盘镜像：iso9660DOS fat16文件系统：msdos<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">Windows</a> 9x fat32文件系统：vfatWindows NT ntfs文件系统：ntfsMount Windows文件<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">网络</a>共享：smbfs<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">UNIX</a>(LINUX) 文件网络共享：nfs</td></tr><tr><td>-o options</td><td>主要用来描述设备或档案的挂接方式。常用的参数有：loop：用来把一个文件当成硬盘分区挂接上系统ro：采用只读方式挂接设备rw：采用读写方式挂接设备　  iocharset：指定访问文件系统所用字符集</td></tr><tr><td>device</td><td>要挂接(mount)的设备</td></tr><tr><td>dir</td><td>设备在系统上的挂接点(mount point)</td></tr></tbody></table><p>4．案例实操</p><p>（1）挂载光盘镜像文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# mkdir &#x2F;mnt&#x2F;cdrom&#x2F;建立挂载点</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# mount -t iso9660 &#x2F;dev&#x2F;cdrom &#x2F;mnt&#x2F;cdrom&#x2F;设备&#x2F;dev&#x2F;cdrom挂载到 挂载点 ：  &#x2F;mnt&#x2F;cdrom中</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]# ll &#x2F;mnt&#x2F;cdrom&#x2F;</span><br></pre></td></tr></table></figure><p>（2）卸载光盘镜像文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# umount &#x2F;mnt&#x2F;cdrom</span><br></pre></td></tr></table></figure><p>5．设置开机自动挂载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# vi &#x2F;etc&#x2F;fstab</span><br></pre></td></tr></table></figure><p>添加红框中内容，保存退出。</p><p>如图1-160所示</p><p><img src="C:\Users\luo\AppData\Local\Temp\ksohtml15952\wps7.jpg" alt="img"> </p><h4 id="5-10-进程线程类"><a href="#5-10-进程线程类" class="headerlink" title="5.10 进程线程类"></a>5.10 进程线程类</h4><p>进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。</p><h5 id="5-10-1-ps查看当前系统进程状态"><a href="#5-10-1-ps查看当前系统进程状态" class="headerlink" title="5.10.1 ps查看当前系统进程状态"></a>5.10.1 ps查看当前系统进程状态</h5><p>ps:process status 进程状态</p><p>1．基本语法</p><p>​    <strong>ps aux</strong> | grep xxx        （功能描述：查看系统中所有进程）</p><p>​    <strong>ps -ef</strong> | grep xxx    （功能描述：可以查看子父进程之间的关系）</p><p>2．选项说明</p><p>表1-35</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-a</td><td>选择所有进程</td></tr><tr><td>-u</td><td>显示所有用户的所有进程</td></tr><tr><td>-x</td><td>显示没有终端的进程</td></tr></tbody></table><p>3．功能说明</p><p>​    （1）ps aux显示信息说明</p><ul><li>​    USER：该进程是由哪个用户产生的</li><li>​    PID：进程的ID号</li><li>​    %CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；</li><li>​    %MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</li><li>​    VSZ：该进程占用虚拟内存的大小，单位KB；</li><li>​    RSS：该进程占用实际物理内存的大小，单位KB；</li><li>​    TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。</li><li>​    STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台</li><li>​    START：该进程的启动时间</li><li>​    TIME：该进程占用CPU的运算时间，注意不是系统时间</li><li>​    COMMAND：产生此进程的命令名</li></ul><p>（2）ps -ef显示信息说明</p><ul><li><p>UID：用户ID </p></li><li><p>PID：进程ID </p></li><li><p>PPID：父进程ID </p></li><li><p>C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 </p></li><li><p>STIME：进程启动的时间 </p></li><li><p>TTY：完整的终端名称 </p></li><li><p>TIME：CPU时间 </p></li><li><p>CMD：启动进程所用的命令和参数</p></li><li><p>4．经验技巧</p><p>​    如果想查看进程的<strong>CPU占用率和内存占用率</strong>，可以使用aux;</p><p>​    如果想查看<strong>进程的父进程ID</strong>可以使用ef;</p></li></ul><p>5．案例实操</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 datas]# ps aux</span><br></pre></td></tr></table></figure><p>​    如图1-161所示</p><p><img src="C:\Users\luo\AppData\Local\Temp\ksohtml15952\wps8.jpg" alt="img"> </p><p>图1-161 查看进程的CPU占用率和内存占用率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 datas]# ps -ef</span><br></pre></td></tr></table></figure><p>​    如图1-162所示</p><p><img src="C:\Users\luo\AppData\Local\Temp\ksohtml15952\wps9.jpg" alt="img"> </p><p>图1-162 查看进程的父进程ID</p><h5 id="5-10-2-kill终止进程"><a href="#5-10-2-kill终止进程" class="headerlink" title="5.10.2 kill终止进程"></a>5.10.2 kill终止进程</h5><p>1．基本语法</p><p>​    kill  [选项] 进程号        （功能描述：通过进程号杀死进程）</p><p>​    killall 进程名称            （功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用）    </p><p>2．选项说明</p><p>表1-36</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-9</td><td>表示强迫进程立即停止</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）杀死浏览器进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 桌面]# kill -9 5102</span><br></pre></td></tr></table></figure><p>​    （2）通过进程名称杀死进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 桌面]# killall firefox</span><br></pre></td></tr></table></figure><h5 id="5-10-3-pstree查看进程树"><a href="#5-10-3-pstree查看进程树" class="headerlink" title="5.10.3 pstree查看进程树"></a>5.10.3 pstree查看进程树</h5><p>1．基本语法</p><p>​    pstree [选项]</p><p>2．选项说明</p><p>表1-37</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-p</td><td>显示进程的PID</td></tr><tr><td>-u</td><td>显示进程的所属用户</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）显示进程pid</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 datas]# pstree -p</span><br></pre></td></tr></table></figure><p>​    （2）显示进程所属用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 datas]# pstree -u</span><br></pre></td></tr></table></figure><h5 id="5-10-4-top查看系统健康状态"><a href="#5-10-4-top查看系统健康状态" class="headerlink" title="5.10.4 top查看系统健康状态"></a>5.10.4 top查看系统健康状态</h5><p>1．基本命令</p><p>​    top [选项]    </p><p>2．选项说明</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-d 秒数</td><td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td></tr><tr><td>-i</td><td>使top不显示任何闲置或者僵死进程。</td></tr><tr><td>-p</td><td>通过指定监控进程ID来仅仅监控某个进程的状态。</td></tr></tbody></table><p>3．操作说明</p><table><thead><tr><th>操作</th><th>功能</th></tr></thead><tbody><tr><td>P</td><td>以CPU使用率排序，默认就是此项</td></tr><tr><td>M</td><td>以内存的使用率排序</td></tr><tr><td>N</td><td>以PID排序</td></tr><tr><td>q</td><td>退出top</td></tr></tbody></table><p>4．查询结果字段解释</p><p>第一行信息为任务队列信息</p><table><thead><tr><th>内容</th><th>说明</th></tr></thead><tbody><tr><td>12:26:46</td><td>系统当前时间</td></tr><tr><td>up 1 day, 13:32</td><td>系统的运行时间，本机已经运行1天13小时32分钟</td></tr><tr><td>2 users</td><td>当前登录了两个用户</td></tr><tr><td>load  average:  0.00, 0.00, 0.00</td><td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td></tr></tbody></table><p>第二行为进程信息</p><p>表1-41</p><table><thead><tr><th>Tasks:  95 total</th><th>系统中的进程总数</th></tr></thead><tbody><tr><td>1 running</td><td>正在运行的进程数</td></tr><tr><td>94 sleeping</td><td>睡眠的进程</td></tr><tr><td>0 stopped</td><td>正在停止的进程</td></tr><tr><td>0 zombie</td><td>僵尸进程。如果不是0，需要手工检查僵尸进程</td></tr></tbody></table><p>第三行为CPU信息</p><table><thead><tr><th>Cpu(s):  0.1%us</th><th>用户模式占用的CPU百分比</th></tr></thead><tbody><tr><td>0.1%sy</td><td>系统模式占用的CPU百分比</td></tr><tr><td>0.0%ni</td><td>改变过优先级的用户进程占用的CPU百分比</td></tr><tr><td>99.7%id</td><td>空闲CPU的CPU百分比</td></tr><tr><td>0.1%wa</td><td>等待输入/输出的进程的占用CPU百分比</td></tr><tr><td>0.0%hi</td><td>硬中断请求服务占用的CPU百分比</td></tr><tr><td>0.1%si</td><td>软中断请求服务占用的CPU百分比</td></tr><tr><td>0.0%st</td><td>st（Steal  time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td></tr></tbody></table><p>第四行为物理内存信息</p><p>表1-43</p><table><thead><tr><th>Mem:   625344k total</th><th>物理内存的总量，单位KB</th></tr></thead><tbody><tr><td>571504k used</td><td>已经使用的物理内存数量</td></tr><tr><td>53840k free</td><td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td></tr><tr><td>65800k buffers</td><td>作为缓冲的内存数量</td></tr></tbody></table><p>第五行为交换分区（swap）信息</p><p>表1-44</p><table><thead><tr><th>Swap:  524280k total</th><th>交换分区（虚拟内存）的总大小</th></tr></thead><tbody><tr><td>0k used</td><td>已经使用的交互分区的大小</td></tr><tr><td>524280k free</td><td>空闲交换分区的大小</td></tr><tr><td>409280k cached</td><td>作为缓存的交互分区的大小</td></tr></tbody></table><p>5．案例实操</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 atguigu]# top -d 1</span><br><span class="line"></span><br><span class="line">[root@hadoop101 atguigu]# top -i</span><br><span class="line"></span><br><span class="line">[root@hadoop101 atguigu]# top -p 2575</span><br></pre></td></tr></table></figure><p>执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。</p><h5 id="5-10-5-netstat显示网络统计信息和端口占用情况"><a href="#5-10-5-netstat显示网络统计信息和端口占用情况" class="headerlink" title="5.10.5 netstat显示网络统计信息和端口占用情况"></a>5.10.5 netstat显示网络统计信息和端口占用情况</h5><p>1．基本语法</p><p>​    netstat -anp |grep 进程号    （功能描述：查看该进程网络信息）</p><p>​    netstat -nlp    | grep 端口号    （功能描述：查看网络端口号占用情况）</p><p>2．选项说明</p><p>表1-45</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-n</td><td>拒绝显示别名，能显示数字的全部转化成数字</td></tr><tr><td>-l</td><td>仅列出有在listen（监听）的服务状态</td></tr><tr><td>-p</td><td>表示显示哪个进程在调用</td></tr></tbody></table><p>3．案例实操</p><p>（1）通过进程号查看该进程的网络信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 火狐浏览器进程号</span><br><span class="line"></span><br><span class="line">unix  2    [ ACC ]   STREAM   LISTENING   20670 3115&#x2F;firefox     &#x2F;tmp&#x2F;orbit-root&#x2F;linc-c2b-0-5734667cbe29</span><br><span class="line"></span><br><span class="line">unix  3    [ ]     STREAM   CONNECTED   20673  3115&#x2F;firefox     &#x2F;tmp&#x2F;orbit-root&#x2F;linc-c2b-0-5734667cbe29</span><br><span class="line"></span><br><span class="line">unix  3    [ ]     STREAM   CONNECTED   20668  3115&#x2F;firefox     </span><br><span class="line"></span><br><span class="line">unix  3    [ ]     STREAM   CONNECTED   20666  3115&#x2F;firefox   </span><br></pre></td></tr></table></figure><p>（2）查看某端口号是否被占用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 桌面]# netstat -nlp | grep 20670</span><br><span class="line"></span><br><span class="line">unix  2    [ ACC ]   STREAM   LISTENING   20670  3115&#x2F;firefox     &#x2F;tmp&#x2F;orbit-root&#x2F;linc-c2b-0-5734667cbe29</span><br></pre></td></tr></table></figure><h4 id="5-11-crond-系统定时任务"><a href="#5-11-crond-系统定时任务" class="headerlink" title="5.11 crond 系统定时任务"></a>5.11 crond 系统定时任务</h4><h5 id="5-11-1-crond-服务管理"><a href="#5-11-1-crond-服务管理" class="headerlink" title="5.11.1 crond 服务管理"></a>5.11.1 crond 服务管理</h5><p>1．重新启动crond服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# service crond restart</span><br></pre></td></tr></table></figure><h5 id="5-11-2-crontab-定时任务设置"><a href="#5-11-2-crontab-定时任务设置" class="headerlink" title="5.11.2 crontab 定时任务设置"></a>5.11.2 crontab 定时任务设置</h5><p>1．基本语法</p><p>crontab [选项]</p><p>2．选项说明</p><p>表1-46</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-e</td><td>编辑crontab定时任务</td></tr><tr><td>-l</td><td>查询crontab任务</td></tr><tr><td>-r</td><td>删除当前用户所有的crontab任务</td></tr></tbody></table><p>3．参数说明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]# crontab -e </span><br></pre></td></tr></table></figure><p>（1）进入crontab编辑界面。会打开vim编辑你的工作。</p><p>* * * * * 执行的任务</p><p>表1-47</p><table><thead><tr><th>项目</th><th>含义</th><th>范围</th></tr></thead><tbody><tr><td>第一个“*”</td><td>一小时当中的第几分钟</td><td>0-59</td></tr><tr><td>第二个“*”</td><td>一天当中的第几小时</td><td>0-23</td></tr><tr><td>第三个“*”</td><td>一个月当中的第几天</td><td>1-31</td></tr><tr><td>第四个“*”</td><td>一年当中的第几月</td><td>1-12</td></tr><tr><td>第五个“*”</td><td>一周当中的星期几</td><td>0-7（0和7都代表星期日）</td></tr></tbody></table><p>（2）特殊符号</p><p>表1-48</p><table><thead><tr><th>特殊符号</th><th>含义</th></tr></thead><tbody><tr><td>*</td><td>代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。</td></tr><tr><td>，</td><td>代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td></tr><tr><td>-</td><td>代表连续的时间范围。比如“0 5  *  *  1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td></tr><tr><td>*/n</td><td>代表每隔多久执行一次。比如“*/10  *  *  *  *  命令”，代表每隔10分钟就执行一遍命令</td></tr></tbody></table><p>（3）特定时间执行命令</p><p>表1-49</p><table><thead><tr><th>时间</th><th>含义</th></tr></thead><tbody><tr><td>45 22 * * * 命令</td><td>在22点45分执行命令</td></tr><tr><td>0 17 * * 1 命令</td><td>每周1 的17点0分执行命令</td></tr><tr><td>0 5 1,15 * * 命令</td><td>每月1号和15号的凌晨5点0分执行命令</td></tr><tr><td>40 4 * * 1-5 命令</td><td>每周一到周五的凌晨4点40分执行命令</td></tr><tr><td>*/10 4 * * * 命令</td><td>每天的凌晨4点，每隔10分钟执行一次命令</td></tr><tr><td>0 0 1,15 * 1 命令</td><td>每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。</td></tr></tbody></table><p>4．案例实操</p><p>​    （1）每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*&#x2F;1 * * * * &#x2F;bin&#x2F;echo ”11” &gt;&gt; &#x2F;root&#x2F;bailongma.txt</span><br></pre></td></tr></table></figure><h3 id="六、软件包管理"><a href="#六、软件包管理" class="headerlink" title="六、软件包管理"></a>六、软件包管理</h3><h4 id="6-1-RPM"><a href="#6-1-RPM" class="headerlink" title="6.1 RPM"></a>6.1 RPM</h4><h5 id="6-1-1-RPM概述"><a href="#6-1-1-RPM概述" class="headerlink" title="6.1.1 RPM概述"></a>6.1.1 RPM概述</h5><p>RPM（RedHat Package Manager），RedHat软件包管理工具，类似windows里面的setup.exe</p><p> 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。</p><p>RPM包的名称格式</p><p>Apache-1.3.23-11.i386.rpm</p><ul><li> “apache” 软件名称</li><li> “1.3.23-11”软件的版本号，主版本和此版本</li><li> “i386”是软件所运行的硬件平台，Intel 32位微处理器的统称</li><li> “rpm”文件扩展名，代表RPM包</li></ul><h5 id="6-1-2-RPM查询命令（rpm-qa）"><a href="#6-1-2-RPM查询命令（rpm-qa）" class="headerlink" title="6.1.2 RPM查询命令（rpm -qa）"></a>6.1.2 <font color="red">RPM查询命令（rpm -qa）</font></h5><p>1．基本语法</p><p>rpm -qa                （功能描述：查询所安装的所有rpm软件包）</p><p>2．经验技巧</p><p>由于软件包比较多，一般都会采取过滤。<strong>rpm -qa | grep rpm</strong>软件包</p><p>3．案例实操</p><p>​    （1）查询firefox软件安装情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 Packages]# rpm -qa |grep firefox </span><br><span class="line"></span><br><span class="line">firefox-45.0.1-1.el6.centos.x86_64</span><br></pre></td></tr></table></figure><h5 id="6-1-3-RPM卸载命令（rpm-e）"><a href="#6-1-3-RPM卸载命令（rpm-e）" class="headerlink" title="6.1.3 RPM卸载命令（rpm -e）"></a>6.1.3 <font color="red">RPM卸载命令（rpm -e）</font></h5><p>1．基本语法</p><p>（1）rpm -e RPM软件包  </p><p>（2） <strong>rpm -e –nodeps 软件包</strong> </p><p>2．选项说明</p><p>表1-50</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-e</td><td>卸载软件包</td></tr><tr><td>–nodeps</td><td>卸载软件时，不检查依赖。这样的话，那些使用该软件包的软件在此之后可能就不能正常工作了。</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）卸载firefox软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 Packages]# rpm -e firefox</span><br></pre></td></tr></table></figure><h5 id="6-1-4-RPM安装命令（rpm-ivh）"><a href="#6-1-4-RPM安装命令（rpm-ivh）" class="headerlink" title="6.1.4 RPM安装命令（rpm -ivh）"></a>6.1.4 <font color="red">RPM安装命令（rpm -ivh）</font></h5><p>1．基本语法</p><p>​    <strong>rpm -ivh RPM包全名</strong></p><p>2．选项说明</p><p>表1-51</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-i</td><td>-i=install，安装</td></tr><tr><td>-v</td><td>-v=verbose，显示详细信息</td></tr><tr><td>-h</td><td>-h=hash，进度条</td></tr><tr><td>–nodeps</td><td>–nodeps，不检测依赖进度</td></tr></tbody></table><p>3．案例实操</p><p>​    （1）安装firefox软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 Packages]# pwd</span><br><span class="line"></span><br><span class="line">&#x2F;media&#x2F;CentOS_6.8_Final&#x2F;Packages</span><br><span class="line"></span><br><span class="line">[root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm </span><br><span class="line"></span><br><span class="line">warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA&#x2F;SHA1 Signature, key ID c105b9de: NOKEY</span><br><span class="line"></span><br><span class="line">Preparing...         ########################################### [100%]</span><br><span class="line"></span><br><span class="line">  1:firefox         ########################################### [100%]</span><br></pre></td></tr></table></figure><h4 id="6-2-YUM仓库配置"><a href="#6-2-YUM仓库配置" class="headerlink" title="6.2 YUM仓库配置"></a>6.2 <font color="red">YUM仓库配置</font></h4><h5 id="6-2-1-YUM概述"><a href="#6-2-1-YUM概述" class="headerlink" title="6.2.1 YUM概述"></a>6.2.1 YUM概述</h5><p>​        YUM（全称为 YeMllow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。</p><p><img src="https://i.loli.net/2020/10/27/qIiHdDa8v2BpmhK.png" alt="img"></p><h5 id="6-2-2-YUM的常用命令"><a href="#6-2-2-YUM的常用命令" class="headerlink" title="6.2.2 YUM的常用命令"></a>6.2.2 YUM的常用命令</h5><p>1．基本语法</p><p>​    yum [选项] [参数]</p><p>2．选项说明</p><p>表1-52</p><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-y</td><td>对所有提问都回答“yes”</td></tr></tbody></table><p>3．参数说明</p><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>install</td><td>安装rpm软件包</td></tr><tr><td>update</td><td>更新rpm软件包</td></tr><tr><td>check-update</td><td>检查是否有可用的更新rpm软件包</td></tr><tr><td>remove</td><td>删除指定的rpm软件包</td></tr><tr><td>list</td><td>显示软件包信息</td></tr><tr><td>clean</td><td>清理yum过期的缓存</td></tr><tr><td>deplist</td><td>显示yum软件包的所有依赖关系</td></tr></tbody></table><p>4．案例实操实操</p><p>​    （1）采用yum方式安装firefox</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 ~]#yum -y install firefox.x86_64</span><br></pre></td></tr></table></figure><h5 id="6-2-3-修改网络YUM源"><a href="#6-2-3-修改网络YUM源" class="headerlink" title="6.2.3 修改网络YUM源"></a>6.2.3 修改网络YUM源</h5><p>默认的系统YUM源，需要连接国外apache网站，网速比较慢，可以修改关联的网络YUM源为国内镜像的网站，比如网易163。</p><p>1．前期文件准备</p><p>（1）前提条件linux系统必须可以联网</p><p>（2）在Linux环境中访问该网络地址：<a href="http://mirrors.163.com/.help/centos.html%EF%BC%8C%E5%9C%A8%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%E4%B8%AD%E7%82%B9%E5%87%BBCentOS6-&gt;%E5%86%8D%E7%82%B9%E5%87%BB%E4%BF%9D%E5%AD%98%EF%BC%8C%E5%A6%82%E5%9B%BE1-164%E6%89%80%E7%A4%BA">http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存，如图1-164所示</a> </p><p>图1-164 下载CentOS6</p><p>（3）查看文件保存的位置，如图1-165，1-166所示</p><p>在打开的终端中输入如下命令，就可以找到文件的保存位置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop101 下载]$ pwd&#x2F;home&#x2F;atguigu&#x2F;下载</span><br></pre></td></tr></table></figure><p>2．替换本地yum文件</p><p>​    （1）把下载的文件移动到/etc/yum.repos.d/目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 下载]# mv CentOS6-Base-163.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br></pre></td></tr></table></figure><p>​    （2）进入到/etc/yum.repos.d/目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 yum.repos.d]# pwd&#x2F;etc&#x2F;yum.repos.d</span><br></pre></td></tr></table></figure><p>​    （3）用CentOS6-Base-163.repo替换CentOS-Base.repo</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo  CentOS-Base.repo</span><br></pre></td></tr></table></figure><p>3．安装命令</p><p>（1）[root@hadoop101 yum.repos.d]#yum clean all</p><p>（2）[root@hadoop101 yum.repos.d]#yum makecache</p><p>yum makecache就是把服务器的包信息下载到本地电脑缓存起来</p><p>4．测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 yum.repos.d]#yum list | grep firefox</span><br><span class="line"></span><br><span class="line">[root@hadoop101 ~]#yum -y install firefox.x86_64</span><br></pre></td></tr></table></figure><h3 id="七、面试题"><a href="#七、面试题" class="headerlink" title="七、面试题"></a>七、面试题</h3><h4 id="7-1-Linux常用命令"><a href="#7-1-Linux常用命令" class="headerlink" title="7.1 Linux常用命令"></a>7.1 Linux常用命令</h4><p>find、df、tar、ps、top、netstat等</p><h4 id="7-2-Linux查看内存、磁盘存储、io读写、端口占用、进程"><a href="#7-2-Linux查看内存、磁盘存储、io读写、端口占用、进程" class="headerlink" title="7.2 Linux查看内存、磁盘存储、io读写、端口占用、进程"></a>7.2 Linux查看内存、磁盘存储、io读写、端口占用、进程</h4><p>查看内存：top</p><p>查看磁盘存储情况：df -h</p><p>查看磁盘IO读写情况：iotop</p><p>查看端口占用情况：netstat -tunlp | grep 端口号</p><p>查看进程：ps aux</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 Linux文件与目录结构 / 第二章 VI/VIM编辑器 /&lt;br&gt;第三章 网络配置和系统管理操作/ 第四章 远程登陆(XShell)/&lt;br&gt;第五章 常用基本命令 / 第六章 软件包管理/&lt;br&gt;第七章 面试题/&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="http://luo6656.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>文本特征提取之TF-IDF</title>
    <link href="http://luo6656.github.io/2019/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%8BTFIDF/"/>
    <id>http://luo6656.github.io/2019/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%8BTFIDF/</id>
    <published>2019-07-25T16:00:00.000Z</published>
    <updated>2020-10-27T06:44:41.994Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><ul><li>是一种加权技术。采用一种统计方法，根据字词在文本中出现的次数和在整个语料中出现的文档频率来计算一个字词在整个语料中的重要程度。</li><li>主要思想：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</li><li>作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的<font color=red>重要程度</font></li><li>优点：能过滤掉一些常见的却无关紧要的词语，同时保留影响整个文本的重要词语。<a id="more"></a></li><li>公式如下：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy84NDE1OTc3LTkyM2VmYTM5MjYzMTQ0MjYucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></li></ul><p>tfidfi,j表示词频 tfi,j 和倒文本词频idfi,j的乘积。TF-IDF值越大，表示该特征词对这个文本的重要性越大。</p><hr><h4 id="TF"><a href="#TF" class="headerlink" title="TF"></a>TF</h4><ul><li><p><em>TF（Term Frequency）：表示某个关键词在整篇文章中出现的频率。</em></p></li><li><p>计算公式：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy84NDE1OTc3LTE5Zjk5MzAwYWRhZGIxNjMucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></p></li><li><p>其中，分子为特征词  t 在文本 dj 中出现的次数，分母则是文本dj中所有特征词的个数。计算的结果即为某个特征词的词频。</p></li></ul><hr><h4 id="IDF"><a href="#IDF" class="headerlink" title="IDF"></a>IDF</h4><ul><li>IDF（Invers Document Frequency）：表示计算倒文本频率。（文本频率是指某个关键词在整个语料所有文章中出现的次数。倒文本频率顾名思义，它是文本频率的倒数，主要用于降低所有文档中一些常见却对文档影响不大的词语的作用。）</li><li>计算公式：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy84NDE1OTc3LTk3MGJmNTk5NjU2OTc5YjgucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></li><li>|D| 表示语料中文本的总数，|Dti| 表示文本中包含特征词ti的数量。为防止该词语在语料库中不存在，即分母为0，使用 1+|Dti| 作为分母</li></ul><h4 id="sklearn工具包下的tfidf"><a href="#sklearn工具包下的tfidf" class="headerlink" title="sklearn工具包下的tfidf"></a>sklearn工具包下的tfidf</h4><ul><li><code>sklearn.feature_extraction.text.TfidfVectorizer</code></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;TF-IDF&quot;&gt;&lt;a href=&quot;#TF-IDF&quot; class=&quot;headerlink&quot; title=&quot;TF-IDF&quot;&gt;&lt;/a&gt;TF-IDF&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;是一种加权技术。采用一种统计方法，根据字词在文本中出现的次数和在整个语料中出现的文档频率来计算一个字词在整个语料中的重要程度。&lt;/li&gt;
&lt;li&gt;主要思想：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。&lt;/li&gt;
&lt;li&gt;作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的&lt;font color=red&gt;重要程度&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;优点：能过滤掉一些常见的却无关紧要的词语，同时保留影响整个文本的重要词语。</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习之朴素贝叶斯算法</title>
    <link href="http://luo6656.github.io/2019/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>http://luo6656.github.io/2019/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</id>
    <published>2019-07-24T16:00:00.000Z</published>
    <updated>2020-10-27T06:39:23.458Z</updated>
    
    <content type="html"><![CDATA[<h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><h4 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h4><ul><li>联合概率<ul><li>定义：包含多个条件，且所有条件同时成立的概率。</li><li>记作：P(A,B)</li><li>P(A,B) = P(A)P(B)</li></ul></li></ul><a id="more"></a><ul><li><p>条件概率</p><ul><li>定义：就是事件A在另外一个事件B已经发生条件下的发生概率</li><li>记作：P(A|B)</li><li>P(A1,A2|B) = P(A1|B)P(A2|B)</li><li><font color=red>注意</font>：此条件概率的成立，是由于A1,A2相互独立的结果</li></ul></li><li><p><font color=red>朴素贝叶斯算法要求各个特征之间相互独立</font></p></li></ul><h4 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h4><ul><li></li><li><img src="https://img-blog.csdnimg.cn/20190915103849487.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE3MDg2Mw==,size_16,color_FFFFFF,t_70" alt="朴素贝叶斯公式"></li></ul><h4 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h4><ul><li>由于可能存在为0的类别，所以使用拉普拉斯平滑系数</li><li>alpha<br><img src="https://img-blog.csdnimg.cn/20190915105008920.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE3MDg2Mw==,size_16,color_FFFFFF,t_70" alt="拉普拉斯平滑系数"></li></ul><h4 id="sklearn朴素贝叶斯算法API"><a href="#sklearn朴素贝叶斯算法API" class="headerlink" title="sklearn朴素贝叶斯算法API"></a>sklearn朴素贝叶斯算法API</h4><ul><li><code>sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</code></li><li>alpha不会对结果产生影响</li></ul><h4 id="朴素贝叶斯分类的优缺点"><a href="#朴素贝叶斯分类的优缺点" class="headerlink" title="朴素贝叶斯分类的优缺点"></a>朴素贝叶斯分类的优缺点</h4><ul><li>优点<ul><li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。</li><li>对缺失值不太敏感，算法比较简单，常用于文本分类。</li><li>分类准确度高，速度快</li></ul></li><li>缺点<ul><li>由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好</li><li>是在训练集中进行统计词这些工作会对结果造成干扰。</li><li>一般只适用于文本分类</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;朴素贝叶斯算法&quot;&gt;&lt;a href=&quot;#朴素贝叶斯算法&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯算法&quot;&gt;&lt;/a&gt;朴素贝叶斯算法&lt;/h3&gt;&lt;h4 id=&quot;概率基础&quot;&gt;&lt;a href=&quot;#概率基础&quot; class=&quot;headerlink&quot; title=&quot;概率基础&quot;&gt;&lt;/a&gt;概率基础&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;联合概率&lt;ul&gt;
&lt;li&gt;定义：包含多个条件，且所有条件同时成立的概率。&lt;/li&gt;
&lt;li&gt;记作：P(A,B)&lt;/li&gt;
&lt;li&gt;P(A,B) = P(A)P(B)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习之决策树</title>
    <link href="http://luo6656.github.io/2019/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://luo6656.github.io/2019/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2019-07-24T16:00:00.000Z</published>
    <updated>2020-10-27T06:39:23.303Z</updated>
    
    <content type="html"><![CDATA[<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="认识决策树"><a href="#认识决策树" class="headerlink" title="认识决策树"></a>认识决策树</h4><ul><li>决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法</li><li>信息的单位：比特bite</li></ul><a id="more"></a><h4 id="信息熵和香农定理"><a href="#信息熵和香农定理" class="headerlink" title="信息熵和香农定理"></a>信息熵和香农定理</h4><ul><li><a href="https://blog.csdn.net/dyx810601/article/details/82226456">https://blog.csdn.net/dyx810601/article/details/82226456</a><ul><li>信息熵在信息传递和压缩中常见<img src="https://img-blog.csdnimg.cn/20190907100247908.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE3MDg2Mw==,size_16,color_FFFFFF,t_70" alt="信息熵"></li></ul></li><li>当得到一些信息时信息熵就减小了</li><li>信息和消除不确定性是相联系的：<font color=red>信息熵越大，不确定性越大</font></li><li>决策树中把能减少更多的不确定性(信息熵)放在根上</li></ul><h4 id="决策树的划分依据之一-信息增益"><a href="#决策树的划分依据之一-信息增益" class="headerlink" title="决策树的划分依据之一:信息增益"></a>决策树的划分依据之一:<font color=red>信息增益</font></h4><ul><li>信息增益：当得知一个特征条件之后，减少的信息熵的大小</li><li>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：<font color=red>g(D,A)=H(D)-H(D|A)</font></li></ul><h4 id="常见决策树使用的算法"><a href="#常见决策树使用的算法" class="headerlink" title="常见决策树使用的算法"></a>常见决策树使用的算法</h4><ul><li>ID3<ul><li><font color=red>信息增益 - 最大的准则</font></li></ul></li><li>C4.5<ul><li>信息增益比 - 最大的准则</li></ul></li><li>CART<ul><li>回归树：平方误差最小</li><li>分类树：基尼系数 最小的准则 在sklearn中可以选择划分的默认原则 划分更加仔细</li></ul></li><li>基尼系数<br><img src="https://img-blog.csdnimg.cn/20190915164223928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE3MDg2Mw==,size_16,color_FFFFFF,t_70" alt="基尼系数"><h4 id="sklearn决策树API"><a href="#sklearn决策树API" class="headerlink" title="sklearn决策树API"></a>sklearn决策树API</h4></li><li><code>sklearn.tree.DecisionTreeClassifier(criterion=&#39;gini&#39;,max_depth=None,random_state=None)</code></li><li>决策树分类器</li><li><code>criterion</code>:默认是’gini’系数，也可以选择信息增益的熵‘entropy’</li><li><code>max_depth</code>:树的深度大小</li><li><code>random_state</code>:随机数种子</li><li><code>method</code>：</li><li><code>decision_path</code>:返回决策树的路径</li></ul><h4 id="决策树的结构、本地保存"><a href="#决策树的结构、本地保存" class="headerlink" title="决策树的结构、本地保存"></a>决策树的结构、本地保存</h4><ul><li><code>sklearn.tree.export_graphviz()</code> 该函数能够导出DOT格式</li><li><code>tree,export_graphviz(estimator,out_flie=&#39;tree.dot&#39;,feature_names=[&quot;,&quot;])</code></li><li>查看dot文件的工具：graphviz</li></ul><h4 id="信息论基础–银行贷款分析"><a href="#信息论基础–银行贷款分析" class="headerlink" title="信息论基础–银行贷款分析"></a>信息论基础–银行贷款分析</h4><ul><li>特征<ul><li>年龄；是否有工作；是否有房子；信贷情况；类别</li></ul></li></ul><h4 id="决策树的优缺点以及改进"><a href="#决策树的优缺点以及改进" class="headerlink" title="决策树的优缺点以及改进"></a>决策树的优缺点以及改进</h4><ul><li><p>优点：</p><ul><li>简单的理解和解释，树木可视化。</li><li>需要很少的数据准备，<font color=red>技术通常需要数据归一化</font></li><li>在企业重要决策中，由于决策树很好的分析能力，在决策过程中应用较多，可以找到主要决策因素</li></ul></li><li><p>缺点</p><ul><li>决策树学习者可以创建不能很好地推广数据地过于复杂的树，这被称为过拟合</li></ul></li><li><p>改进</p><ul><li>减枝cart算法(<font color=red>决策树API当中已经实现，随机森林参数调优</font>):小于参数的sample分支不要</li><li><font color=red>随机森林</font></li></ul></li></ul><h4 id="集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林"><a href="#集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林" class="headerlink" title="集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林"></a>集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林</h4><ul><li>集成学习方法：通过建立几个模型组合来解决单一预测问题。它的工作原理是<font color=red>生成多个分类/模型</font>,各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类地做出预测</li><li>定义：在机器学习中，<font color=red>随机森林</font>是一个包含多个决策树地分类器，并且其输出地类别是由个别输出的类别的众数而定。</li><li>例如：如果你训练了5个树，其中有4个数的结果是True，1个是False，那么最终结果会是True</li></ul><h4 id="随机森林建立多个决策树的过程"><a href="#随机森林建立多个决策树的过程" class="headerlink" title="随机森林建立多个决策树的过程"></a>随机森林建立多个决策树的过程</h4><ul><li>单个树的建立过程<ul><li>1.随机在N个样本中选择一个样本,重复N次(样本有可能重复，<font color=red>有放回</font>)</li><li>2.随机在M个特征中选择m个特征，m&lt;&lt;M</li></ul></li><li>建立多颗决策树，样本特征大多不一样</li><li>bootstrap:随机有放回抽样</li><li>为什么要随机抽样训练集<ul><li>如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出来的决策树的分类结果也完全一样</li></ul></li><li>为什么要有放回地抽样<ul><li>如果不是有放回地抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”,都是绝对”片面的“，也就是说每棵树训练出来都是有很大差异的；而随机森林最后分类取决于多棵树(弱分类器)的投票表决。</li></ul></li></ul><h4 id="随机森林API"><a href="#随机森林API" class="headerlink" title="随机森林API"></a>随机森林API</h4><ul><li><code>sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion=&#39;gini&#39;,max_depth=None,bootstrap=True,random_state=None)</code></li><li><code>n_estimators</code>:森林里的树木的数量，default=10,有120,200,300,500,800,1200</li><li><code>criteria</code>:分割特征的测量方法，default=”gini”</li><li><code>max_depth</code>:树的最大深度，default为无</li><li><code>max_features=&quot;auto&quot;</code>,每个决策树的最大特征数量，有”sqrt”,”log2”,”None”</li><li><code>boostrap</code>:是否在构建树时放回抽样，default=True<h4 id="随机森林的优点"><a href="#随机森林的优点" class="headerlink" title="随机森林的优点"></a>随机森林的优点</h4></li><li>在当前所有算法中，具有极好的准确率</li><li>能够有效地运行在大数据集上:样本多，特征多</li><li>能够处理具有高维特征的输入样本，而且不需要降维</li><li>能够评估各个特征在分类问题上的重要性</li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h3&gt;&lt;h4 id=&quot;认识决策树&quot;&gt;&lt;a href=&quot;#认识决策树&quot; class=&quot;headerlink&quot; title=&quot;认识决策树&quot;&gt;&lt;/a&gt;认识决策树&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法&lt;/li&gt;
&lt;li&gt;信息的单位：比特bite&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习：数据预处理之One-Hot编码</title>
    <link href="http://luo6656.github.io/2019/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%88One-Hot)/"/>
    <id>http://luo6656.github.io/2019/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%88One-Hot)/</id>
    <published>2019-07-15T16:00:00.000Z</published>
    <updated>2020-10-27T06:49:17.777Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据预处理之独热编码-One-Hot）"><a href="#数据预处理之独热编码-One-Hot）" class="headerlink" title="数据预处理之独热编码(One-Hot）"></a>数据预处理之独热编码(One-Hot）</h3><h4 id="转载https-www-imooc-com-article-35900"><a href="#转载https-www-imooc-com-article-35900" class="headerlink" title="转载https://www.imooc.com/article/35900"></a>转载<a href="https://www.imooc.com/article/35900">https://www.imooc.com/article/35900</a></h4><p>在机器学习算法中，我们经常会遇到分类特征，例如：人的性别有男女，祖国有中国，美国，法国等。<br>这些特征值并不是连续的，而是离散的，无序的。通常我们需要对其进行特征数字化。</p><p>那什么是特征数字化呢？例子如下：</p><a id="more"></a><p>性别特征：[“男”，”女”]</p><p>祖国特征：[“中国”，”美国，”法国”]</p><p>运动特征：[“足球”，”篮球”，”羽毛球”，”乒乓球”]</p><p>假如某个样本（某个人），他的特征是这样的[“男”,”中国”,”乒乓球”]，我们可以用 [0,0,4] 来表示，但是这样的特征处理并不能直接放入机器学习算法中。因为类别之间是无序的（运动数据就是任意排序的）。</p><p>什么是独热编码（One-Hot）？</p><p>————————————————————————————————————————</p><p>One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。</p><p>One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。</p><p>One-Hot实际案例</p><p>————————————————————————————————————————</p><p>就拿上面的例子来说吧，性别特征：[“男”,”女”]，按照N位状态寄存器来对N个状态进行编码的原理，咱们处理后应该是这样的（这里只有两个特征，所以N=2）：</p><p>男  =&gt;  10</p><p>女  =&gt;  01</p><p>祖国特征：[“中国”，”美国，”法国”]（这里N=3）：</p><p>中国  =&gt;  100</p><p>美国  =&gt;  010</p><p>法国  =&gt;  001</p><p>运动特征：[“足球”，”篮球”，”羽毛球”，”乒乓球”]（这里N=4）：</p><p>足球  =&gt;  1000</p><p>篮球  =&gt;  0100</p><p>羽毛球  =&gt;  0010</p><p>乒乓球  =&gt;  0001</p><p>所以，当一个样本为[“男”,”中国”,”乒乓球”]的时候，完整的特征数字化的结果为：</p><p>[1，0，1，0，0，0，0，0，1]</p><p>下图可能会更好理解：</p><p><a href="https://img.mukewang.com/5b20f1b90001cc2202550045.jpg">https://img.mukewang.com/5b20f1b90001cc2202550045.jpg</a></p><p>One-Hot在python中的使用</p><p>————————————————————————————————————————</p><p>from sklearn import preprocessing  </p><p>enc = preprocessing.OneHotEncoder()<br>enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])  #这里一共有4个数据，3种特征</p><p>array = enc.transform([[0,1,3]]).toarray()  #这里使用一个新的数据来测试</p><p>print array   # [[ 1  0  0  1  0  0  0  0  1]]<br>结果为 1 0 0 1 0 0 0 0 1</p><p>为什么使用one-hot编码来处理离散型特征?<br>————————————————————————————————————————</p><p>在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。</p><p>而我们使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。</p><p>将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。</p><p>比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。</p><p>不需要使用one-hot编码来处理的情况<br>————————————————————————————————————————</p><p>将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。</p><p>比如，该离散特征共有1000个取值，我们分成两组，分别是400和600,两个小组之间的距离有合适的定义，组内的距离也有合适的定义，那就没必要用one-hot 编码。</p><p>离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。</p><p>作者：NateHuang<br>链接：<a href="https://www.imooc.com/article/35900">https://www.imooc.com/article/35900</a><br>来源：慕课网<br>本文原创发布于慕课网 ，转载请注明出处，谢谢合作</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;数据预处理之独热编码-One-Hot）&quot;&gt;&lt;a href=&quot;#数据预处理之独热编码-One-Hot）&quot; class=&quot;headerlink&quot; title=&quot;数据预处理之独热编码(One-Hot）&quot;&gt;&lt;/a&gt;数据预处理之独热编码(One-Hot）&lt;/h3&gt;&lt;h4 id=&quot;转载https-www-imooc-com-article-35900&quot;&gt;&lt;a href=&quot;#转载https-www-imooc-com-article-35900&quot; class=&quot;headerlink&quot; title=&quot;转载https://www.imooc.com/article/35900&quot;&gt;&lt;/a&gt;转载&lt;a href=&quot;https://www.imooc.com/article/35900&quot;&gt;https://www.imooc.com/article/35900&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;在机器学习算法中，我们经常会遇到分类特征，例如：人的性别有男女，祖国有中国，美国，法国等。&lt;br&gt;这些特征值并不是连续的，而是离散的，无序的。通常我们需要对其进行特征数字化。&lt;/p&gt;
&lt;p&gt;那什么是特征数字化呢？例子如下：&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习之特征预处理</title>
    <link href="http://luo6656.github.io/2019/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://luo6656.github.io/2019/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/</id>
    <published>2019-07-14T16:00:00.000Z</published>
    <updated>2020-10-27T06:46:01.394Z</updated>
    
    <content type="html"><![CDATA[<h3 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h3><p><font color =red>通过特定的统计方法(数学方法)将数据转化成算法要求的数据</font></p><h4 id="数值型数据"><a href="#数值型数据" class="headerlink" title="数值型数据"></a>数值型数据</h4><ul><li>标准缩放：<ul><li>1.归一化</li><li>2.标准化</li><li>3.缺失值</li></ul></li></ul><h4 id="类别型数据"><a href="#类别型数据" class="headerlink" title="类别型数据"></a>类别型数据</h4><ul><li>one-hot 编码</li><li>关于one-hot编码：<a href="https://blog.csdn.net/weixin_43170863/article/details/100184168">https://blog.csdn.net/weixin_43170863/article/details/100184168</a></li></ul><h4 id="时间类型"><a href="#时间类型" class="headerlink" title="时间类型"></a>时间类型</h4><ul><li>时间的切分</li></ul><h4 id="sklearn特征处理API"><a href="#sklearn特征处理API" class="headerlink" title="sklearn特征处理API"></a>sklearn特征处理API</h4><ul><li>sklearn.preprocessing</li></ul><hr><a id="more"></a><h4 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h4><ul><li><p><font color=red>特点：通过对原始数据进行变换把数据映射到(默认为[0,1])之间</font></p></li><li><p>公式：<br><img src="https://img-blog.csdn.net/20180811231252291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dqcGxlYXJuaW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br>X = Xnorm*(mx-mi)+mi</p></li><li><p>注：作用于每一列，max为一列的最大值，min为一列的最小值，X为最终结果，mx，mi分别为指定区间值默认mx为1，mi为0</p></li><li><p>缺点：在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受<font color=red>异常点</font>影响，所以这种方法鲁棒性较差，只适合<font color=red>传统精确小数据场景。</font></p></li><li><p>鲁棒性—-反应产品的稳定性</p></li></ul><h4 id="sklearn归一化API"><a href="#sklearn归一化API" class="headerlink" title="sklearn归一化API"></a>sklearn归一化API</h4><ul><li><code>sklearn.preprocessing.MinMaxScalar</code></li><li><code>MinMaxScalar(feature_range=(0,1))...</code><ul><li>每个特征缩放到给定范围</li></ul></li><li><code>MinMaxScalar.fit_transform(X)</code><ul><li>X:numpy array格式的数据[n_samples,n_features]</li><li>返回值：转换后的形状相同的array<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">guiyi</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    归一化处理</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    array1 = [[<span class="number">100</span>,<span class="number">4</span>,<span class="number">15</span>,<span class="number">45</span>],[<span class="number">63</span>,<span class="number">41</span>,<span class="number">25</span>,<span class="number">34</span>],[<span class="number">34</span>,<span class="number">31</span>,<span class="number">130</span>,<span class="number">34</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化</span></span><br><span class="line">    mm = MinMaxScaler(feature_range=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    data = mm.fit_transform(array1)</span><br><span class="line">    print(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h4><ul><li>特点：通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内</li><li>公式<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTM1NTM4Ny8yMDE4MDUvMTM1NTM4Ny0yMDE4MDUyNTEyNTc0NDI4MC0xNDY5NDM1MzQucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></li><li>注：作用于每一列，Xmean为平均值，S为标准差</li><li>优点：如果出现异常点，由于<font color=red>具有一定数据量，少量的异常点对于平均值的影响并不大</font>，从而方差改变较小。</li><li>在已有<font color=red>样本足够多的情况下比较稳定</font>,适合现代嘈杂大数据场景。</li></ul><h4 id="sklearn标准化API"><a href="#sklearn标准化API" class="headerlink" title="sklearn标准化API"></a>sklearn标准化API</h4><ul><li><code>sklearn.preprocessing.StandardScaler</code><ul><li>处理后每列来说所有数据都聚集在<font color=red>均值0附近标准差为1</font></li></ul></li><li><code>StandardScaler.fit_transform(X)</code></li><li><code>StandardScaler.mean_</code>    原始数据中每列特征的平均值</li><li><code>StandardScaler.std_</code>   原始数据每列特征的方差<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stand</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    标准化缩放</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    list1 = [[<span class="number">4</span>,<span class="number">-1</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">-4</span>,<span class="number">2</span>],[<span class="number">6</span>,<span class="number">6</span>,<span class="number">-1</span>]]</span><br><span class="line">    std = StandardScaler()</span><br><span class="line">    data = std.fit_transform(list1)</span><br><span class="line">    print(data)</span><br><span class="line">    print(std.mean_)</span><br><span class="line">    print(std.scale_)</span><br><span class="line">    <span class="keyword">return</span> NaN</span><br></pre></td></tr></table></figure></li></ul><h4 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h4><ul><li>删除</li><li>插补：添补平均值或中位数</li><li>sklearn缺失值API<ul><li><code>sklearn.preprocessing.Imputer</code></li><li><code>Imputer(missing_values=&#39;NaN&#39;,strategy=&#39;mean&#39;,axis=0)</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">im</span>():</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   缺失值处理</span></span><br><span class="line"><span class="string">   :return: None</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   im = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=<span class="number">0</span>)</span><br><span class="line">   data = im.fit_transform([[<span class="number">1</span>,<span class="number">2</span>],[np.nan,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">   print(data)</span><br><span class="line">   <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;特征预处理&quot;&gt;&lt;a href=&quot;#特征预处理&quot; class=&quot;headerlink&quot; title=&quot;特征预处理&quot;&gt;&lt;/a&gt;特征预处理&lt;/h3&gt;&lt;p&gt;&lt;font color =red&gt;通过特定的统计方法(数学方法)将数据转化成算法要求的数据&lt;/font&gt;&lt;/p&gt;
&lt;h4 id=&quot;数值型数据&quot;&gt;&lt;a href=&quot;#数值型数据&quot; class=&quot;headerlink&quot; title=&quot;数值型数据&quot;&gt;&lt;/a&gt;数值型数据&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;标准缩放：&lt;ul&gt;
&lt;li&gt;1.归一化&lt;/li&gt;
&lt;li&gt;2.标准化&lt;/li&gt;
&lt;li&gt;3.缺失值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;类别型数据&quot;&gt;&lt;a href=&quot;#类别型数据&quot; class=&quot;headerlink&quot; title=&quot;类别型数据&quot;&gt;&lt;/a&gt;类别型数据&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;one-hot 编码&lt;/li&gt;
&lt;li&gt;关于one-hot编码：&lt;a href=&quot;https://blog.csdn.net/weixin_43170863/article/details/100184168&quot;&gt;https://blog.csdn.net/weixin_43170863/article/details/100184168&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;时间类型&quot;&gt;&lt;a href=&quot;#时间类型&quot; class=&quot;headerlink&quot; title=&quot;时间类型&quot;&gt;&lt;/a&gt;时间类型&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;时间的切分&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;sklearn特征处理API&quot;&gt;&lt;a href=&quot;#sklearn特征处理API&quot; class=&quot;headerlink&quot; title=&quot;sklearn特征处理API&quot;&gt;&lt;/a&gt;sklearn特征处理API&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;sklearn.preprocessing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>转化器和估计器</title>
    <link href="http://luo6656.github.io/2019/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BD%AC%E5%8C%96%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8/"/>
    <id>http://luo6656.github.io/2019/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BD%AC%E5%8C%96%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8/</id>
    <published>2019-07-14T16:00:00.000Z</published>
    <updated>2020-10-27T06:48:01.605Z</updated>
    
    <content type="html"><![CDATA[<h3 id="转化器和估计器"><a href="#转化器和估计器" class="headerlink" title="转化器和估计器"></a>转化器和估计器</h3><h4 id="转化器"><a href="#转化器" class="headerlink" title="转化器"></a>转化器</h4><ul><li>fit():输入数据但不做事情，就是计算平均值，方差等等</li><li>transform(): 通过fit产生的平均值和方差转换数据</li><li>fit_transform() = fit() + transform()</li></ul><h4 id="估计器"><a href="#估计器" class="headerlink" title="估计器"></a>估计器</h4><a id="more"></a><ul><li><p>在sklearn中，估计器(estimator)是一个重要角色，<font color=red>是一类实现了算法的API</font></p></li><li><p>用于分类的估计器：</p><ul><li>sklearn.neighbors  k-近邻算法</li><li>sklearn.naive_bayes  贝叶斯</li><li>sklearn.linear_model.LogisticRegression  逻辑回归</li><li>sklearn.tree   决策树与随机森林</li></ul></li><li><p>用于回归的估计器：</p><ul><li>sklearn.linear_model.LinearRegression  线性回归</li><li>sklearn.linear_model.Ridge   岭回归</li></ul></li><li><p>用于聚类的估计器：</p><ul><li>k-means</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;转化器和估计器&quot;&gt;&lt;a href=&quot;#转化器和估计器&quot; class=&quot;headerlink&quot; title=&quot;转化器和估计器&quot;&gt;&lt;/a&gt;转化器和估计器&lt;/h3&gt;&lt;h4 id=&quot;转化器&quot;&gt;&lt;a href=&quot;#转化器&quot; class=&quot;headerlink&quot; title=&quot;转化器&quot;&gt;&lt;/a&gt;转化器&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;fit():输入数据但不做事情，就是计算平均值，方差等等&lt;/li&gt;
&lt;li&gt;transform(): 通过fit产生的平均值和方差转换数据&lt;/li&gt;
&lt;li&gt;fit_transform() = fit() + transform()&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;估计器&quot;&gt;&lt;a href=&quot;#估计器&quot; class=&quot;headerlink&quot; title=&quot;估计器&quot;&gt;&lt;/a&gt;估计器&lt;/h4&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习之k-近邻算法</title>
    <link href="http://luo6656.github.io/2019/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8Bk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://luo6656.github.io/2019/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8Bk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</id>
    <published>2019-07-07T16:00:00.000Z</published>
    <updated>2020-10-27T06:41:47.814Z</updated>
    
    <content type="html"><![CDATA[<h3 id="k-近邻算法-KNN"><a href="#k-近邻算法-KNN" class="headerlink" title="k-近邻算法(KNN)"></a>k-近邻算法(KNN)</h3><ul><li>定义：如果一个样本在特征空间中的<font color=red>k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别</font>，则该样本也属于这个类别。</li><li>来源：KNN算法最早由Cover和Hart提出的一种分类算法。</li></ul><a id="more"></a><h4 id="计算距离公式"><a href="#计算距离公式" class="headerlink" title="计算距离公式"></a>计算距离公式</h4><ul><li>两个样本的距离可以通过如下公式计算，又称为<font color=red>欧式距离</font>，比如说，a(a1,a2,a3),b(b1,b2,b3)</li><li><img src="https://img-blog.csdnimg.cn/20190901134615992.png" alt="欧式距离"></li><li>k-近邻算法需要标准化</li></ul><h4 id="sklearn-k-近邻算法API"><a href="#sklearn-k-近邻算法API" class="headerlink" title="sklearn k-近邻算法API"></a>sklearn k-近邻算法API</h4><ul><li><code>sklearn.neighbors.KNeightborsClassifier(n_neighbors=5,algorithm=&#39;auto&#39;)</code></li><li><code>n_neighbors</code>:int类型，可选(默认=5)，n_neighbors查询默认使用的邻居数</li><li><code>algorithm</code>:{‘auto’,’ball_tree’,’kd_tree’,’brute’}，可选用于计算最近邻居的算法：’ball_tree’将会使用BallTree，’kd_tree’将会使用KDTree。’auto’将会尝试根据传递给fit方法的值来决定最合适的算法。(不同的实现方式会影响效率)</li></ul><h4 id="k的取值问题"><a href="#k的取值问题" class="headerlink" title="k的取值问题"></a>k的取值问题</h4><ul><li>k值取很小<ul><li>易受异常点影响</li></ul></li><li>k值取很大<ul><li>易受k值数量(类别)波动</li></ul></li><li>性能问题<ul><li>时间复杂度高</li><li>样本会越来越大</li></ul></li><li>参数调优</li></ul><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><ul><li>优点<ul><li>简单，易于理解，易于实现，<font color=red>无需估计参数，无需训练</font></li></ul></li><li>缺点<ul><li>懒惰算法，对测试样本分类时的计算量大，内存开销大</li><li>必须指定K值，K值选择不当则分类精度不能保证</li></ul></li></ul><h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><ul><li>小数据场景，几千至几万样本，具体场景具体业务去测试</li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;k-近邻算法-KNN&quot;&gt;&lt;a href=&quot;#k-近邻算法-KNN&quot; class=&quot;headerlink&quot; title=&quot;k-近邻算法(KNN)&quot;&gt;&lt;/a&gt;k-近邻算法(KNN)&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;定义：如果一个样本在特征空间中的&lt;font color=red&gt;k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别&lt;/font&gt;，则该样本也属于这个类别。&lt;/li&gt;
&lt;li&gt;来源：KNN算法最早由Cover和Hart提出的一种分类算法。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://luo6656.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
</feed>
