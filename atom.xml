<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Eric个人博客</title>
  
  
  <link href="http://luo6656.github.io/atom.xml" rel="self"/>
  
  <link href="http://luo6656.github.io/"/>
  <updated>2020-11-10T07:29:07.808Z</updated>
  <id>http://luo6656.github.io/</id>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据仓库-建模</title>
    <link href="http://luo6656.github.io/2020/09/26/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1/"/>
    <id>http://luo6656.github.io/2020/09/26/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1/</id>
    <published>2020-09-25T16:00:00.000Z</published>
    <updated>2020-11-10T07:29:07.808Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据仓库-建模方法"><a href="#数据仓库-建模方法" class="headerlink" title="数据仓库-建模方法"></a>数据仓库-建模方法</h1><h3 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h3><h4 id="OLTP系统建模方法"><a href="#OLTP系统建模方法" class="headerlink" title="OLTP系统建模方法"></a>OLTP系统建模方法</h4><ul><li>OLTP(在线事务处理)系统中，主要操作是随机读写</li><li>为了保证数据的一致性、减少冗余，常使用关系模型</li><li>在关系模型中，常使用3NF来减少冗余</li></ul><h4 id="OLAP系统"><a href="#OLAP系统" class="headerlink" title="OLAP系统"></a>OLAP系统</h4><h5 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h5><ol><li>OLAP系统，主要操作是<font color="red">复杂分析查询</font>；关注数据聚合，以及分析、处理性能</li><li>OLAP根据数据存储方法不同，又分为ROLAP、MOLAP、HOLAP<a id="more"></a></li></ol><h5 id="2-分类"><a href="#2-分类" class="headerlink" title="2. 分类"></a>2. 分类</h5><ul><li><font color="red">ROLAP(Relation)</font> : 使用关系模型构建，存储系统一般为RDBMS</li><li><font color="red">MOLAP(Multidimensional)</font> : 预先聚合计算，使用多维数组的形式保存数据结果，加快查询分析时间</li><li><font color="red">HOLAP(Hybrid)</font> : ROLAP和MOLAP两者的集成；如底层是关系型的，高层是多维矩阵型的；查询效率高于ROLAP，低于MOLAP</li><li>ROLAP依赖于模型设计，MOLAP和HOLAP依赖于产品的选型</li></ul><h3 id="二、ROLAP"><a href="#二、ROLAP" class="headerlink" title="二、ROLAP"></a>二、ROLAP</h3><h4 id="建模方法"><a href="#建模方法" class="headerlink" title="建模方法"></a>建模方法</h4><p>维度模型适合数据仓库需要经常发生变化的情况。</p><h5 id="1-ER模型"><a href="#1-ER模型" class="headerlink" title="1. ER模型"></a>1. ER模型</h5><ul><li>出发点是整合数据，为数据分析决策服务</li><li>需要全面了解业务和数据</li><li>实施周期昌</li><li>对建模人员能力要求不高</li></ul><h5 id="2-维度模型"><a href="#2-维度模型" class="headerlink" title="2. 维度模型"></a>2. 维度模型</h5><ul><li>为分析需求服务，更快完成需求分析</li><li>具有较好大规模复杂查询相应性能</li><li><font color="red">最流行的数仓建模方法</font></li></ul><h5 id="3-Data-Value"><a href="#3-Data-Value" class="headerlink" title="3.Data Value"></a>3.Data Value</h5><ul><li>ER模型的衍生</li><li>强调数据的历史性、可追溯、原子性</li><li>弱化一致性处理和整合</li><li>引入范式，应对原系统的扩展性</li></ul><h5 id="4-Anchor"><a href="#4-Anchor" class="headerlink" title="4.Anchor"></a>4.Anchor</h5><ul><li>Data Value模型的衍生</li><li>初衷为设计一个高度可扩展模型</li><li>会带来较多的join操作</li></ul><h4 id="维度模型"><a href="#维度模型" class="headerlink" title="维度模型"></a>维度模型</h4><p>维度模型中，表被分为维度表、事实表，维度是对事实的一种组织</p><p>维度一般包含分类、时间、地域</p><p>维度模型分为星型模型、雪花模型、星座模型</p><p>维度模型建立后，方便对数据进行多维分析</p><h5 id="1-星型模型"><a href="#1-星型模型" class="headerlink" title="1. 星型模型"></a>1. 星型模型</h5><p><img src="https://i.loli.net/2020/11/10/CmSca6DftOEWrp7.png" alt="image-20201110141723052"></p><p>只有一层维度，只有一个事实表带着一层维度表，分析性能最优</p><h5 id="2-雪花模型"><a href="#2-雪花模型" class="headerlink" title="2. 雪花模型"></a>2. 雪花模型</h5><p><img src="https://i.loli.net/2020/11/10/OgScAaMQBYoZ5EL.png" alt="image-20201110141858151"></p><p>多层维度，比较接近三范式，较为灵活</p><h5 id="3-星座模型"><a href="#3-星座模型" class="headerlink" title="3. 星座模型"></a>3. 星座模型</h5><p><img src="https://i.loli.net/2020/11/10/bY6VnL8C57gIkry.png" alt="image-20201110142002597"></p><p>星座模型基于多个事实表，事实表之间会共享一些维度表</p><p>随着业务的增长</p><h4 id="宽表模型"><a href="#宽表模型" class="headerlink" title="宽表模型"></a>宽表模型</h4><ul><li>宽表模型是维度模型的衍生，适合join性能不佳的数据仓库产品</li><li>宽表模型将维度冗余到实施表中，形成宽表，以此减少join操作</li></ul><h3 id="三、MOLAP"><a href="#三、MOLAP" class="headerlink" title="三、MOLAP"></a>三、MOLAP</h3><p>实际就是一种以空间换时间的方式</p><ul><li>MOLAP将数据进行预计算，并将聚合结果存储到CUBE模型中</li><li>CUBE模型以多维数据的形式，物化到存储系统中，加快后续的查询</li><li>生成CUBE需要大量的时间，空间，维度预处理可能会导致数据膨胀</li></ul><p><img src="https://i.loli.net/2020/11/10/xdc9KY8DaTpLHtG.png" alt="image-20201110145727029"></p><h3 id="四、多维分析"><a href="#四、多维分析" class="headerlink" title="四、多维分析"></a>四、多维分析</h3><p>OLAP主要操作是复杂查询，可以多表关联，使用count、sum、avg等聚合函数</p><p>OLAP对复杂查询操作做了值观的定义，包括钻取、切片、切块、旋转</p><p><img src="https://i.loli.net/2020/11/10/aZwBzeQsJnTX26W.png" alt="image-20201110150752383"></p><h3 id="五、表"><a href="#五、表" class="headerlink" title="五、表"></a>五、表</h3><h4 id="表类型"><a href="#表类型" class="headerlink" title="表类型"></a>表类型</h4><h5 id="1-维度表"><a href="#1-维度表" class="headerlink" title="1.维度表"></a>1.维度表</h5><h5 id="2-事实表"><a href="#2-事实表" class="headerlink" title="2.事实表"></a>2.事实表</h5><p><strong>事务事实表</strong></p><p>随着业务不断产生的数据，一旦产生不会再变化，如交易流水、操作日志、出库入库记录</p><p><img src="https://i.loli.net/2020/11/10/2C3JqOHdL8sEoDW.png" alt="image-20201110150959052"></p><p><strong>周期快照事实表</strong></p><ul><li>随着业务周期型的推进而变化，完成间隔周期内的度量统计，如年、季度累计</li><li>使用周期+状态度量的组合，如年累计订单数，年是周期，订单总数是量度</li></ul><p><img src="https://i.loli.net/2020/11/10/RTWUJmS2l7oLe89.png" alt="image-20201110151039241"></p><p><strong>累计快照事实表</strong></p><ul><li>记录不确定周期的度量统计，完全覆盖一个事实的生命周期，如订单状态表</li><li>通常有多个时间字段，用于记录生命周期中的关键时间点</li><li>只有一条记录，针对此记录不断更新</li></ul><p><img src="https://i.loli.net/2020/11/10/MxcQ1kjmfihopeD.png" alt="image-20201110151148701"></p><p><strong>拉链表</strong></p><ul><li>拉链表记录每条信息的生命周期，用于保留数据的所有历史（变更）状态</li><li>拉链表将表数据的随机修改方式，变为顺序追加</li></ul><p><img src="https://i.loli.net/2020/11/10/p2w1AMUq9yJl4Rk.png" alt="image-20201110151553782"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;数据仓库-建模方法&quot;&gt;&lt;a href=&quot;#数据仓库-建模方法&quot; class=&quot;headerlink&quot; title=&quot;数据仓库-建模方法&quot;&gt;&lt;/a&gt;数据仓库-建模方法&lt;/h1&gt;&lt;h3 id=&quot;一、基本概念&quot;&gt;&lt;a href=&quot;#一、基本概念&quot; class=&quot;headerlink&quot; title=&quot;一、基本概念&quot;&gt;&lt;/a&gt;一、基本概念&lt;/h3&gt;&lt;h4 id=&quot;OLTP系统建模方法&quot;&gt;&lt;a href=&quot;#OLTP系统建模方法&quot; class=&quot;headerlink&quot; title=&quot;OLTP系统建模方法&quot;&gt;&lt;/a&gt;OLTP系统建模方法&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;OLTP(在线事务处理)系统中，主要操作是随机读写&lt;/li&gt;
&lt;li&gt;为了保证数据的一致性、减少冗余，常使用关系模型&lt;/li&gt;
&lt;li&gt;在关系模型中，常使用3NF来减少冗余&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;OLAP系统&quot;&gt;&lt;a href=&quot;#OLAP系统&quot; class=&quot;headerlink&quot; title=&quot;OLAP系统&quot;&gt;&lt;/a&gt;OLAP系统&lt;/h4&gt;&lt;h5 id=&quot;1-基本概念&quot;&gt;&lt;a href=&quot;#1-基本概念&quot; class=&quot;headerlink&quot; title=&quot;1.基本概念&quot;&gt;&lt;/a&gt;1.基本概念&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;OLAP系统，主要操作是&lt;font color=&quot;red&quot;&gt;复杂分析查询&lt;/font&gt;；关注数据聚合，以及分析、处理性能&lt;/li&gt;
&lt;li&gt;OLAP根据数据存储方法不同，又分为ROLAP、MOLAP、HOLAP</summary>
    
    
    
    <category term="数据仓库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>数据仓库-架构</title>
    <link href="http://luo6656.github.io/2020/09/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/"/>
    <id>http://luo6656.github.io/2020/09/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/</id>
    <published>2020-09-19T16:00:00.000Z</published>
    <updated>2020-11-10T07:29:23.703Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据仓库架构"><a href="#数据仓库架构" class="headerlink" title="数据仓库架构"></a>数据仓库架构</h1><h3 id="一、架构图"><a href="#一、架构图" class="headerlink" title="一、架构图"></a>一、架构图</h3><p><img src="https://i.loli.net/2020/11/10/5e1aKBLCuw6MXFN.png" alt="image-20201110101550468"></p><h3 id="二、ETL"><a href="#二、ETL" class="headerlink" title="二、ETL"></a>二、ETL</h3><p>Extract-Transform-Load</p><ul><li>将数据从来源端经过抽取（extract）、交互转换（transform）、加载（load）至目的端的过程 </li><li>构建数据仓库的重要一环，用户从数据源抽取出所需的数据，经过数据清洗，最终按照预先定义好的数据仓库模型，将数据加载到数据仓库中去</li><li>ETL 规则的设计和实施约占整个数据仓库搭建工作量的 60%～80%<a id="more"></a></li></ul><p><img src="https://i.loli.net/2020/11/10/NQPySwBCxId2WcG.png" alt="image-20201110102549877"></p><h4 id="Extract"><a href="#Extract" class="headerlink" title="Extract"></a>Extract</h4><h5 id="1-数据源"><a href="#1-数据源" class="headerlink" title="1.数据源"></a>1.数据源</h5><ul><li>抽取的数据源可以分为结构化数据、非结构化数据、半结构化数据 </li><li>结构化数据一般采用JDBC、数据库日志方式，非/半结构化数据（日志/Json数据)会监听文件变动</li></ul><h5 id="2-抽取方式"><a href="#2-抽取方式" class="headerlink" title="2.抽取方式"></a>2.抽取方式</h5><ul><li>全量同步会将全部数据进行抽取，一般用于初始化数据装载 </li><li>增量同步方式会检测数据的变动，抽取发生变动的数据，一般用于数据更新</li></ul><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><h5 id="1-清洗"><a href="#1-清洗" class="headerlink" title="1.清洗"></a>1.清洗</h5><p>对出现的重复、二义性、不完整、违反业务或逻辑规则等问题的数据进行统一的处理</p><h5 id="2-转换"><a href="#2-转换" class="headerlink" title="2.转换"></a>2.转换</h5><p>对数据进行标准化处理，进行字段、数据类型、数据定义的转换</p><h4 id="Load"><a href="#Load" class="headerlink" title="Load"></a>Load</h4><p>将处理玩的数据导入到相应的目标源中</p><h4 id="ETL工具"><a href="#ETL工具" class="headerlink" title="ETL工具"></a>ETL工具</h4><h5 id="1-结构化数据工具"><a href="#1-结构化数据工具" class="headerlink" title="1. 结构化数据工具"></a>1. 结构化数据工具</h5><ul><li>Sqoop</li><li>Kettle</li><li>Datastage</li><li>Informatica</li><li>Kafka</li></ul><h5 id="2-非-半结构化数据"><a href="#2-非-半结构化数据" class="headerlink" title="2.非/半结构化数据"></a>2.非/半结构化数据</h5><ul><li>Flume</li><li>Logstash</li></ul><h3 id="三、数仓分层"><a href="#三、数仓分层" class="headerlink" title="三、数仓分层"></a>三、数仓分层</h3><h4 id="ODS层"><a href="#ODS层" class="headerlink" title="ODS层"></a>ODS层</h4><ul><li>数据与原业务数据保持一致，可以增加字段用来进行数据管理 </li><li>存储的历史数据是只读的，提供业务系统查询使用 </li><li>业务系统对历史数据完成修改后，将update_type字段更新为UPDATE，追加回ODS中</li></ul><p><img src="https://i.loli.net/2020/11/10/5th7JgGFyDCQbnP.png" alt="image-20201110111017790"></p><ul><li>在离线数仓中，业务数据定期通过ETL流程导入到ODS中，导入方式有全量、增量两种 <ul><li>全量导入：数据第一次导入时，选择此种方式 </li><li>增量导入：数据非第一次导入，每次只需要导入新增、更改的数据，建议使用外连接&amp;全覆盖方式</li></ul></li></ul><h4 id="DWD层"><a href="#DWD层" class="headerlink" title="DWD层"></a>DWD层</h4><ul><li>数据明细层对ODS层的数据进行清洗、标准化、<font color="red">维度退化</font>（时间、分类、地域） </li><li>数据仍然满足3NF模型，为分析运算做准备</li></ul><p><img src="https://i.loli.net/2020/11/10/qVJ4dhaL7NyxsWf.png" alt="image-20201110111447895"></p><h4 id="DWS层"><a href="#DWS层" class="headerlink" title="DWS层"></a>DWS层</h4><ul><li>数据汇总层的数据对数据明细层的数据，按照分析主题进行计算汇总，存放便于分析的宽表 </li><li>存储模型并非3NF，而是注重数据聚合，复杂查询、处理性能更优的数仓模型，如<font color="red">维度模型</font></li></ul><h4 id="ADS层"><a href="#ADS层" class="headerlink" title="ADS层"></a>ADS层</h4><ul><li>数据应用层也被称为数据集市 </li><li>存储数据分析结果，为不同业务场景提供接口，减轻数据仓库的负担 </li><li>数据仓库擅长数据分析，直接开放业务查询接口，会加重其负担，一般用ADS层存储结果，快速查询和交互</li></ul><p><img src="https://i.loli.net/2020/11/10/iYSmelMtwDR6Wah.png" alt="image-20201110111751436"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;数据仓库架构&quot;&gt;&lt;a href=&quot;#数据仓库架构&quot; class=&quot;headerlink&quot; title=&quot;数据仓库架构&quot;&gt;&lt;/a&gt;数据仓库架构&lt;/h1&gt;&lt;h3 id=&quot;一、架构图&quot;&gt;&lt;a href=&quot;#一、架构图&quot; class=&quot;headerlink&quot; title=&quot;一、架构图&quot;&gt;&lt;/a&gt;一、架构图&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/11/10/5e1aKBLCuw6MXFN.png&quot; alt=&quot;image-20201110101550468&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;二、ETL&quot;&gt;&lt;a href=&quot;#二、ETL&quot; class=&quot;headerlink&quot; title=&quot;二、ETL&quot;&gt;&lt;/a&gt;二、ETL&lt;/h3&gt;&lt;p&gt;Extract-Transform-Load&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将数据从来源端经过抽取（extract）、交互转换（transform）、加载（load）至目的端的过程 &lt;/li&gt;
&lt;li&gt;构建数据仓库的重要一环，用户从数据源抽取出所需的数据，经过数据清洗，最终按照预先定义好的数据仓库模型，将数据加载到数据仓库中去&lt;/li&gt;
&lt;li&gt;ETL 规则的设计和实施约占整个数据仓库搭建工作量的 60%～80%</summary>
    
    
    
    <category term="数据仓库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>数据仓库-概述</title>
    <link href="http://luo6656.github.io/2020/09/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    <id>http://luo6656.github.io/2020/09/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/</id>
    <published>2020-09-18T16:00:00.000Z</published>
    <updated>2020-11-10T07:29:23.755Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据仓库概述"><a href="#数据仓库概述" class="headerlink" title="数据仓库概述"></a>数据仓库概述</h1><h4 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h4><ul><li>数据仓库是一个面向主题的、集成的、非易失的且随时间变化的数据集合</li><li>主要用于组织积累的历史数据，并使用分析方法（OLAP、数据分析）进行分析整理，进而辅助决策，为管理者、企业系统提供数据支持，构建商业智能。</li></ul><h4 id="2-特点"><a href="#2-特点" class="headerlink" title="2. 特点"></a>2. 特点</h4><h5 id="1-面向主题"><a href="#1-面向主题" class="headerlink" title="1.面向主题"></a>1.面向主题</h5><p>为数据分析提供服务，根据主题将原始数据集合在一起</p><h5 id="2-集成"><a href="#2-集成" class="headerlink" title="2.集成"></a>2.集成</h5><a id="more"></a><p>原始数据来源于不同数据源，要整合成最终数据，需要经过抽取、清洗、转换的过程</p><h5 id="3-非易失"><a href="#3-非易失" class="headerlink" title="3.非易失"></a>3.非易失</h5><p>保存的数据是一系列历史快照，不允许修改，只允许通过工具进行查询、分析</p><h5 id="4-时变性"><a href="#4-时变性" class="headerlink" title="4.时变性"></a>4.时变性</h5><p>数仓会定期接收、集成新的数据，从而反映出数据的最新变化</p><h4 id="3-数仓-VS-数据库"><a href="#3-数仓-VS-数据库" class="headerlink" title="3. 数仓 VS 数据库"></a>3. 数仓 VS 数据库</h4><p>数据库面向事务设计，属于OLTP系统，主要操作是随机读写；在设计时尽量避免冗余，常采用符合范式规范来设计</p><p>数据仓库面向主题，属于OLAP系统，主要操作是批量读写；关注数据整合，以及分析、处理性能；会有意引入冗余，采用反范式方式设计</p><h4 id="4-MPP传统-VS-Hadoop大数据平台"><a href="#4-MPP传统-VS-Hadoop大数据平台" class="headerlink" title="4. MPP传统 VS Hadoop大数据平台"></a>4. MPP传统 VS Hadoop大数据平台</h4><h5 id="1-传统数据仓库"><a href="#1-传统数据仓库" class="headerlink" title="1. 传统数据仓库"></a>1. 传统数据仓库</h5><p>由关系型数据库组成MPP(大规模并行处理) 集群</p><p><img src="https://i.loli.net/2020/11/10/yCUJ3iVIrX9SPgQ.png" alt="image-20201110093802576"></p><ul><li>传统数仓中常见的技术架构，将单机数据库节点组成集群，提升整体处理处理性能</li><li>节点间为非共享架构，每个节点都有独立的磁盘存储系统和内存系统</li><li>每台数据节点通过专用网络或者商业通用网络互相连接，彼此协同计算，作为整体提供服务</li><li>设计上优先考虑C(一致性),其次A(可用性) , 最后P(分区容错性)</li></ul><h6 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h6><ul><li>运算方式精细，延迟低、吞吐低</li><li>适合中等规模的结构化数据处理</li></ul><h6 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h6><ul><li>存储位置不透明，通过Hash确定数据所在的物理节点，查询任务在所有节点均会执行</li><li>并行计算时，单节点瓶颈会成为整个系统的短板，容错性差</li><li>分布式事务的实现会导致扩展性降低</li></ul><p><font color="red"><strong>问题</strong></font></p><ul><li>扩展性有限</li><li>热点问题 ：</li></ul><h5 id="2-大数据数据仓库"><a href="#2-大数据数据仓库" class="headerlink" title="2. 大数据数据仓库"></a>2. 大数据数据仓库</h5><p><img src="https://i.loli.net/2020/11/10/QxZysDMXdvjEAwN.png" alt="image-20201110094413401"></p><ul><li>大数据中常见的技术架构，也称为Hadoop架构/批处理架构</li><li>各节点实现场地自治（可以单独运行局部应用），数据在集群中全局透明共享</li><li>每台节点通过局域网或广域网相连，节点间的通信开销较大，在运算要减少数据的移动</li><li>优先考虑P(分区容错性) , A(可用性) , C(一致性)</li></ul><p><font color="red"><strong>问题</strong></font></p><ul><li>SQL支持率</li><li>事务支持</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;数据仓库概述&quot;&gt;&lt;a href=&quot;#数据仓库概述&quot; class=&quot;headerlink&quot; title=&quot;数据仓库概述&quot;&gt;&lt;/a&gt;数据仓库概述&lt;/h1&gt;&lt;h4 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1. 概念&quot;&gt;&lt;/a&gt;1. 概念&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;数据仓库是一个面向主题的、集成的、非易失的且随时间变化的数据集合&lt;/li&gt;
&lt;li&gt;主要用于组织积累的历史数据，并使用分析方法（OLAP、数据分析）进行分析整理，进而辅助决策，为管理者、企业系统提供数据支持，构建商业智能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;2-特点&quot;&gt;&lt;a href=&quot;#2-特点&quot; class=&quot;headerlink&quot; title=&quot;2. 特点&quot;&gt;&lt;/a&gt;2. 特点&lt;/h4&gt;&lt;h5 id=&quot;1-面向主题&quot;&gt;&lt;a href=&quot;#1-面向主题&quot; class=&quot;headerlink&quot; title=&quot;1.面向主题&quot;&gt;&lt;/a&gt;1.面向主题&lt;/h5&gt;&lt;p&gt;为数据分析提供服务，根据主题将原始数据集合在一起&lt;/p&gt;
&lt;h5 id=&quot;2-集成&quot;&gt;&lt;a href=&quot;#2-集成&quot; class=&quot;headerlink&quot; title=&quot;2.集成&quot;&gt;&lt;/a&gt;2.集成&lt;/h5&gt;</summary>
    
    
    
    <category term="数据仓库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>数据仓库</title>
    <link href="http://luo6656.github.io/2020/09/18/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    <id>http://luo6656.github.io/2020/09/18/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/</id>
    <published>2020-09-17T16:00:00.000Z</published>
    <updated>2020-10-30T02:54:33.384Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h1><h3 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h3><p>数据仓库（Data Warehouse）简称DW或DWH，是数据库的一种概念上的升级，可以说是为满足新需求设计的一种新数据库，而这个数据库是需容纳更多的数据，更加庞大的数据集，从逻辑上讲数据仓库和数据库是没有什么区别的。为企业所有级别的决策制定过程，提供所有类型数据支撑的战略集合，主要是用于数据挖掘和数据分析，以建立数据沙盘为基础，为消灭消息孤岛和支持决策为目的而创建的。</p><h3 id="二、数据仓库分层"><a href="#二、数据仓库分层" class="headerlink" title="二、数据仓库分层"></a>二、数据仓库分层</h3><h5 id="1-ODS层（原始数据层）"><a href="#1-ODS层（原始数据层）" class="headerlink" title="1. ODS层（原始数据层）"></a>1. ODS层（原始数据层）</h5><p>存放原始数据，直接加载原始日志、数据，数据保持原貌不做处理</p><h5 id="2-DWD层（明细数据层）"><a href="#2-DWD层（明细数据层）" class="headerlink" title="2. DWD层（明细数据层）"></a>2. DWD层（明细数据层）</h5><p>结构和粒度与原始表保持一致，对ODS层数据进行清洗（去除空值，脏数据，超过极限范围的数据)</p><h5 id="3-DWS层（服务数据层）"><a href="#3-DWS层（服务数据层）" class="headerlink" title="3. DWS层（服务数据层）"></a>3. DWS层（服务数据层）</h5><p>进行轻度汇总</p><h5 id="4-ADS层（数据应用层）"><a href="#4-ADS层（数据应用层）" class="headerlink" title="4. ADS层（数据应用层）"></a>4. ADS层（数据应用层）</h5><a id="more"></a><p>为各个统计报表提供数据</p><h4 id="为什么要进行分层"><a href="#为什么要进行分层" class="headerlink" title="为什么要进行分层"></a>为什么要进行分层</h4><p>(1) 把复杂的问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一步骤，比较简单、并且方便定位问题</p><p>(2) 减少重复开发：规范数据分层，通过的中间层数据，能够减少极大的重复计算，增加一次计算结果的复用性。</p><p>(3) 隔离原始数据：不论是数据的异常还是数据的敏感性，都使真实数据与统计数据解耦开。</p><h3 id="二、数据仓库与数据库的区别"><a href="#二、数据仓库与数据库的区别" class="headerlink" title="二、数据仓库与数据库的区别"></a>二、数据仓库与数据库的区别</h3><h5 id="1-数据来源"><a href="#1-数据来源" class="headerlink" title="1. 数据来源"></a>1. 数据来源</h5><p>数据库的数据来源于业务，数据仓库的数据来自于多个数据源</p><h5 id="2-时间维度"><a href="#2-时间维度" class="headerlink" title="2. 时间维度"></a>2. 时间维度</h5><p>数据库保留的是当前的数据，数据仓库保留的数据是时间轴上的全部信息</p><h5 id="3-组织方式"><a href="#3-组织方式" class="headerlink" title="3. 组织方式"></a>3. 组织方式</h5><p>数据库面向<font color="red">业务</font>组织，数据仓库面向<font color="red">主题</font></p><h5 id="4-建模方式"><a href="#4-建模方式" class="headerlink" title="4. 建模方式"></a>4. 建模方式</h5><p>数据库服从范式建模，数据仓库一般是合理冗余的</p><h5 id="5-设计目的"><a href="#5-设计目的" class="headerlink" title="5. 设计目的"></a>5. 设计目的</h5><p>数据库以存储、管理为主，数据仓库以组织、计算为主</p><h3 id="三、数据仓库的存储方式"><a href="#三、数据仓库的存储方式" class="headerlink" title="三、数据仓库的存储方式"></a>三、数据仓库的存储方式</h3><h5 id="1-全量存储"><a href="#1-全量存储" class="headerlink" title="1. 全量存储"></a>1. 全量存储</h5><p>没有分区，会直接覆盖</p><h5 id="2-增量数据"><a href="#2-增量数据" class="headerlink" title="2. 增量数据"></a>2. 增量数据</h5><p>每个分区只存储当日从数据库表中增量抽取出的数据</p><h5 id="3-快照存储"><a href="#3-快照存储" class="headerlink" title="3. 快照存储"></a>3. 快照存储</h5><p>每个分区相当于一个全量表</p><h5 id="4-拉链存储"><a href="#4-拉链存储" class="headerlink" title="4. 拉链存储"></a>4. 拉链存储</h5><p>存储了数据的变化情况</p><h3 id="四、数仓的模型"><a href="#四、数仓的模型" class="headerlink" title="四、数仓的模型"></a>四、数仓的模型</h3><h5 id="1-星型模型"><a href="#1-星型模型" class="headerlink" title="1. 星型模型"></a>1. 星型模型</h5><h5 id="2-雪花模型"><a href="#2-雪花模型" class="headerlink" title="2. 雪花模型"></a>2. 雪花模型</h5><h5 id="3-星座模型"><a href="#3-星座模型" class="headerlink" title="3. 星座模型"></a>3. 星座模型</h5><p>对比：雪花模型是将星型模型的维度进一步划分，使各个维度表满足规范化设计。而星座模型则是允许星形模式中出现多个事实表。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;数据仓库&quot;&gt;&lt;a href=&quot;#数据仓库&quot; class=&quot;headerlink&quot; title=&quot;数据仓库&quot;&gt;&lt;/a&gt;数据仓库&lt;/h1&gt;&lt;h3 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h3&gt;&lt;p&gt;数据仓库（Data Warehouse）简称DW或DWH，是数据库的一种概念上的升级，可以说是为满足新需求设计的一种新数据库，而这个数据库是需容纳更多的数据，更加庞大的数据集，从逻辑上讲数据仓库和数据库是没有什么区别的。为企业所有级别的决策制定过程，提供所有类型数据支撑的战略集合，主要是用于数据挖掘和数据分析，以建立数据沙盘为基础，为消灭消息孤岛和支持决策为目的而创建的。&lt;/p&gt;
&lt;h3 id=&quot;二、数据仓库分层&quot;&gt;&lt;a href=&quot;#二、数据仓库分层&quot; class=&quot;headerlink&quot; title=&quot;二、数据仓库分层&quot;&gt;&lt;/a&gt;二、数据仓库分层&lt;/h3&gt;&lt;h5 id=&quot;1-ODS层（原始数据层）&quot;&gt;&lt;a href=&quot;#1-ODS层（原始数据层）&quot; class=&quot;headerlink&quot; title=&quot;1. ODS层（原始数据层）&quot;&gt;&lt;/a&gt;1. ODS层（原始数据层）&lt;/h5&gt;&lt;p&gt;存放原始数据，直接加载原始日志、数据，数据保持原貌不做处理&lt;/p&gt;
&lt;h5 id=&quot;2-DWD层（明细数据层）&quot;&gt;&lt;a href=&quot;#2-DWD层（明细数据层）&quot; class=&quot;headerlink&quot; title=&quot;2. DWD层（明细数据层）&quot;&gt;&lt;/a&gt;2. DWD层（明细数据层）&lt;/h5&gt;&lt;p&gt;结构和粒度与原始表保持一致，对ODS层数据进行清洗（去除空值，脏数据，超过极限范围的数据)&lt;/p&gt;
&lt;h5 id=&quot;3-DWS层（服务数据层）&quot;&gt;&lt;a href=&quot;#3-DWS层（服务数据层）&quot; class=&quot;headerlink&quot; title=&quot;3. DWS层（服务数据层）&quot;&gt;&lt;/a&gt;3. DWS层（服务数据层）&lt;/h5&gt;&lt;p&gt;进行轻度汇总&lt;/p&gt;
&lt;h5 id=&quot;4-ADS层（数据应用层）&quot;&gt;&lt;a href=&quot;#4-ADS层（数据应用层）&quot; class=&quot;headerlink&quot; title=&quot;4. ADS层（数据应用层）&quot;&gt;&lt;/a&gt;4. ADS层（数据应用层）&lt;/h5&gt;</summary>
    
    
    
    <category term="数据仓库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>YARN</title>
    <link href="http://luo6656.github.io/2020/09/10/BigDataFrame/YARN/"/>
    <id>http://luo6656.github.io/2020/09/10/BigDataFrame/YARN/</id>
    <published>2020-09-09T16:00:00.000Z</published>
    <updated>2020-11-11T07:59:51.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h1><p><img src="https://i.loli.net/2020/11/11/7CFfxilY9cM4Bpy.png" alt="image-20201111114651553"></p><h3 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h3><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><ol><li>处理客户端请求</li><li>监控NodeManager</li><li>启动或监控ApplicationMaster</li><li>资源的分配和调度</li></ol><p>负责处理客户端请求，对各NodeManager上的资源进行统一管理和调度。给ApplicationMaster分配空闲的Container运行并监控其运行状态。主要由两个组件构成：调度器和应用程序管理器</p><h4 id="1-调度器-Scheduler"><a href="#1-调度器-Scheduler" class="headerlink" title="1. 调度器(Scheduler)"></a>1. 调度器(Scheduler)</h4><p>调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。<font color="red">调度器仅根据各个应用程序的资源需求进行资源分配</font>，而资源分配的单位是Container。Scheduler不负责监控或者跟踪应用程序的状态。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。</p><a id="more"></a><h4 id="2-应用程序管理器-Applications-Manager"><a href="#2-应用程序管理器-Applications-Manager" class="headerlink" title="2. 应用程序管理器(Applications Manager)"></a>2. 应用程序管理器(Applications Manager)</h4><p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给Container的进度、状态也是其职责</p><h3 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h3><h5 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h5><ol><li>管理单个节点上的资源</li><li>处理来自ResouceManager的命令</li><li>处理来自ApplicationMaster的命令</li></ol><p>是每个节点上的资源和任务管理器。它会定时地向ResourceManger汇报本节点上地资源使用情况和各个Container运行状态；同时接收并处理来自ApplicationMaster地Container启动/停止请求。</p><h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><ol><li>负责数据的切分</li><li>为应用程序申请资源并分配给内部的任务</li><li>任务的监控和容错</li></ol><p>用户提交地应用程序均包含一个ApplicationMaster,负责应用地监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResouceManager协调资源，并且与NodeManager协同工作完成Task地执行和监控。</p><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><p>Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如<font color="red">内存、CPU、磁盘、网络等</font>。当ApplicationMaster向RM盛情资源时，返回的资源就以Container表示。</p><h3 id="Yarn工作流程"><a href="#Yarn工作流程" class="headerlink" title="Yarn工作流程"></a>Yarn工作流程</h3><p><img src="https://i.loli.net/2020/11/11/Ntfe7ZU3KPr8Lnw.png" alt="image-20201111151508429"></p><h3 id="Yarn的调度器"><a href="#Yarn的调度器" class="headerlink" title="Yarn的调度器"></a>Yarn的调度器</h3><h4 id="FIFO调度器（很少使用）"><a href="#FIFO调度器（很少使用）" class="headerlink" title="FIFO调度器（很少使用）"></a>FIFO调度器（很少使用）</h4><p>job1先来先让job1工作完再给job2工作，这样job2就会等待很长时间</p><p><img src="https://i.loli.net/2020/11/11/4jYeUJO1NvycEon.png" alt="image-20201111155328581"></p><h4 id="容量调度器（apache版本默认使用）"><a href="#容量调度器（apache版本默认使用）" class="headerlink" title="容量调度器（apache版本默认使用）"></a>容量调度器（apache版本默认使用）</h4><p>而对于Capacity Scheduler，提前预留一个专门的通道给小任务执行，但是这样会预先占用一些资源，大任务只能利用80%的资源，就导致大任务的运行时间要大于FIFO的大任务的运行时间；如上图，而且小任务前后会造成资源浪费</p><p><img src="https://i.loli.net/2020/11/11/AOGI7F63yVYicl1.png" alt="image-20201111155443695"></p><h4 id="公平调度器（CDH版默认使用）"><a href="#公平调度器（CDH版默认使用）" class="headerlink" title="公平调度器（CDH版默认使用）"></a>公平调度器（CDH版默认使用）</h4><p><img src="https://i.loli.net/2020/11/11/5TIYjuoVtQgsHif.png" alt="image-20201111155620159"></p><p>Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。在上面的“Yarn调度器对比图”展示了一个队列中两个应用的公平调度；当然，公平调度在也可以在多个队列间工作。举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。<br>a) 公平调度器，就是能够共享整个集群的资源<br>b) 不用预先占用资源，每一个作业都是共享的<br>c) 每当提交一个作业的时候，就会占用整个资源。如果再提交一个作业，那么第一个作业就会分给第二个作业一部分资源，第一个作业也就释放一部分资源。再提交其他的作业时，也同理。。。。也就是说每一个作业进来，都有机会获取资源</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;YARN&quot;&gt;&lt;a href=&quot;#YARN&quot; class=&quot;headerlink&quot; title=&quot;YARN&quot;&gt;&lt;/a&gt;YARN&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/11/11/7CFfxilY9cM4Bpy.png&quot; alt=&quot;image-20201111114651553&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;ResourceManager&quot;&gt;&lt;a href=&quot;#ResourceManager&quot; class=&quot;headerlink&quot; title=&quot;ResourceManager&quot;&gt;&lt;/a&gt;ResourceManager&lt;/h3&gt;&lt;h5 id=&quot;作用&quot;&gt;&lt;a href=&quot;#作用&quot; class=&quot;headerlink&quot; title=&quot;作用&quot;&gt;&lt;/a&gt;作用&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;处理客户端请求&lt;/li&gt;
&lt;li&gt;监控NodeManager&lt;/li&gt;
&lt;li&gt;启动或监控ApplicationMaster&lt;/li&gt;
&lt;li&gt;资源的分配和调度&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;负责处理客户端请求，对各NodeManager上的资源进行统一管理和调度。给ApplicationMaster分配空闲的Container运行并监控其运行状态。主要由两个组件构成：调度器和应用程序管理器&lt;/p&gt;
&lt;h4 id=&quot;1-调度器-Scheduler&quot;&gt;&lt;a href=&quot;#1-调度器-Scheduler&quot; class=&quot;headerlink&quot; title=&quot;1. 调度器(Scheduler)&quot;&gt;&lt;/a&gt;1. 调度器(Scheduler)&lt;/h4&gt;&lt;p&gt;调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。&lt;font color=&quot;red&quot;&gt;调度器仅根据各个应用程序的资源需求进行资源分配&lt;/font&gt;，而资源分配的单位是Container。Scheduler不负责监控或者跟踪应用程序的状态。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS联邦机制</title>
    <link href="http://luo6656.github.io/2020/09/06/BigDataFrame/HDFS%E8%81%94%E9%82%A6%E6%9C%BA%E5%88%B6/"/>
    <id>http://luo6656.github.io/2020/09/06/BigDataFrame/HDFS%E8%81%94%E9%82%A6%E6%9C%BA%E5%88%B6/</id>
    <published>2020-09-05T16:00:00.000Z</published>
    <updated>2020-11-11T03:21:41.784Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS联邦机制"><a href="#HDFS联邦机制" class="headerlink" title="HDFS联邦机制"></a>HDFS联邦机制</h1><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>单NameNode的架构使得HDFS在集群扩展和性能上都有潜在的问题，当集群大到一定程度后，NameNode进程使用的内存可能达到上百G。NameNode成为了性能的瓶颈。因此提出了NameNode水平扩展方案-Federation。</p><p>多个NameNode意味着有多个namespace,区别于HA模式下的NameNode,他们是拥有着同一个NameSpace.</p><a id="more"></a><p><img src="https://i.loli.net/2020/11/11/JbxGqryBHsQZRkd.png" alt="image-20201111111006969"></p><p><img src="https://i.loli.net/2020/11/11/OXdaxu6t3qPyW8J.png" alt="image-20201111111113372"></p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p><img src="https://i.loli.net/2020/11/11/dX2j4i6PEqR9tOZ.png" alt="image-20201111111540239"></p><p>不同应用可以使用不同NameNode进行数据管理</p><p>Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）</p><h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>联邦机制并没有完全解决单点故障问题。所以可以使用HA-Federation的部署方案实现横向扩展和高可用</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;HDFS联邦机制&quot;&gt;&lt;a href=&quot;#HDFS联邦机制&quot; class=&quot;headerlink&quot; title=&quot;HDFS联邦机制&quot;&gt;&lt;/a&gt;HDFS联邦机制&lt;/h1&gt;&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;单NameNode的架构使得HDFS在集群扩展和性能上都有潜在的问题，当集群大到一定程度后，NameNode进程使用的内存可能达到上百G。NameNode成为了性能的瓶颈。因此提出了NameNode水平扩展方案-Federation。&lt;/p&gt;
&lt;p&gt;多个NameNode意味着有多个namespace,区别于HA模式下的NameNode,他们是拥有着同一个NameSpace.&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS高可用机制</title>
    <link href="http://luo6656.github.io/2020/09/05/BigDataFrame/HDFS%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>http://luo6656.github.io/2020/09/05/BigDataFrame/HDFS%E9%AB%98%E5%8F%AF%E7%94%A8/</id>
    <published>2020-09-04T16:00:00.000Z</published>
    <updated>2020-11-11T03:21:21.382Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS高可用机制"><a href="#HDFS高可用机制" class="headerlink" title="HDFS高可用机制"></a>HDFS高可用机制</h1><h3 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h3><p>由于HDFS中最重要的就是NameNode,所以NameNode的可用性直接决定了Hadoop的可用性，一旦NameNode进程不能工作，就会影响集群的正常运行。</p><p>在典型的HA集群中，<font color="red">两台独立的机器</font>被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。</p><h3 id="二、工作要点"><a href="#二、工作要点" class="headerlink" title="二、工作要点"></a>二、工作要点</h3><h5 id="1-元数据管理方式需要改变"><a href="#1-元数据管理方式需要改变" class="headerlink" title="1.元数据管理方式需要改变"></a>1.元数据管理方式需要改变</h5><a id="more"></a><p>内存中各自保存一份元数据；</p><p>Edits日志只有Active状态的NameNode节点可以做写操作；</p><p>两个NameNode都可以读取Edits；</p><p>共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</p><h5 id="2-需要一个状态管理功能模块"><a href="#2-需要一个状态管理功能模块" class="headerlink" title="2.需要一个状态管理功能模块"></a>2.需要一个状态管理功能模块</h5><p>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</p><h5 id="3-必须保证两个NameNode之间能够ssh无密码登陆"><a href="#3-必须保证两个NameNode之间能够ssh无密码登陆" class="headerlink" title="3.必须保证两个NameNode之间能够ssh无密码登陆"></a>3.必须保证两个NameNode之间能够ssh无密码登陆</h5><h5 id="4-隔离，即同一时刻仅仅有一个NameNode对外提供服务"><a href="#4-隔离，即同一时刻仅仅有一个NameNode对外提供服务" class="headerlink" title="4.隔离，即同一时刻仅仅有一个NameNode对外提供服务"></a>4.隔离，即同一时刻仅仅有一个NameNode对外提供服务</h5><h3 id="三、结构组件"><a href="#三、结构组件" class="headerlink" title="三、结构组件"></a>三、结构组件</h3><p><img src="https://i.loli.net/2020/11/11/LHdw6cRahB4puNt.png" alt="image-20201111102728418"></p><h4 id="ZKFailoverController"><a href="#ZKFailoverController" class="headerlink" title="ZKFailoverController"></a>ZKFailoverController</h4><p>是基于Zookeeper的故障转移控制器，它负责控制NameNode的主备切换，ZKFailoverController会监测NameNode的健康状态，当发现Active NameNode出现异常时会通过Zookeeper进行一次新的选举，完成Avtive和Standby状态的切换</p><h4 id="HealthMonitor"><a href="#HealthMonitor" class="headerlink" title="HealthMonitor"></a>HealthMonitor</h4><p>周期性调用NameNode的HAServiceProtocol RPC接口监控NameNode的健康状态并向ZKFailoverController反馈</p><h4 id="AciveStandbyElector"><a href="#AciveStandbyElector" class="headerlink" title="AciveStandbyElector"></a>AciveStandbyElector</h4><p>接收ZKFC的选举请求，通过Zookeeper自动完成主备选举，选举玩抽回调ZKFailoverController的主备切换方法对NameNode进行Active和Standby状态的切换</p><h4 id="共享存储系统"><a href="#共享存储系统" class="headerlink" title="共享存储系统"></a>共享存储系统</h4><p>共享存储系统负责存储HDFS的元数据(EditsLog) , Active NameNode(写入) 和 Standby NameNode(读取) 通过共享存储系统实现元数据同步，在主备切换过程中，新的Active NameNode必须保证元数据同步完成才能对外提供服务</p><h3 id="三、HDFS-HA故障转移工作机制"><a href="#三、HDFS-HA故障转移工作机制" class="headerlink" title="三、HDFS-HA故障转移工作机制"></a>三、HDFS-HA故障转移工作机制</h3><p><img src="https://i.loli.net/2020/11/11/s4E5QWmJizlvVeZ.png" alt="image-20201111105612112"></p><p><img src="https://i.loli.net/2020/11/10/gJM4tr9knUXOdoZ.png" alt="image-20201110231836588"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;HDFS高可用机制&quot;&gt;&lt;a href=&quot;#HDFS高可用机制&quot; class=&quot;headerlink&quot; title=&quot;HDFS高可用机制&quot;&gt;&lt;/a&gt;HDFS高可用机制&lt;/h1&gt;&lt;h3 id=&quot;一、概述&quot;&gt;&lt;a href=&quot;#一、概述&quot; class=&quot;headerlink&quot; title=&quot;一、概述&quot;&gt;&lt;/a&gt;一、概述&lt;/h3&gt;&lt;p&gt;由于HDFS中最重要的就是NameNode,所以NameNode的可用性直接决定了Hadoop的可用性，一旦NameNode进程不能工作，就会影响集群的正常运行。&lt;/p&gt;
&lt;p&gt;在典型的HA集群中，&lt;font color=&quot;red&quot;&gt;两台独立的机器&lt;/font&gt;被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。&lt;/p&gt;
&lt;h3 id=&quot;二、工作要点&quot;&gt;&lt;a href=&quot;#二、工作要点&quot; class=&quot;headerlink&quot; title=&quot;二、工作要点&quot;&gt;&lt;/a&gt;二、工作要点&lt;/h3&gt;&lt;h5 id=&quot;1-元数据管理方式需要改变&quot;&gt;&lt;a href=&quot;#1-元数据管理方式需要改变&quot; class=&quot;headerlink&quot; title=&quot;1.元数据管理方式需要改变&quot;&gt;&lt;/a&gt;1.元数据管理方式需要改变&lt;/h5&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>NameNode|SecondaryNameNode|DataNode</title>
    <link href="http://luo6656.github.io/2020/09/03/BigDataFrame/NM%E5%92%8CSNM%E5%92%8CDN/"/>
    <id>http://luo6656.github.io/2020/09/03/BigDataFrame/NM%E5%92%8CSNM%E5%92%8CDN/</id>
    <published>2020-09-02T16:00:00.000Z</published>
    <updated>2020-11-10T14:44:50.535Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NameNode-SecondaryNamenode-DataNode"><a href="#NameNode-SecondaryNamenode-DataNode" class="headerlink" title="NameNode|SecondaryNamenode|DataNode"></a>NameNode|SecondaryNamenode|DataNode</h1><h3 id="如何确保NameNode高可用"><a href="#如何确保NameNode高可用" class="headerlink" title="如何确保NameNode高可用"></a>如何确保NameNode高可用</h3><p>由于NameNode需要经常进行随机访问，并响应请求，所以需要将元数据放置内存中。但如果断电，元数据就将丢失，整个集群就无法工作。因此<font color="red">需要在磁盘中备份元数据的<strong>FsImage</strong></font></p><p>但是，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。</p><p>因此，<font color="red">引入Edits文件(只进行追加操作，效率很高)</font>。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p><h3 id="为何需要SecondaryNameNode"><a href="#为何需要SecondaryNameNode" class="headerlink" title="为何需要SecondaryNameNode"></a>为何需要SecondaryNameNode</h3><a id="more"></a><p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点<font color="red">SecondaryNamenode</font>，专门用于<font color="red">FsImage和Edits的合并</font>。</p><h3 id="NameNode和SNameNode工作机制"><a href="#NameNode和SNameNode工作机制" class="headerlink" title="NameNode和SNameNode工作机制"></a>NameNode和SNameNode工作机制</h3><p><img src="https://i.loli.net/2020/11/10/gHjoIe92PS63Wph.png" alt="image-20201110202859240"></p><h5 id="NameNode启动"><a href="#NameNode启动" class="headerlink" title="NameNode启动"></a>NameNode启动</h5><p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p><p>（2）客户端对元数据进行增删改的请求。</p><p>（3）NameNode记录操作日志，更新滚动日志。</p><p>（4）NameNode在内存中对元数据进行增删改。</p><h5 id="Secondary-NameNode工作"><a href="#Secondary-NameNode工作" class="headerlink" title="Secondary NameNode工作"></a>Secondary NameNode工作</h5><p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</p><p>（2）Secondary NameNode请求执行CheckPoint。</p><p>（3）NameNode滚动正在写的Edits日志。</p><p>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p><p>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p><p>（6）生成新的镜像文件fsimage.chkpoint。</p><p>（7）拷贝fsimage.chkpoint到NameNode。</p><p>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p><h3 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h3><p><img src="https://i.loli.net/2020/11/10/tlxP3OJDEj2IeA6.png" alt="image-20201110224010166"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;NameNode-SecondaryNamenode-DataNode&quot;&gt;&lt;a href=&quot;#NameNode-SecondaryNamenode-DataNode&quot; class=&quot;headerlink&quot; title=&quot;NameNode|SecondaryNamenode|DataNode&quot;&gt;&lt;/a&gt;NameNode|SecondaryNamenode|DataNode&lt;/h1&gt;&lt;h3 id=&quot;如何确保NameNode高可用&quot;&gt;&lt;a href=&quot;#如何确保NameNode高可用&quot; class=&quot;headerlink&quot; title=&quot;如何确保NameNode高可用&quot;&gt;&lt;/a&gt;如何确保NameNode高可用&lt;/h3&gt;&lt;p&gt;由于NameNode需要经常进行随机访问，并响应请求，所以需要将元数据放置内存中。但如果断电，元数据就将丢失，整个集群就无法工作。因此&lt;font color=&quot;red&quot;&gt;需要在磁盘中备份元数据的&lt;strong&gt;FsImage&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;但是，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。&lt;/p&gt;
&lt;p&gt;因此，&lt;font color=&quot;red&quot;&gt;引入Edits文件(只进行追加操作，效率很高)&lt;/font&gt;。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。&lt;/p&gt;
&lt;h3 id=&quot;为何需要SecondaryNameNode&quot;&gt;&lt;a href=&quot;#为何需要SecondaryNameNode&quot; class=&quot;headerlink&quot; title=&quot;为何需要SecondaryNameNode&quot;&gt;&lt;/a&gt;为何需要SecondaryNameNode&lt;/h3&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS读写流程</title>
    <link href="http://luo6656.github.io/2020/09/01/BigDataFrame/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/"/>
    <id>http://luo6656.github.io/2020/09/01/BigDataFrame/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/</id>
    <published>2020-08-31T16:00:00.000Z</published>
    <updated>2020-11-10T12:04:14.198Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h1><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><p><img src="https://i.loli.net/2020/11/10/QPehBKkCawimc98.png" alt="image-20201110185557285"></p><p><strong>机架感知</strong></p><p>第一个副本存在Client所在的机架上</p><p>第二个副本存在与第一个副本相同的机架上，减少IO</p><p>第三个副本存放在不同的机架上，避免整个机架出现故障</p><a id="more"></a><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><p><img src="https://i.loli.net/2020/11/10/kEzApePyVHq34Rf.png" alt="image-20201110200015617"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;HDFS读写流程&quot;&gt;&lt;a href=&quot;#HDFS读写流程&quot; class=&quot;headerlink&quot; title=&quot;HDFS读写流程&quot;&gt;&lt;/a&gt;HDFS读写流程&lt;/h1&gt;&lt;h3 id=&quot;HDFS写数据流程&quot;&gt;&lt;a href=&quot;#HDFS写数据流程&quot; class=&quot;headerlink&quot; title=&quot;HDFS写数据流程&quot;&gt;&lt;/a&gt;HDFS写数据流程&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/11/10/QPehBKkCawimc98.png&quot; alt=&quot;image-20201110185557285&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;机架感知&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第一个副本存在Client所在的机架上&lt;/p&gt;
&lt;p&gt;第二个副本存在与第一个副本相同的机架上，减少IO&lt;/p&gt;
&lt;p&gt;第三个副本存放在不同的机架上，避免整个机架出现故障&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>SQL语句调优</title>
    <link href="http://luo6656.github.io/2020/07/28/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E8%B0%83%E4%BC%98/"/>
    <id>http://luo6656.github.io/2020/07/28/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E8%B0%83%E4%BC%98/</id>
    <published>2020-07-27T16:00:00.000Z</published>
    <updated>2020-10-30T02:56:37.076Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SQL语句调优"><a href="#SQL语句调优" class="headerlink" title="SQL语句调优"></a>SQL语句调优</h1><h3 id="一、Mysql的整体执行过程"><a href="#一、Mysql的整体执行过程" class="headerlink" title="一、Mysql的整体执行过程"></a>一、Mysql的整体执行过程</h3><p><img src="https://i.loli.net/2020/10/30/oGBKYNmygnJI4us.png" alt="image-20201030100953732"></p><h3 id="二、sql的执行顺序"><a href="#二、sql的执行顺序" class="headerlink" title="二、sql的执行顺序"></a>二、sql的执行顺序</h3><p><img src="https://i.loli.net/2020/10/30/Nvs4hSaPoC1ku9Y.png" alt="image-20201030100114897"></p><p>(1) from</p><p> 第一步就是选择出from关键词后面跟的表,这也是sql执行的第一步:表示要从数据库中执行哪张表。</p><p>(2) join on</p><p>join是表示要关联的表，on是连接的条件。通过from和join on选择出需要执行的数据库表T和S,产生笛卡尔积,生成T和S合并的临时中间表Temp1。on:确定表的绑定关系,通过on产生临时中间表Temp2.</p><p>(3) where</p><p>where表示筛选,根据where后面的条件进行过滤,按照指定的字段的值(如果有and连接符会进行联合筛选)从临时中间表Temp2中筛选需要的数据,注意如果在此阶段找不到数据，会直接返回客户端,不会往下进行.这个过程会生成一个临时中间表Temp3。注意在where中不可以使用聚合函数，聚合函数主要是(min\max\count\sum等函数)</p><a id="more"></a><p>(4) group by</p><p>group by是进行分组，对where条件过滤后的临时表Temp3按照固定的字段进行分组,产生临时中间表Temp4，这个过程只是数据的顺序发生改变,而数据总量不会变化,表中的数据以组的形式存在</p><p>(5) Having</p><p>对临时中间表Temp4进行聚合,这里可以为count等计数，然后产生中间表Temp5，在此阶段可以使用select中的别名</p><p>(6) select</p><p>对分组聚合完的表挑选出需要查询的数据,如果为会解析为所有数据,此时会产生中间表Temp6</p><p>(7) Distinct</p><p>distinct对所有的数据进行去重,此时如果有min、max函数会执行字段函数计算，然后产生临时表Temp7</p><p>(8) Order by</p><p>排序</p><p>(9) limit</p><p>需要多少条数据</p><h3 id="三、SQL调优"><a href="#三、SQL调优" class="headerlink" title="三、SQL调优"></a>三、SQL调优</h3><p>SQL调优的本质就是充分利用索引，而不让索引失效</p><p>(1) 尽量 使用全值匹配</p><p>(2) 最佳左前缀</p><p>(3) 避免在索引列上做计算</p><p>(4) 避免在索引列上做值类型转换</p><p>(5) 减少使用select *</p><p>(6) like后模糊匹配会造成索引失效</p><p>(7) 减少使用or，使用union all或union代替</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;SQL语句调优&quot;&gt;&lt;a href=&quot;#SQL语句调优&quot; class=&quot;headerlink&quot; title=&quot;SQL语句调优&quot;&gt;&lt;/a&gt;SQL语句调优&lt;/h1&gt;&lt;h3 id=&quot;一、Mysql的整体执行过程&quot;&gt;&lt;a href=&quot;#一、Mysql的整体执行过程&quot; class=&quot;headerlink&quot; title=&quot;一、Mysql的整体执行过程&quot;&gt;&lt;/a&gt;一、Mysql的整体执行过程&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/10/30/oGBKYNmygnJI4us.png&quot; alt=&quot;image-20201030100953732&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;二、sql的执行顺序&quot;&gt;&lt;a href=&quot;#二、sql的执行顺序&quot; class=&quot;headerlink&quot; title=&quot;二、sql的执行顺序&quot;&gt;&lt;/a&gt;二、sql的执行顺序&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/10/30/Nvs4hSaPoC1ku9Y.png&quot; alt=&quot;image-20201030100114897&quot;&gt;&lt;/p&gt;
&lt;p&gt;(1) from&lt;/p&gt;
&lt;p&gt; 第一步就是选择出from关键词后面跟的表,这也是sql执行的第一步:表示要从数据库中执行哪张表。&lt;/p&gt;
&lt;p&gt;(2) join on&lt;/p&gt;
&lt;p&gt;join是表示要关联的表，on是连接的条件。通过from和join on选择出需要执行的数据库表T和S,产生笛卡尔积,生成T和S合并的临时中间表Temp1。on:确定表的绑定关系,通过on产生临时中间表Temp2.&lt;/p&gt;
&lt;p&gt;(3) where&lt;/p&gt;
&lt;p&gt;where表示筛选,根据where后面的条件进行过滤,按照指定的字段的值(如果有and连接符会进行联合筛选)从临时中间表Temp2中筛选需要的数据,注意如果在此阶段找不到数据，会直接返回客户端,不会往下进行.这个过程会生成一个临时中间表Temp3。注意在where中不可以使用聚合函数，聚合函数主要是(min\max\count\sum等函数)&lt;/p&gt;</summary>
    
    
    
    <category term="数据库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Mysql底层原理</title>
    <link href="http://luo6656.github.io/2020/07/26/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql%E5%8E%9F%E7%90%86/"/>
    <id>http://luo6656.github.io/2020/07/26/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql%E5%8E%9F%E7%90%86/</id>
    <published>2020-07-25T16:00:00.000Z</published>
    <updated>2020-10-30T02:56:36.959Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mysql数据库底层原理"><a href="#Mysql数据库底层原理" class="headerlink" title="Mysql数据库底层原理"></a>Mysql数据库底层原理</h1><h3 id="一、索引"><a href="#一、索引" class="headerlink" title="一、索引"></a>一、索引</h3><h4 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h4><p>索引是一种数据结构，用于帮助我们在大量数据中快速定位到我们想要查找的数据。<br>索引最形象的比喻就是图书的目录了。注意这里的大量，数据量大了索引才显得有意义，如果我想要在 [1,2,3,4] 中找到 4 这个数据，直接对全数据检索也很快，没有必要费力气建索引再去查找。</p><h4 id="2-种类"><a href="#2-种类" class="headerlink" title="2. 种类"></a>2. 种类</h4><ol><li>B+树索引</li><li>Hash索引</li><li>全文索引</li></ol><h3 id="二、为何使用B-树结构"><a href="#二、为何使用B-树结构" class="headerlink" title="二、为何使用B+树结构"></a>二、为何使用B+树结构</h3><a id="more"></a><h4 id="1-二叉树"><a href="#1-二叉树" class="headerlink" title="1. 二叉树"></a>1. 二叉树</h4><p>如果采用二叉树，如果所有节点都在树的右子树上时，查询就相当于进行全表扫描了。</p><h4 id="2-AVL树"><a href="#2-AVL树" class="headerlink" title="2. AVL树"></a>2. AVL树</h4><p>由于树倾斜问题，所以采用平衡二叉树结构，</p><p>由于AVL树每个节点只能存储一个键值和数据，所以树的高度会很高，并且查询会进行多次磁盘IO，查找数据的效率会很低。</p><h4 id="3-B树"><a href="#3-B树" class="headerlink" title="3. B树"></a>3. B树</h4><p><img src="https://i.loli.net/2020/10/30/OTh5LfoPaBVR8t7.png" alt="image-20201030095213544"></p><p>B 树相对于平衡二叉树，每个节点存储了更多的键值（key）和数据（data），并且每个节点拥有更多的子节点，子节点的个数一般称为阶，上述图中的 B 树为 3 阶 B 树，高度也会很低。</p><p>查找流程</p><p>(1) 先找到根节点也就是页 1，判断 28 在键值 17 和 35 之间，那么我们根据页 1 中的指针 p2 找到页 3。</p><p>(2) 将 28 和页 3 中的键值相比较，28 在 26 和 30 之间，我们根据页 3 中的指针 p2 找到页 8。</p><p>(3) 将 28 和页 8 中的键值相比较，发现有匹配的键值 28，键值 28 对应的用户信息为（28，bv）。</p><h4 id="4-B-树"><a href="#4-B-树" class="headerlink" title="4. B+树"></a>4. B+树</h4><p><img src="https://i.loli.net/2020/10/30/MczOEHBkW76NFCJ.png" alt="image-20201030095326731"></p><p>根据上图我们来看下 B+ 树和 B 树有什么不同：</p><p>(1) B+ 树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储键值，也会存储数据。之所以这么做是因为在数据库中页的大小是固定的，InnoDB 中页的默认大小是 16KB。如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数又会再次减少，数据查询的效率也会更快。</p><p>(2) 因为 B+ 树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的。那么 B+ 树使得范围查找，排序查找，分组查找以及去重查找变得异常简单。而 B 树因为数据分散在各个节点，要实现这一点是很不容易的。</p><p> B+ 树中各个页之间是通过双向链表连接的，叶子节点中的数据是通过单向链表连接的。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Mysql数据库底层原理&quot;&gt;&lt;a href=&quot;#Mysql数据库底层原理&quot; class=&quot;headerlink&quot; title=&quot;Mysql数据库底层原理&quot;&gt;&lt;/a&gt;Mysql数据库底层原理&lt;/h1&gt;&lt;h3 id=&quot;一、索引&quot;&gt;&lt;a href=&quot;#一、索引&quot; class=&quot;headerlink&quot; title=&quot;一、索引&quot;&gt;&lt;/a&gt;一、索引&lt;/h3&gt;&lt;h4 id=&quot;1-定义&quot;&gt;&lt;a href=&quot;#1-定义&quot; class=&quot;headerlink&quot; title=&quot;1. 定义&quot;&gt;&lt;/a&gt;1. 定义&lt;/h4&gt;&lt;p&gt;索引是一种数据结构，用于帮助我们在大量数据中快速定位到我们想要查找的数据。&lt;br&gt;索引最形象的比喻就是图书的目录了。注意这里的大量，数据量大了索引才显得有意义，如果我想要在 [1,2,3,4] 中找到 4 这个数据，直接对全数据检索也很快，没有必要费力气建索引再去查找。&lt;/p&gt;
&lt;h4 id=&quot;2-种类&quot;&gt;&lt;a href=&quot;#2-种类&quot; class=&quot;headerlink&quot; title=&quot;2. 种类&quot;&gt;&lt;/a&gt;2. 种类&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;B+树索引&lt;/li&gt;
&lt;li&gt;Hash索引&lt;/li&gt;
&lt;li&gt;全文索引&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;二、为何使用B-树结构&quot;&gt;&lt;a href=&quot;#二、为何使用B-树结构&quot; class=&quot;headerlink&quot; title=&quot;二、为何使用B+树结构&quot;&gt;&lt;/a&gt;二、为何使用B+树结构&lt;/h3&gt;</summary>
    
    
    
    <category term="数据库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>MySql数据库的引擎</title>
    <link href="http://luo6656.github.io/2020/07/25/%E6%95%B0%E6%8D%AE%E5%BA%93/MySql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%BC%95%E6%93%8E/"/>
    <id>http://luo6656.github.io/2020/07/25/%E6%95%B0%E6%8D%AE%E5%BA%93/MySql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%BC%95%E6%93%8E/</id>
    <published>2020-07-24T16:00:00.000Z</published>
    <updated>2020-10-30T02:56:36.846Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MySql数据库的引擎"><a href="#MySql数据库的引擎" class="headerlink" title="MySql数据库的引擎"></a>MySql数据库的引擎</h1><h3 id="1-InnoDB"><a href="#1-InnoDB" class="headerlink" title="1. InnoDB"></a>1. InnoDB</h3><h5 id="1-数据库文件"><a href="#1-数据库文件" class="headerlink" title="1. 数据库文件"></a>1. 数据库文件</h5><p>(1) .frm 是表的定义文件，主要是表结构</p><p>(2) .idb是索引和数据，因为InnoDB引擎叶子节点上存储的是索引和数据。</p><h5 id="2-存储结构"><a href="#2-存储结构" class="headerlink" title="2. 存储结构"></a>2. 存储结构</h5><p>采用B+树存储数据</p><a id="more"></a><h5 id="3-注意点"><a href="#3-注意点" class="headerlink" title="3. 注意点"></a>3. 注意点</h5><p>(1) InnoDB 中存在表锁和行锁，不过行锁是在命中索引的情况下才会起作用。</p><p>(2) InnoDB 支持事务，且支持四种隔离级别（读未提交、读已提交、可重复读、串行化），默认的为可重复读；而在 Oracle 数据库中，只支持串行化级别和读已提交这两种级别，其中默认的为读已提交级别。</p><h5 id="4-InnoBD是聚集索引"><a href="#4-InnoBD是聚集索引" class="headerlink" title="4. InnoBD是聚集索引"></a>4. InnoBD是聚集索引</h5><p>(1) 聚集索引表示表中存储的数据按照索引的顺序存储，检索效率比非聚集索引高，但对数据更新影响较大。</p><p>(2) 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。<font color="red">如果主键不是自增id</font>，那么可以想象，它会干些什么，<font color="red">不断地调整数据的物理地址、分页</font>，当然也有其他一些措施来减少这些操作，但却无法彻底避免。但，如果是自增的，那就简单了，它只需要一页一页地写，索引结构相对紧凑，磁盘碎片少，效率也高。因此采用innoDB存储引擎的数据库，<font color="red">表一定要设置自增列作为主键，这样才能提高数据查询以及插入的效率</font>。</p><h3 id="2-MyISAM"><a href="#2-MyISAM" class="headerlink" title="2. MyISAM"></a>2. MyISAM</h3><h5 id="1-数据库文件-1"><a href="#1-数据库文件-1" class="headerlink" title="1. 数据库文件"></a>1. 数据库文件</h5><p>(1) .frm是表结构</p><p>(2) .MYD是Data数据</p><p>(3) .MYI是Index数据，从MYI中获取数据的磁盘指针到MYD中取数据。</p><h5 id="2-存储结构-1"><a href="#2-存储结构-1" class="headerlink" title="2. 存储结构"></a>2. 存储结构</h5><p>采用B+树存储数据</p><h5 id="3-注意点-1"><a href="#3-注意点-1" class="headerlink" title="3. 注意点"></a>3. 注意点</h5><p>(1) Myisam 只支持表锁，且不支持事务。Myisam 由于有单独的索引文件，在读取数据方面的性能很高 。</p><h5 id="4-MyISAM是非聚集索引"><a href="#4-MyISAM是非聚集索引" class="headerlink" title="4. MyISAM是非聚集索引"></a>4. MyISAM是非聚集索引</h5><p>(1) 非聚集索引表示数据存储在一个地方，索引存储在另一个地方，索引带有指针指向数据的存储位置，非聚集索引检索效率比聚集索引低，但对数据更新影响较小。</p><p>(2) MyISAM的主索引并非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。<br>(3) 如果涉及到大数据量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;MySql数据库的引擎&quot;&gt;&lt;a href=&quot;#MySql数据库的引擎&quot; class=&quot;headerlink&quot; title=&quot;MySql数据库的引擎&quot;&gt;&lt;/a&gt;MySql数据库的引擎&lt;/h1&gt;&lt;h3 id=&quot;1-InnoDB&quot;&gt;&lt;a href=&quot;#1-InnoDB&quot; class=&quot;headerlink&quot; title=&quot;1. InnoDB&quot;&gt;&lt;/a&gt;1. InnoDB&lt;/h3&gt;&lt;h5 id=&quot;1-数据库文件&quot;&gt;&lt;a href=&quot;#1-数据库文件&quot; class=&quot;headerlink&quot; title=&quot;1. 数据库文件&quot;&gt;&lt;/a&gt;1. 数据库文件&lt;/h5&gt;&lt;p&gt;(1) .frm 是表的定义文件，主要是表结构&lt;/p&gt;
&lt;p&gt;(2) .idb是索引和数据，因为InnoDB引擎叶子节点上存储的是索引和数据。&lt;/p&gt;
&lt;h5 id=&quot;2-存储结构&quot;&gt;&lt;a href=&quot;#2-存储结构&quot; class=&quot;headerlink&quot; title=&quot;2. 存储结构&quot;&gt;&lt;/a&gt;2. 存储结构&lt;/h5&gt;&lt;p&gt;采用B+树存储数据&lt;/p&gt;</summary>
    
    
    
    <category term="数据库" scheme="http://luo6656.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>HBase入门</title>
    <link href="http://luo6656.github.io/2020/07/15/BigDataFrame/HBase%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/07/15/BigDataFrame/HBase%E5%85%A5%E9%97%A8/</id>
    <published>2020-07-14T16:00:00.000Z</published>
    <updated>2020-10-27T05:49:46.459Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 HBase简介 / 第二章 HBase安装 / 第三章 HBase Shell操作 /<br>第四章 HBase数据结构 / 第五章 HBase原理 / 第六章 HBase API操作</p><a id="more"></a><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h3 id="第一章-HBase简介"><a href="#第一章-HBase简介" class="headerlink" title="第一章 HBase简介"></a>第一章 HBase简介</h3><h4 id="1-1-什么是HBase"><a href="#1-1-什么是HBase" class="headerlink" title="1.1 什么是HBase"></a>1.1 什么是HBase</h4><p>  HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p><h4 id="1-2-HBase特点"><a href="#1-2-HBase特点" class="headerlink" title="1.2 HBase特点"></a>1.2 HBase特点</h4><p>(1) <strong>海量存储</strong>：适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到几百毫秒内返回数据。这与Hbase的极易扩展性息息相关。正式因为Hbase良好的扩展性，才为海量数据的存储提供了便利。</p><p>(2) <strong>列式存储</strong>：这里的列式存储其实说的是<font color="red">列族（ColumnFamily）存储</font>，Hbase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。</p><p>(3) <strong>极易扩展</strong>：Hbase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。</p><p>通过横向添加RegionSever的机器，进行水平扩展，提升Hbase上层的处理能力，提升Hbsae服务更多Region的能力。</p><p>备注：RegionServer的作用是管理region、承接业务的访问，这个后面会详细的介绍通过横向添加Datanode的机器，进行存储层扩容，提升Hbase的数据存储能力和提升后端存储的读写能力。</p><p>(4) <strong>高并发（多核）</strong>：由于目前大部分使用Hbase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，Hbase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。</p><p>(5) <strong>稀疏</strong>：稀疏主要是针对Hbase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。</p><h4 id="1-3-HBase架构"><a href="#1-3-HBase架构" class="headerlink" title="1.3 HBase架构"></a>1.3 HBase架构</h4><p><img src="https://i.loli.net/2020/10/27/BtuKpVU9rScGnyi.png"></p><p>从图中可以看出Hbase是由Client、Zookeeper、Master、HRegionServer、HDFS等几个组件组成，下面来介绍一下几个组件的相关功能：</p><p><strong>(1) Client</strong></p><p>Client包含了访问Hbase的接口，另外Client还维护了对应的cache来加速Hbase的访问，比如cache的.META.元数据的信息。</p><p><strong>(2) Zookeeper</strong></p><p>HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。具体工作如下：</p><p>通过Zoopkeeper来保证集群中只有1个master在运行，如果master异常，会通过竞争机制产生新的master提供服务</p><p>通过Zoopkeeper来监控RegionServer的状态，当RegionSevrer有异常的时候，通过回调的形式通知Master RegionServer上下线的信息</p><p>通过Zoopkeeper存储元数据的统一入口地址</p><p><strong>(3) Hmaster(NameNode)</strong></p><p>master节点的主要职责如下：<br>为RegionServer分配Region<br>维护整个集群的负载均衡<br>维护集群的元数据信息<br>发现失效的Region，并将失效的Region分配到正常的RegionServer上<br>当RegionSever失效的时候，协调对应Hlog的拆分</p><p><strong>(4) HregionServer(DataNode)</strong></p><p><font color="red">HregionServer直接对接用户的读写请求</font>，是真正的“干活”的节点。它的功能概括如下：<br>管理master为其分配的Region<br>处理来自客户端的读写请求<br>负责和底层HDFS的交互，存储数据到HDFS<br>负责Region变大以后的拆分<br>负责Storefile的合并工作</p><p><strong>(5) HDFS</strong></p><p>HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用（Hlog存储在HDFS）的支持，具体功能概括如下：<br>提供元数据和表数据的底层分布式存储服务<br>数据多副本，保证的高可靠和高可用性</p><h4 id="1-3-HBase中的角色"><a href="#1-3-HBase中的角色" class="headerlink" title="1.3 HBase中的角色"></a>1.3 HBase中的角色</h4><h5 id="1-3-1-HMaster"><a href="#1-3-1-HMaster" class="headerlink" title="1.3.1 HMaster"></a>1.3.1 HMaster</h5><p><strong>功能</strong></p><p>1．监控RegionServer</p><p>2．处理RegionServer故障转移</p><p>3．处理元数据的变更</p><p>4．处理region的分配或转移</p><p>5．在空闲时间进行数据的负载均衡</p><p>6．通过Zookeeper发布自己的位置给客户端</p><h5 id="1-3-2-RegionServer"><a href="#1-3-2-RegionServer" class="headerlink" title="1.3.2 RegionServer"></a>1.3.2 RegionServer</h5><p><strong>功能</strong></p><p>1．负责存储HBase的实际数据</p><p>2．处理分配给它的Region</p><p>3．刷新缓存到HDFS</p><p>4．维护Hlog</p><p>5．执行压缩</p><p>6．负责处理Region分片</p><h5 id="1-3-3-其他组件"><a href="#1-3-3-其他组件" class="headerlink" title="1.3.3 其他组件"></a>1.3.3 其他组件</h5><p><strong>1．Write-Ahead logs</strong></p><p>HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p><p><strong>2．Region</strong></p><p>Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。</p><p><strong>3．Store</strong></p><p>HFile存储在Store中，一个Store对应HBase表中的一个列族(列簇， Column Family)。</p><p><strong>4．MemStore</strong></p><p>顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。</p><p><strong>5．HFile</strong></p><p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。StoreFile是以Hfile的形式存储在HDFS的。</p><h3 id="第二章-HBase安装"><a href="#第二章-HBase安装" class="headerlink" title="第二章 HBase安装"></a>第二章 HBase安装</h3><h4 id="2-1-Zookeeper正常部署"><a href="#2-1-Zookeeper正常部署" class="headerlink" title="2.1 Zookeeper正常部署"></a>2.1 Zookeeper正常部署</h4><p>首先保证Zookeeper集群的正常部署，并启动之：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><h4 id="2-2-Hadoop正常部署"><a href="#2-2-Hadoop正常部署" class="headerlink" title="2.2 Hadoop正常部署"></a>2.2 Hadoop正常部署</h4><p>Hadoop集群的正常部署并启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><h4 id="2-3-HBase的解压"><a href="#2-3-HBase的解压" class="headerlink" title="2.3 HBase的解压"></a>2.3 HBase的解压</h4><p>解压HBase到指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module</span><br></pre></td></tr></table></figure><h4 id="2-4-HBase的配置文件"><a href="#2-4-HBase的配置文件" class="headerlink" title="2.4 HBase的配置文件"></a>2.4 HBase的配置文件</h4><p>修改HBase对应的配置文件</p><p>(1) hbase-env.sh修改内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line">JDK1.8需要注释</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HBASE_MASTER_OPTS。。。。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HBASE_REGIONSERVER_OPTS。。。</span></span><br></pre></td></tr></table></figure><p>(2) hbase-site.xml修改内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">&lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.4.10/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(3) regionservers:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>(4) 软连接hadoop配置文件到hbase</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml </span><br><span class="line">/opt/module/hbase/conf/core-site.xml</span><br><span class="line">[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml </span><br><span class="line">/opt/module/hbase/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure><h4 id="2-5-HBase远程发送到其他集群"><a href="#2-5-HBase远程发送到其他集群" class="headerlink" title="2.5 HBase远程发送到其他集群"></a>2.5 HBase远程发送到其他集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync hbase/ </span><br></pre></td></tr></table></figure><h4 id="2-6-HBase服务的启动"><a href="#2-6-HBase服务的启动" class="headerlink" title="2.6 HBase服务的启动"></a>2.6 HBase服务的启动</h4><ol><li>启动方式1</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</p><p><font color="red">修复提示</font>：</p><p>a、同步时间服务：参照Hadoop入门</p><p>b、属性：hbase.master.maxclockskew设置更发的值</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>  2.启动方式2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/start-hbase.sh</span><br></pre></td></tr></table></figure><p>对应的停止服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure><h4 id="2-7-查看HBase页面"><a href="#2-7-查看HBase页面" class="headerlink" title="2.7 查看HBase页面"></a>2.7 查看HBase页面</h4><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p><p><a href="http://hadoop102:16010/">http://hadoop102:16010</a></p><h3 id="第三章-HBase-Shell操作"><a href="#第三章-HBase-Shell操作" class="headerlink" title="第三章 HBase Shell操作"></a>第三章 HBase Shell操作</h3><h4 id="3-1-基本操作"><a href="#3-1-基本操作" class="headerlink" title="3.1 基本操作"></a>3.1 基本操作</h4><ol><li><p>进入HBase客户端命令行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure></li><li><p>查看帮助命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; help</span><br></pre></td></tr></table></figure></li><li><p>查看当前数据库中有哪些表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-2-表的操作"><a href="#3-2-表的操作" class="headerlink" title="3.2 表的操作"></a>3.2 表的操作</h4><ol><li><p>创建表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure></li><li><p>插入数据到表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;</span><br><span class="line">hbase(main):004:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;</span><br><span class="line">hbase(main):005:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;</span><br><span class="line">hbase(main):006:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">hbase(main):007:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br></pre></td></tr></table></figure></li><li><p>扫描查看表数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan &#x27;student&#x27;</span><br><span class="line">hbase(main):009:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;, STOPROW  =&gt; &#x27;1001&#x27;&#125;</span><br><span class="line">hbase(main):010:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;&#125;</span><br></pre></td></tr></table></figure></li><li><p>查看表结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe &#x27;student&#x27;</span><br></pre></td></tr></table></figure></li><li><p>更新指定字段的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):012:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;</span><br><span class="line">hbase(main):013:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;</span><br></pre></td></tr></table></figure></li><li><p>查看“指定行”或“指定列族：列”的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;</span><br><span class="line">hbase(main):015:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;</span><br></pre></td></tr></table></figure></li><li><p>统计表数据行数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):021:0&gt; count &#x27;student&#x27;</span><br></pre></td></tr></table></figure></li><li><p>删除数据</p><p>删除某rowkey的全部数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall &#x27;student&#x27;,&#x27;1001&#x27;</span><br></pre></td></tr></table></figure><p>删除某rowkey的一列数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;</span><br></pre></td></tr></table></figure></li><li><p>清空表数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>清空表的操作顺序为先disable，然后再truncate</p></li><li><p>删除表</p><p>首先需要先让该表为disable状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):019:0&gt; disable &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p>然后才能drop这个表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>如果直接drop表，会报错：Error: Table student is enabled. Disable it first.</p></li><li><p>变更表信息</p><p>将info列族中的数据存放3个版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;</span><br><span class="line">hbase(main):022:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#123;COLUMN=&gt;&#x27;info:name&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="第四章-HBase数据结构"><a href="#第四章-HBase数据结构" class="headerlink" title="第四章 HBase数据结构"></a>第四章 HBase数据结构</h3><h4 id="4-1-RowKey"><a href="#4-1-RowKey" class="headerlink" title="4.1 RowKey"></a>4.1 RowKey</h4><p>与nosql数据库们一样,RowKey是用来检索记录的主键。访问HBASE table中的行，只有三种方式：</p><p><strong>1.通过单个RowKey访问(get)</strong></p><p><strong>2.通过RowKey的range（正则）(like)</strong></p><p><strong>3.全表扫描(scan)</strong></p><p>RowKey行键 (RowKey)可以是<strong>任意字符串</strong>(最大长度是64KB，实际应用中长度一般为 10-100bytes)，在HBASE内部，RowKey保存为字节数组。存储时，数据按照RowKey的字典序(byte order)排序存储。设计RowKey时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。<font color="red">(位置相关性)</font></p><h4 id="4-2-Column-Family"><a href="#4-2-Column-Family" class="headerlink" title="4.2 Column Family"></a>4.2 Column Family</h4><p>列族：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部 分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如 courses:history，courses:math都属于courses 这个列族。</p><h4 id="4-3-Cell"><a href="#4-3-Cell" class="headerlink" title="4.3 Cell"></a>4.3 Cell</h4><p>由{rowkey, column Family:columu, version} 唯一确定的单元。<font color="red">cell中的数据是没有类型的，全部是字节码形式存贮</font>。</p><p>关键字：无类型、字节码</p><h4 id="4-4-Time-Stamp"><a href="#4-4-Time-Stamp" class="headerlink" title="4.4 Time Stamp"></a>4.4 Time Stamp</h4><p>HBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒 的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版 本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。</p><p>为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。</p><h4 id="4-5-命名空间"><a href="#4-5-命名空间" class="headerlink" title="4.5 命名空间"></a>4.5 命名空间</h4><p>命名空间的结构</p><p><img src="https://i.loli.net/2020/10/27/8wgkpNXH47WfjmU.png"></p><p>(1) Table：表，所有的表都是命名空间的成员，即表必属于某个命名空间，如果没有指定，则在default默认的命名空间中。</p><p>(2) RegionServer group：一个命名空间包含了默认的RegionServer Group。</p><p>(3) Permission：权限，命名空间能够让我们来定义访问控制列表ACL（Access Control List）。例如，创建表，读取表，删除，更新等等操作。</p><p>(4) Quota：限额，可以强制一个命名空间可包含的region的数量。</p><h3 id="第五章-HBase原理"><a href="#第五章-HBase原理" class="headerlink" title="第五章 HBase原理"></a>第五章 HBase原理</h3><h4 id="5-1-读流程"><a href="#5-1-读流程" class="headerlink" title="5.1 读流程"></a>5.1 读流程</h4><p><img src="https://i.loli.net/2020/10/27/JkdYl4fx7SnRPjb.png"></p><p>1）Client先访问zookeeper，从meta表读取region的位置，然后读取meta表中的数据。meta中又存储了用户表的region信息；</p><p>2）根据namespace、表名和rowkey在meta表中找到对应的region信息；</p><p>3）找到这个region对应的regionserver；</p><p>4）查找对应的region；</p><p>5）先从MemStore找数据，如果没有，再到BlockCache里面读；</p><p>6）BlockCache还没有，再到StoreFile上读(为了读取的效率)；</p><p>7）<font color="red">如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。</font></p><h4 id="5-2-写流程"><a href="#5-2-写流程" class="headerlink" title="5.2 写流程"></a>5.2 写流程</h4><p><img src="https://i.loli.net/2020/10/27/atWvhcZoDiJrbH4.png"></p><p>1）Client向HregionServer发送写请求；</p><p>2）HregionServer将数据写到HLog（write ahead log）。为了数据的持久化和恢复；</p><p>3）HregionServer将数据写到内存（MemStore）；</p><p>4）反馈Client写成功。</p><h4 id="5-3-数据flush过程"><a href="#5-3-数据flush过程" class="headerlink" title="5.3 数据flush过程"></a>5.3 数据flush过程</h4><p>1）当MemStore数据达到阈值（默认是128M，老版本是64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据；</p><p>2）并将数据存储到HDFS中；</p><p>3）在HLog中做标记点。</p><h4 id="5-4-数据合并过程"><a href="#5-4-数据合并过程" class="headerlink" title="5.4 数据合并过程"></a>5.4 数据合并过程</h4><p>1）当数据块达到3块，Hmaster触发合并操作，Region将数据块加载到本地，进行合并；</p><p>2）当合并的数据超过256M，进行拆分，将拆分后的Region分配给不同的HregionServer管理；</p><p>3）当HregionServer宕机后，将HregionServer上的hlog拆分，然后分配给不同的HregionServer加载，修改.META.；</p><p>4）注意：HLog会同步到HDFS。</p><h3 id="第六章-HBase-API操作"><a href="#第六章-HBase-API操作" class="headerlink" title="第六章 HBase API操作"></a>第六章 HBase API操作</h3><h4 id="6-1-环境准备"><a href="#6-1-环境准备" class="headerlink" title="6.1 环境准备"></a>6.1 环境准备</h4><p>新建项目后在pom.xml中添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="6-2-HBase-API"><a href="#6-2-HBase-API" class="headerlink" title="6.2 HBase API"></a>6.2 HBase API</h4><h5 id="6-2-1-获取Configuration-对象"><a href="#6-2-1-获取Configuration-对象" class="headerlink" title="6.2.1 获取Configuration 对象"></a>6.2.1 获取Configuration 对象</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Configuration conf;</span><br><span class="line"><span class="keyword">static</span>&#123;</span><br><span class="line"><span class="comment">//使用HBaseConfiguration的单例方法实例化</span></span><br><span class="line">conf = HBaseConfiguration.create();</span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;192.168.9.102&quot;</span>);</span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-2-判断表是否存在"><a href="#6-2-2-判断表是否存在" class="headerlink" title="6.2.2 判断表是否存在"></a>6.2.2 判断表是否存在</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExist</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException,</span></span><br><span class="line"><span class="function"> ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line"><span class="comment">//在HBase中管理、访问表需要先创建HBaseAdmin对象</span></span><br><span class="line"><span class="comment">//Connection connection = ConnectionFactory.createConnection(conf);</span></span><br><span class="line"><span class="comment">//HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span></span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="keyword">return</span> admin.tableExists(tableName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-3-创建表"><a href="#6-2-3-创建表" class="headerlink" title="6.2.3 创建表"></a>6.2.3 创建表</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName, String... columnFamily)</span> <span class="keyword">throws</span></span></span><br><span class="line"><span class="function"> MasterNotRunningException, ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="comment">//判断表是否存在</span></span><br><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;已存在&quot;</span>);</span><br><span class="line"><span class="comment">//System.exit(0);</span></span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="comment">//创建表属性对象,表名需要转字节</span></span><br><span class="line">HTableDescriptor descriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span><br><span class="line"><span class="comment">//创建多个列族</span></span><br><span class="line"><span class="keyword">for</span>(String cf : columnFamily)&#123;</span><br><span class="line">descriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(cf));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据对表的配置，创建表</span></span><br><span class="line">admin.createTable(descriptor);</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;创建成功！&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-4-删除表"><a href="#6-2-4-删除表" class="headerlink" title="6.2.4 删除表"></a>6.2.4 删除表</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException,</span></span><br><span class="line"><span class="function"> ZooKeeperConnectionException, IOException</span>&#123;</span><br><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span><br><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">admin.deleteTable(tableName);</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;删除成功！&quot;</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;表&quot;</span> + tableName + <span class="string">&quot;不存在！&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-5-向表中插入数据"><a href="#6-2-5-向表中插入数据" class="headerlink" title="6.2.5 向表中插入数据"></a>6.2.5 向表中插入数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addRowData</span><span class="params">(String tableName, String rowKey, String columnFamily, String</span></span></span><br><span class="line"><span class="function"><span class="params"> column, String value)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line"><span class="comment">//创建HTable对象</span></span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line"><span class="comment">//向表中插入数据</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line"><span class="comment">//向Put对象中组装数据</span></span><br><span class="line">put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));</span><br><span class="line">hTable.put(put);</span><br><span class="line">hTable.close();</span><br><span class="line">System.out.println(<span class="string">&quot;插入数据成功&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-6-删除多行数据"><a href="#6-2-6-删除多行数据" class="headerlink" title="6.2.6 删除多行数据"></a>6.2.6 删除多行数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteMultiRow</span><span class="params">(String tableName, String... rows)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">List&lt;Delete&gt; deleteList = <span class="keyword">new</span> ArrayList&lt;Delete&gt;();</span><br><span class="line"><span class="keyword">for</span>(String row : rows)&#123;</span><br><span class="line">Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(row));</span><br><span class="line">deleteList.add(delete);</span><br><span class="line">&#125;</span><br><span class="line">hTable.delete(deleteList);</span><br><span class="line">hTable.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-7-获取所有数据"><a href="#6-2-7-获取所有数据" class="headerlink" title="6.2.7 获取所有数据"></a>6.2.7 获取所有数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getAllRows</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line"><span class="comment">//得到用于扫描region的对象</span></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"><span class="comment">//使用HTable得到resultcanner实现类的对象</span></span><br><span class="line">ResultScanner resultScanner = hTable.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span>(Result result : resultScanner)&#123;</span><br><span class="line">Cell[] cells = result.rawCells();</span><br><span class="line"><span class="keyword">for</span>(Cell cell : cells)&#123;</span><br><span class="line"><span class="comment">//得到rowkey</span></span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span><br><span class="line"><span class="comment">//得到列族</span></span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-8-获取某一行数据"><a href="#6-2-8-获取某一行数据" class="headerlink" title="6.2.8 获取某一行数据"></a>6.2.8 获取某一行数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line"><span class="comment">//get.setMaxVersions();显示所有版本</span></span><br><span class="line">    <span class="comment">//get.setTimeStamp();显示指定时间戳的版本</span></span><br><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(result.getRow()));</span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;时间戳:&quot;</span> + cell.getTimestamp());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-2-9-获取某一行指定“列族-列”的数据"><a href="#6-2-9-获取某一行指定“列族-列”的数据" class="headerlink" title="6.2.9 获取某一行指定“列族:列”的数据"></a>6.2.9 获取某一行指定“列族:列”的数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRowQualifier</span><span class="params">(String tableName, String rowKey, String family, String</span></span></span><br><span class="line"><span class="function"><span class="params"> qualifier)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));</span><br><span class="line">Result result = table.get(get);</span><br><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;行键:&quot;</span> + Bytes.toString(result.getRow()));</span><br><span class="line">System.out.println(<span class="string">&quot;列族&quot;</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;列:&quot;</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">System.out.println(<span class="string">&quot;值:&quot;</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-3-MapReduce"><a href="#6-3-MapReduce" class="headerlink" title="6.3 MapReduce"></a>6.3 MapReduce</h4><p>通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。</p><h5 id="6-3-1-官方HBase-MapReduce"><a href="#6-3-1-官方HBase-MapReduce" class="headerlink" title="6.3.1 官方HBase-MapReduce"></a>6.3.1 官方HBase-MapReduce</h5><ol><li><p><strong>查看HBase的MapReduce任务的执行</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/hbase mapredcp</span></span><br></pre></td></tr></table></figure></li><li><p><strong>环境变量的导入</strong></p><p>(1) 执行环境变量的导入（临时生效，在命令行执行下述操作）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HBASE_HOME=/opt/module/hbase-1.3.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_CLASSPATH=`<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin/hbase mapredcp`</span></span><br></pre></td></tr></table></figure><p>(2) 永久生效：在/etc/profile配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/module/hbase-1.3.1</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><p>并在hadoop-env.sh中配置：(注意：在for循环之后配)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><strong>运行官方的MapReduce任务</strong></li></ol><p>​     – 案例一：统计Student表中有多少行数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter student</span></span><br></pre></td></tr></table></figure><p>​     – 案例二：使用MapReduce将本地数据导入到HBase</p><p>​     (1) 在本地创建一个tsv格式的文件：fruit.tsv</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1001AppleRed</span><br><span class="line">1002PearYellow</span><br><span class="line">1003PineappleYellow</span><br></pre></td></tr></table></figure><p>​     (2) 创建HBase表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#39;fruit&#39;,&#39;info&#39;</span><br></pre></td></tr></table></figure><p>​     (3) 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/</span></span><br></pre></td></tr></table></figure><p>​     (4) 执行MapReduce到HBase的fruit表中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;bin&#x2F;yarn jar lib&#x2F;hbase-server-1.3.1.jar importtsv \</span><br><span class="line">-Dimporttsv.columns&#x3D;HBASE_ROW_KEY,info:name,info:color fruit \</span><br><span class="line">hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F;input_fruit</span><br></pre></td></tr></table></figure><p>​     (5) 使用scan命令查看导入后的结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; scan ‘fruit’</span><br></pre></td></tr></table></figure><h5 id="6-3-2-自定义HBase-MapReduce1"><a href="#6-3-2-自定义HBase-MapReduce1" class="headerlink" title="6.3.2 自定义HBase-MapReduce1"></a>6.3.2 自定义HBase-MapReduce1</h5><p>目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。</p><p>分步实现：</p><p><strong>1．构建ReadFruitMapper类，用于读取fruit表中的数据</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.Cell;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.CellUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Result;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFruitMapper</span> <span class="keyword">extends</span> <span class="title">TableMapper</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(ImmutableBytesWritable key, Result value, Context context)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(key.get());</span><br><span class="line"><span class="comment">//遍历添加column行</span></span><br><span class="line"><span class="keyword">for</span>(Cell cell: value.rawCells())&#123;</span><br><span class="line"><span class="comment">//添加/克隆列族:info</span></span><br><span class="line"><span class="keyword">if</span>(<span class="string">&quot;info&quot;</span>.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123;</span><br><span class="line"><span class="comment">//添加/克隆列：name</span></span><br><span class="line"><span class="keyword">if</span>(<span class="string">&quot;name&quot;</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line"><span class="comment">//将该列cell加入到put对象中</span></span><br><span class="line">put.add(cell);</span><br><span class="line"><span class="comment">//添加/克隆列:color</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;color&quot;</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line"><span class="comment">//向该列cell加入到put对象中</span></span><br><span class="line">put.add(cell);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将从fruit读取到的每行数据写入到context中作为map的输出</span></span><br><span class="line">context.write(key, put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2． 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hbase_mr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteFruitMRReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//读出来的每一行数据写入到fruit_mr表中</span></span><br><span class="line"><span class="keyword">for</span>(Put put: values)&#123;</span><br><span class="line">context.write(NullWritable.get(), put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3．构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//组装Job</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//得到Configuration</span></span><br><span class="line">Configuration conf = <span class="keyword">this</span>.getConf();</span><br><span class="line"><span class="comment">//创建Job任务</span></span><br><span class="line">Job job = Job.getInstance(conf, <span class="keyword">this</span>.getClass().getSimpleName());</span><br><span class="line">job.setJarByClass(Fruit2FruitMRRunner.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//配置Job</span></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setCacheBlocks(<span class="keyword">false</span>);</span><br><span class="line">scan.setCaching(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本</span></span><br><span class="line">TableMapReduceUtil.initTableMapperJob(</span><br><span class="line"><span class="string">&quot;fruit&quot;</span>, <span class="comment">//数据源的表名</span></span><br><span class="line">scan, <span class="comment">//scan扫描控制器</span></span><br><span class="line">ReadFruitMapper.class,<span class="comment">//设置Mapper类</span></span><br><span class="line">ImmutableBytesWritable.class,<span class="comment">//设置Mapper输出key类型</span></span><br><span class="line">Put.class,<span class="comment">//设置Mapper输出value值类型</span></span><br><span class="line">job<span class="comment">//设置给哪个JOB</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">//设置Reducer</span></span><br><span class="line">TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;fruit_mr&quot;</span>, WriteFruitMRReducer.class, job);</span><br><span class="line"><span class="comment">//设置Reduce数量，最少1个</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> isSuccess = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">if</span>(!isSuccess)&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Job running with error&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> isSuccess ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4．主函数中调用运行该Job任务</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line"><span class="keyword">int</span> status = ToolRunner.run(conf, <span class="keyword">new</span> Fruit2FruitMRRunner(), args);</span><br><span class="line">System.exit(status);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5．打包运行任务</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;bin&#x2F;yarn jar ~&#x2F;softwares&#x2F;jars&#x2F;hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr1.Fruit2FruitMRRunner</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：运行任务前，如果待数据导入的表不存在，则需要提前创建。</p><p><font color="red">提示</font>：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p><h5 id="6-3-3-自定义HBase-MapReduce2"><a href="#6-3-3-自定义HBase-MapReduce2" class="headerlink" title="6.3.3 自定义HBase-MapReduce2"></a>6.3.3 自定义HBase-MapReduce2</h5><p>目标：实现将HDFS中的数据写入到HBase表中。</p><p>分步实现：</p><p><strong>1．构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadFruitFromHDFSMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//从HDFS中读取的数据</span></span><br><span class="line">String lineValue = value.toString();</span><br><span class="line"><span class="comment">//读取出来的每行数据使用\t进行分割，存于String数组</span></span><br><span class="line">String[] values = lineValue.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据数据中值的含义取值</span></span><br><span class="line">String rowKey = values[<span class="number">0</span>];</span><br><span class="line">String name = values[<span class="number">1</span>];</span><br><span class="line">String color = values[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化rowKey</span></span><br><span class="line">ImmutableBytesWritable rowKeyWritable = <span class="keyword">new</span> ImmutableBytesWritable(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化put对象</span></span><br><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line"></span><br><span class="line"><span class="comment">//参数分别:列族、列、值  </span></span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;name&quot;</span>),  Bytes.toBytes(name)); </span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">&quot;info&quot;</span>), Bytes.toBytes(<span class="string">&quot;color&quot;</span>),  Bytes.toBytes(color)); </span><br><span class="line">        </span><br><span class="line">        context.write(rowKeyWritable, put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2．构建WriteFruitMRFromTxtReducer类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.z.hbase.mr2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteFruitMRFromTxtReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//读出来的每一行数据写入到fruit_hdfs表中</span></span><br><span class="line"><span class="keyword">for</span>(Put put: values)&#123;</span><br><span class="line">context.write(NullWritable.get(), put);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3．创建Txt2FruitRunner组装Job</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//得到Configuration</span></span><br><span class="line">Configuration conf = <span class="keyword">this</span>.getConf();</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建Job任务</span></span><br><span class="line">Job job = Job.getInstance(conf, <span class="keyword">this</span>.getClass().getSimpleName());</span><br><span class="line">job.setJarByClass(Txt2FruitRunner.class);</span><br><span class="line">Path inPath = <span class="keyword">new</span> Path(<span class="string">&quot;hdfs://hadoop102:9000/input_fruit/fruit.tsv&quot;</span>);</span><br><span class="line">FileInputFormat.addInputPath(job, inPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Mapper</span></span><br><span class="line">job.setMapperClass(ReadFruitFromHDFSMapper.class);</span><br><span class="line">job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Reducer</span></span><br><span class="line">TableMapReduceUtil.initTableReducerJob(<span class="string">&quot;fruit_mr&quot;</span>, WriteFruitMRFromTxtReducer.class, job);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置Reduce数量，最少1个</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> isSuccess = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">if</span>(!isSuccess)&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Job running with error&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> isSuccess ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4．调用执行Job</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">    <span class="keyword">int</span> status = ToolRunner.run(conf, <span class="keyword">new</span> Txt2FruitRunner(), args);</span><br><span class="line">    System.exit(status);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5．打包运行</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/module/hadoop-2.7.2/bin/yarn jar hbase-0.0.1-SNAPSHOT.jar com.atguigu.hbase.mr2.Txt2FruitRunner</span></span><br></pre></td></tr></table></figure><p>提示：运行任务前，如果待数据导入的表不存在，则需要提前创建之。</p><p>提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p><h4 id="6-4-与Hive的集成"><a href="#6-4-与Hive的集成" class="headerlink" title="6.4 与Hive的集成"></a>6.4 与Hive的集成</h4><h5 id="6-4-1-HBase与Hive的对比"><a href="#6-4-1-HBase与Hive的对比" class="headerlink" title="6.4.1 HBase与Hive的对比"></a>6.4.1 HBase与Hive的对比</h5><p><strong>1．Hive</strong></p><p>(1) 数据仓库</p><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p><p>(2) 用于数据分析、清洗</p><p>Hive适用于离线的数据分析和清洗，延迟较高。</p><p>(3) 基于HDFS、MapReduce</p><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p><p><strong>2．HBase</strong></p><p>(1) 数据库</p><p>是一种面向列存储的非关系型数据库。</p><p>(2) 用于存储结构化和非结构化的数据</p><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p><p>(3) 基于HDFS</p><p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p><p>(4) 延迟较低，接入在线业务使用</p><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 HBase简介 / 第二章 HBase安装 / 第三章 HBase Shell操作 /&lt;br&gt;第四章 HBase数据结构 / 第五章 HBase原理 / 第六章 HBase API操作&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive高阶</title>
    <link href="http://luo6656.github.io/2020/07/05/BigDataFrame/Hive%E9%AB%98%E9%98%B6/"/>
    <id>http://luo6656.github.io/2020/07/05/BigDataFrame/Hive%E9%AB%98%E9%98%B6/</id>
    <published>2020-07-04T16:00:00.000Z</published>
    <updated>2020-10-27T05:11:13.734Z</updated>
    
    <content type="html"><![CDATA[<p>第七章 函数 / 第八章 压缩和存储 </p><a id="more"></a><h3 id="第七章-函数"><a href="#第七章-函数" class="headerlink" title="第七章 函数"></a>第七章 函数</h3><h4 id="7-1-系统内置函数"><a href="#7-1-系统内置函数" class="headerlink" title="7.1 系统内置函数"></a>7.1 系统内置函数</h4><ol><li><p>查看系统自带的函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br></pre></td></tr></table></figure></li><li><p>显示自带的函数的用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function upper;</span><br></pre></td></tr></table></figure></li><li><p>详细显示自带的函数的用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function extended upper;</span><br></pre></td></tr></table></figure></li></ol><h4 id="7-2-自定义函数"><a href="#7-2-自定义函数" class="headerlink" title="7.2 自定义函数"></a>7.2 自定义函数</h4><ol><li><p>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</p></li><li><p>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户定义函数（UDF：user-defined function）</p></li><li><p>根据用户自定义函数类别分为以下三种    （多针对的是行）</p><p>（1）UDF（User-Defined-Function）</p><p>​        一进一出</p><p>（2）UDAF（User-Defined Aggregation Function）</p><p>​        聚集函数，多进一出</p><p>​        类似于：count/max/min</p><p>（3）UDTF（User-Defined Table-Generating Functions）</p><p>​        一进多出</p><p>​        如lateral view explore()</p></li><li><p>官方文档地址</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p></blockquote></li><li><p>编程步骤</p><p>（1）<font color="red">继承org.apache.hadoop.hive.ql.exec.UDF</font></p><p>（2）<font color="red">需要实现evaluate函数；evaluate函数支持重载；</font></p><p>（3）<font color="red">在hive的命令行窗口创建函数</font></p><p>​        a）<font color="red">添加jar</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure><p>​        b）<font color="red">创建function</font></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [<span class="keyword">temporary</span>] <span class="keyword">function</span> [dbname.]function_name <span class="keyword">AS</span> class_name;</span><br></pre></td></tr></table></figure><p>​    （4）在hive的命令行窗口删除函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Drop</span> [<span class="keyword">temporary</span>] <span class="keyword">function</span> [<span class="keyword">if</span> <span class="keyword">exists</span>] [dbname.]function_name;</span><br></pre></td></tr></table></figure></li><li><p>注意事项</p><p>（1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p></li></ol><h4 id="7-3-自定义UDF函数"><a href="#7-3-自定义UDF函数" class="headerlink" title="7.3 自定义UDF函数"></a>7.3 自定义UDF函数</h4><ol><li><p>创建一个Maven工程Hive</p></li><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>创建一个类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lower</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span> <span class="params">(<span class="keyword">final</span> String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s.toLowerCase();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打包成jar包上传到服务器/opt/module/jars/udf.jar</p></li><li><p>将jar包添加到hive的classpath</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/datas/udf.jar;</span><br></pre></td></tr></table></figure></li><li><p>创建临时函数与开发好的Java class关联</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function mylower as &quot;com.atguigu.hive.Lower&quot;; </span><br><span class="line"><span class="comment">/*退出hive就不可用了*/</span></span><br></pre></td></tr></table></figure></li><li><p>即可在hql中使用自定义的函数strip</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, mylower(ename) lowername from emp;</span><br></pre></td></tr></table></figure></li></ol><h3 id="第八章-压缩和存储"><a href="#第八章-压缩和存储" class="headerlink" title="第八章 压缩和存储"></a>第八章 压缩和存储</h3><h4 id="8-1-Hadoop源码编译支持Snappy压缩"><a href="#8-1-Hadoop源码编译支持Snappy压缩" class="headerlink" title="8.1 Hadoop源码编译支持Snappy压缩"></a>8.1 Hadoop源码编译支持Snappy压缩</h4><h5 id="8-1-1-资源准备"><a href="#8-1-1-资源准备" class="headerlink" title="8.1.1 资源准备"></a>8.1.1 资源准备</h5><ol><li><p>CentOS联网</p><p>配置CentOS能连接外网。Linux虚拟机ping <a href="http://www.baidu.com是畅通的/">www.baidu.com是畅通的</a></p><p>注意：<font color="red">采用root角色编译</font>，减少文件夹权限出现问题</p></li><li><p>jar包准备(hadoop源码、JDK8、maven、protobuf)</p><p>（1）hadoop-2.7.2-src.tar.gz</p><p>（2）jdk-8u144-linux-x64.tar.gz</p><p>（3）snappy-1.1.3.tar.gz</p><p>（4）apache-maven-3.0.5-bin.tar.gz</p><p>（5）protobuf-2.5.0.tar.gz</p></li></ol><h5 id="8-1-2-jar包安装"><a href="#8-1-2-jar包安装" class="headerlink" title="8.1.2 jar包安装"></a>8.1.2 jar包安装</h5><p>​        <font color="red">注意：所有操作必须在root用户下完成</font></p><ol><li><p>JDK解压、配置环境变量JAVA_HOME和PATH，验证<a href="http://lib.csdn.net/base/javase">java</a>-version(如下都需要验证是否配置成功)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 software]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@hadoop101 software]#source /etc/profile</span><br></pre></td></tr></table></figure><p>验证命令：java -version</p></li><li><p>Maven解压、配置 MAVEN_HOME和PATH</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.0.5</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line">[root@hadoop101 software]#source /etc/profile</span><br></pre></td></tr></table></figure><p>验证命令：mvn -version</p></li></ol><h5 id="8-1-3-编译源码"><a href="#8-1-3-编译源码" class="headerlink" title="8.1.3 编译源码"></a>8.1.3 编译源码</h5><ol><li><p>准备编译环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# yum install svn</span><br><span class="line">[root@hadoop101 software]# yum install autoconf automake libtool cmake</span><br><span class="line">[root@hadoop101 software]# yum install ncurses-devel</span><br><span class="line">[root@hadoop101 software]# yum install openssl-devel</span><br><span class="line">[root@hadoop101 software]# yum install gcc*</span><br></pre></td></tr></table></figure></li><li><p>编译安装snappy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 module]# cd snappy-1.1.3/</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# ./configure</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# make</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# make install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看snappy库文件</span></span><br><span class="line">[root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy</span><br></pre></td></tr></table></figure></li><li><p>编译安装protobuf </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 module]# cd protobuf-2.5.0/</span><br><span class="line">[root@hadoop101 protobuf-2.5.0]# ./configure </span><br><span class="line">[root@hadoop101 protobuf-2.5.0]#  make </span><br><span class="line">[root@hadoop101 protobuf-2.5.0]#  make install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看protobuf版本以测试是否安装成功</span></span><br><span class="line">[root@hadoop101 protobuf-2.5.0]# protoc --version</span><br></pre></td></tr></table></figure></li><li><p>编译hadoop native</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz</span><br><span class="line">[root@hadoop101 software]# cd hadoop-2.7.2-src/</span><br><span class="line">[root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy</span><br></pre></td></tr></table></figure><p>执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/<a href="http://lib.csdn.net/base/hadoop">hadoop</a>-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。</p></li></ol><h4 id="8-2-Hadoop压缩配置"><a href="#8-2-Hadoop压缩配置" class="headerlink" title="8.2 Hadoop压缩配置"></a>8.2 Hadoop压缩配置</h4><h5 id="8-2-1-MR支持的压缩编码"><a href="#8-2-1-MR支持的压缩编码" class="headerlink" title="8.2.1 MR支持的压缩编码"></a>8.2.1 MR支持的压缩编码</h5><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFLATE</td><td>无</td><td>DEFLATE</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>无</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较：</p><p>表6-10</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h5 id="8-2-2-压缩参数配置"><a href="#8-2-2-压缩参数配置" class="headerlink" title="8.2.2 压缩参数配置"></a>8.2.2 压缩参数配置</h5><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h4 id="8-3-开启Map输出阶段压缩"><a href="#8-3-开启Map输出阶段压缩" class="headerlink" title="8.3 开启Map输出阶段压缩"></a>8.3 开启Map输出阶段压缩</h4><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p><p><strong>案例实操</strong></p><ol><li><p>开启hive中间传输数据压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure></li><li><p>开启mapreduce中map输出压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce中map输出数据的压缩方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li><li><p>执行查询语句</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure></li></ol><h4 id="8-4-开启Reduce输出阶段压缩"><a href="#8-4-开启Reduce输出阶段压缩" class="headerlink" title="8.4 开启Reduce输出阶段压缩"></a>8.4 开启Reduce输出阶段压缩</h4><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p><p><strong>案例实操</strong></p><ol><li><p>开启hive最终输出数据压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure></li><li><p>开启mapreduce最终输出数据压缩</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce最终数据输出压缩方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce最终数据输出压缩为块压缩</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure></li><li><p>测试一下输出结果是否是压缩文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/module/datas/distribute-result&#x27; <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="8-5-文件存储格式"><a href="#8-5-文件存储格式" class="headerlink" title="8.5 文件存储格式"></a>8.5 文件存储格式</h4><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h5 id="8-5-1-列式存储和行式存储"><a href="#8-5-1-列式存储和行式存储" class="headerlink" title="8.5.1 列式存储和行式存储"></a>8.5.1 列式存储和行式存储</h5><p><img src="https://i.loli.net/2020/10/27/oWu7lQi6reEbjKI.png"></p><p>如图6-10所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><p>1．行存储的特点</p><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><p>2．列存储的特点</p><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><p>​    <font color="red">TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</font></p><p>​    <font color="red">ORC和PARQUET是基于列式存储的。</font></p><h5 id="8-5-2-TextFile格式"><a href="#8-5-2-TextFile格式" class="headerlink" title="8.5.2 TextFile格式"></a>8.5.2 TextFile格式</h5><p>​        默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h5 id="8-5-3-Orc格式"><a href="#8-5-3-Orc格式" class="headerlink" title="8.5.3 Orc格式"></a>8.5.3 Orc格式</h5><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p><p>如图6-11所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p><p><img src="https://i.loli.net/2020/10/27/SZj1Bvfl3KchVGb.png"></p><p>​     1）Index Data：一个轻量级的index，默认是<font color="red">每隔1W行做一个索引</font>。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p><pre><code>  2）Row Data：存的是具体的数据，&lt;font color=&quot;red&quot;&gt;先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储&lt;/font&gt;。  3）Stripe Footer：存的是各个Stream的类型，长度等信息。</code></pre><p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h5 id="8-5-4-Parquet格式"><a href="#8-5-4-Parquet格式" class="headerlink" title="8.5.4 Parquet格式"></a>8.5.4 Parquet格式</h5><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，<font color="red">因此Parquet格式文件是自解析的。</font></p><ol><li><p>行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p></li><li><p>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p></li><li><p>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p></li></ol><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把<font color="red">每一个行组由一个Mapper任务处理，增大任务执行并行度。</font>Parquet文件的格式如图6-12所示。</p><p><img src="https://i.loli.net/2020/10/27/h7MIWijkGQXaut1.png"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h5 id="8-5-5-主流文件存储格式对比实验"><a href="#8-5-5-主流文件存储格式对比实验" class="headerlink" title="8.5.5 主流文件存储格式对比实验"></a>8.5.5 主流文件存储格式对比实验</h5><p>从存储文件的压缩比和查询速度两个角度对比。</p><p><strong>存储文件的压缩比测试：</strong></p><ol><li><p>测试数据</p><p>log.data</p></li><li><p>TextFile</p><p>(1)创建表，存储数据格式为TEXTFILE </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/log.data&#x27; into table log_text ;</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;</span><br></pre></td></tr></table></figure><p>18.1 M  /user/hive/warehouse/log_text/log.data</p></li><li><p>ORC（ORC格式默认开启压缩）</p><p>(1)创建表，存储数据格式为ORC </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc select * from log_text ; /*不能使用load，因为load是直接put的*/</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/;</span><br><span class="line">2.8 M  /user/hive/warehouse/log_orc/000000_0</span><br></pre></td></tr></table></figure></li><li><p>Parquet</p><p>(1)创建表，存储数据格式为parquet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;</span><br><span class="line">13.1 M /user/hive/warehouse/log_parquet/000000_0</span><br></pre></td></tr></table></figure><p>存储文件的压缩比总结：</p><p>ORC &gt;  Parquet &gt;  textFile</p></li></ol><p><strong>存储文件的查询速度测试：</strong></p><p>TextFile </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_text;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 21.54 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 21.08 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 19.298 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>ORC </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_orc;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 20.867 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 22.667 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 18.36 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>Parquet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_parquet;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 22.922 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 21.074 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 18.384 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>存储文件的查询速度总结：查询速度相近。</p><h4 id="8-6-存储和压缩结合"><a href="#8-6-存储和压缩结合" class="headerlink" title="8.6 存储和压缩结合"></a>8.6 存储和压缩结合</h4><h5 id="8-6-1-修改Hadoop集群具有Snappy压缩方法"><a href="#8-6-1-修改Hadoop集群具有Snappy压缩方法" class="headerlink" title="8.6.1 修改Hadoop集群具有Snappy压缩方法"></a>8.6.1 修改Hadoop集群具有Snappy压缩方法</h5><ol><li><p>查看hadoop checknative命令使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ hadoop</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br></pre></td></tr></table></figure></li><li><p>查看hadoop支持的压缩方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative</span><br><span class="line">17&#x2F;12&#x2F;24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load&#x2F;initialize native-bzip2 library system-native, will use pure-Java version</span><br><span class="line">17&#x2F;12&#x2F;24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libhadoop.so</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  false </span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   false</span><br></pre></td></tr></table></figure></li><li><p>将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中</p></li></ol><ol start="4"><li><p>解压hadoop-2.7.2.tar.gz到当前路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 native]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native</span><br><span class="line">[atguigu@hadoop102 native]$ ll</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  472950 9月   1 10:19 libsnappy.a</span><br><span class="line">-rwxr-xr-x. 1 atguigu atguigu     955 9月   1 10:19 libsnappy.la</span><br><span class="line">lrwxrwxrwx. 1 atguigu atguigu      18 12月 24 20:39 libsnappy.so -&gt; libsnappy.so.1.3.0</span><br><span class="line">lrwxrwxrwx. 1 atguigu atguigu      18 12月 24 20:39 libsnappy.so.1 -&gt; libsnappy.so.1.3.0</span><br><span class="line">-rwxr-xr-x. 1 atguigu atguigu  228177 9月   1 10:19 libsnappy.so.1.3.0</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 native]$ cp ..&#x2F;native&#x2F;* &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>分发集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$ xsync native&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>再次查看hadoop支持的压缩类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop checknative</span><br><span class="line">17&#x2F;12&#x2F;24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load&#x2F;initialize native-bzip2 library system-native, will use pure-Java version</span><br><span class="line">17&#x2F;12&#x2F;24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libhadoop.so</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libsnappy.so.1</span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   false</span><br></pre></td></tr></table></figure></li><li><p>重新启动hadoop集群和hive</p></li></ol><h5 id="8-6-2-测试存储和压缩"><a href="#8-6-2-测试存储和压缩" class="headerlink" title="8.6.2 测试存储和压缩"></a>8.6.2 测试存储和压缩</h5><p>官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p><p>ORC存储方式的压缩：</p><table><thead><tr><th>Key</th><th>Default</th><th>Notes</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>high level compression (one of NONE, ZLIB, SNAPPY)</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>number of bytes in each compression chunk</td></tr><tr><td>orc.stripe.size</td><td>268,435,456</td><td>number of bytes in each stripe</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>number of rows between index entries (must be &gt;= 1000)</td></tr><tr><td>orc.create.index</td><td>true</td><td>whether to create row indexes</td></tr><tr><td>orc.bloom.filter.columns</td><td>“”</td><td>comma separated list of column names for which bloom filter should be created</td></tr><tr><td>orc.bloom.filter.fpp</td><td>0.05</td><td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td></tr></tbody></table><p>注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现</p><ol><li><p>创建一个非压缩的ORC存储方式</p><p>（1）建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">&quot;orc.compress&quot;</span>=<span class="string">&quot;NONE&quot;</span>);</span><br></pre></td></tr></table></figure><p>（2）插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc_none select * from log_text ;</span><br></pre></td></tr></table></figure><p>（3）查看插入后数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ;</span><br><span class="line">7.7 M  /user/hive/warehouse/log_orc_none/000000_0</span><br></pre></td></tr></table></figure></li><li><p>创建一个SNAPPY压缩的ORC存储方式</p><p>（1）建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">&quot;orc.compress&quot;</span>=<span class="string">&quot;SNAPPY&quot;</span>);</span><br></pre></td></tr></table></figure><p>（2）插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc_snappy select * from log_text ;</span><br></pre></td></tr></table></figure><p>（3）查看插入后数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ;</span><br></pre></td></tr></table></figure><p>3.8 M  /user/hive/warehouse/log_orc_snappy/000000_0</p></li><li><p>上一节中默认创建的ORC存储方式，导入数据后的大小为2.8 M /user/hive/warehouse/log_orc/000000_0</p><p>比Snappy压缩的还小。原因是orc存储文件默认采用<font color="red">ZLIB压缩</font>，ZLIB采用的是deflate压缩算法。比snappy压缩的小。</p></li><li><p>存储方式和压缩总结</p><p>在实际的项目开发当中，hive表的数据存储格式一般选择:orc或parquet。压缩方式一般选择snappy，lzo</p></li></ol><h3 id="第九章-企业级调优"><a href="#第九章-企业级调优" class="headerlink" title="第九章 企业级调优"></a>第九章 企业级调优</h3><h4 id="9-1-Fetch抓取"><a href="#9-1-Fetch抓取" class="headerlink" title="9.1 Fetch抓取"></a>9.1 Fetch抓取</h4><p>Fetch抓取是指，<font color="red">Hive中对某些情况的查询可以不必使用MapReduce计算</font>。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是<font color="red">minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce</font>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;more&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some <span class="keyword">select</span> queries can be converted <span class="keyword">to</span> single <span class="keyword">FETCH</span> task minimizing latency.</span><br><span class="line">      Currently the <span class="keyword">query</span> should be single sourced <span class="keyword">not</span> <span class="keyword">having</span> <span class="keyword">any</span> subquery <span class="keyword">and</span> should <span class="keyword">not</span> have <span class="keyword">any</span> aggregations <span class="keyword">or</span> distincts (which incurs RS), <span class="keyword">lateral</span> views <span class="keyword">and</span> joins.</span><br><span class="line">      <span class="number">0.</span> <span class="keyword">none</span> : <span class="keyword">disable</span> hive.fetch.task.conversion</span><br><span class="line">      <span class="number">1.</span> minimal : <span class="keyword">SELECT</span> STAR, FILTER <span class="keyword">on</span> <span class="keyword">partition</span> <span class="keyword">columns</span>, <span class="keyword">LIMIT</span> <span class="keyword">only</span></span><br><span class="line">      <span class="number">2.</span> more  : <span class="keyword">SELECT</span>, FILTER, <span class="keyword">LIMIT</span> <span class="keyword">only</span> (support <span class="keyword">TABLESAMPLE</span> <span class="keyword">and</span> <span class="keyword">virtual</span> <span class="keyword">columns</span>)</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><p><strong>案例实操：</strong></p><p>​    1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.fetch.task.conversion=none;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp limit 3;</span><br></pre></td></tr></table></figure><p>​    2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.fetch.task.conversion=more;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp limit 3;</span><br></pre></td></tr></table></figure><h4 id="9-2-本地模式"><a href="#9-2-本地模式" class="headerlink" title="9.2 本地模式"></a>9.2 本地模式</h4><p>​        大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，<font color="red">Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</font></p><p>​        用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化，默认是false。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  //开启本地mr</span><br><span class="line">//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line">//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max=<span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><strong>案例实操：</strong></p><p>1）开启本地模式，并执行查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.exec.mode.local.auto=true; </span><br><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">Time taken: 1.328 seconds, Fetched: 14 row(s)</span><br></pre></td></tr></table></figure><p>2）关闭本地模式，并执行查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.exec.mode.local.auto=false; </span><br><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">Time taken: 20.09 seconds, Fetched: 14 row(s)</span><br></pre></td></tr></table></figure><h4 id="9-3-表的优化"><a href="#9-3-表的优化" class="headerlink" title="9.3 表的优化"></a>9.3 表的优化</h4><h5 id="9-3-1-小表、大表Join"><a href="#9-3-1-小表、大表Join" class="headerlink" title="9.3.1 小表、大表Join"></a>9.3.1 小表、大表Join</h5><p>​        将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p>​        <font color="red">实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</font></p><p><strong>案例实操</strong></p><ol><li><p>需求</p><p>测试大表join小表和小表join大表的效率</p></li><li><p>建大表、小表和JOIN后表的语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建大表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建小表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建join后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>分别向大表和小表中导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/bigtable&#x27; into table bigtable;</span><br><span class="line">hive (default)&gt;load data local inpath &#x27;/opt/module/datas/smalltable&#x27; into table smalltable;</span><br></pre></td></tr></table></figure></li><li><p>关闭mapjoin功能（默认是打开的）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">false</span>;</span><br></pre></td></tr></table></figure></li><li><p>执行小表JOIN大表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable  b</span><br><span class="line"><span class="keyword">on</span> b.id = s.id;</span><br></pre></td></tr></table></figure><p>Time taken: 35.921 seconds</p><p>No rows affected (44.456 seconds)</p></li><li><p>执行大表JOIN小表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> smalltable  s</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 34.196 seconds</p><p>No rows affected (26.287 seconds)</p></li></ol><h5 id="9-3-2-大表join大表"><a href="#9-3-2-大表join大表" class="headerlink" title="9.3.2 大表join大表"></a>9.3.2 大表join大表</h5><ol><li><p>空key过滤</p><p>​        有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p><p><strong>案例实操</strong></p><p>（1）配置历史服务器</p><p>配置mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>查看jobhistroy</p><p><a href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p>（2）创建原始数据表、空id表、合并后数据表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建原始表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建空id表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> nullidtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建join后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）分别加载原始数据和空id数据到对应表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/ori&#x27; into table ori;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/nullid&#x27; into table nullidtable;</span><br></pre></td></tr></table></figure><p>（4）测试不过滤空id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table jointable select n.* from nullidtable n</span><br><span class="line">left join ori o on n.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 42.038 seconds</p><p>Time taken: 37.284 seconds</p><p>（5）测试过滤空id</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table jointable select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 31.725 seconds</p><p>Time taken: 28.876 seconds</p></li><li><p>空key转换</p><p>​        有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p><p><strong>案例实操</strong>：</p><p>不随机分布空null值：</p><p>（1）设置5个reduce个数</p><p>​            set mapreduce.job.reduces = 5;</p><p>（2）join两张表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> ori b <span class="keyword">on</span> n.id = b.id;</span><br></pre></td></tr></table></figure><p><strong>结果：可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</strong></p><p><img src="https://i.loli.net/2020/10/27/pd6UjBXHtNvPcem.png"></p><p>（1）设置分布空null值</p><p>​        set mapreduce.job.reduces = 5;</p><p>（2）JOIN两张表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">&#x27;hive&#x27;</span>, <span class="keyword">rand</span>()) <span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span><br></pre></td></tr></table></figure><p><strong>结果：如图6-14所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</strong></p><p><img src="https://i.loli.net/2020/10/27/OlcrFoHfz28nY5m.png"></p></li></ol><h5 id="9-3-3-MapJoin-小表join大表"><a href="#9-3-3-MapJoin-小表join大表" class="headerlink" title="9.3.3 MapJoin(小表join大表)"></a>9.3.3 MapJoin(小表join大表)</h5><p>​        如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p><ol><li><p>开启MapJoin参数设置</p><p>（1）设置自动选择mapjoin</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; <span class="comment">/*默认为true*/</span></span><br></pre></td></tr></table></figure><p>（2）大表小表的阈值设置（默认25M一下认为是小表）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>MapJoin工作机制，如下图</p><p><img src="https://i.loli.net/2020/10/27/wNUjZQpnEmACX26.png"></p></li></ol><p><strong>案例实操</strong></p><p>​    （1）开启Mapjoin功能</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; 默认为true</span><br></pre></td></tr></table></figure><p>​    （2）执行小表join大表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">join</span> bigtable  b</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 24.594 seconds</p><p>​    （3）执行大表join小表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line"><span class="keyword">join</span> smalltable  s</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 24.315 seconds</p><h5 id="9-3-4-Group-By"><a href="#9-3-4-Group-By" class="headerlink" title="9.3.4 Group By"></a>9.3.4 Group By</h5><p><font color="red">默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了</font>。</p><p><img src="https://i.loli.net/2020/10/27/S6WBaLj18eMoAtw.png"></p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><p><strong>开启Map端聚合参数设置</strong></p><ol><li><p>是否在Map端进行聚合，默认为true</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>在Map端进行聚合操作的条目数目</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval = <span class="number">100000</span></span><br></pre></td></tr></table></figure></li><li><p>有数据倾斜的时候进行负载均衡（默认是false）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno from emp group by deptno;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 23.68 sec   HDFS Read: 19987 HDFS Write: 9 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 23 seconds 680 msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line">10</span><br><span class="line">20</span><br><span class="line">30</span><br></pre></td></tr></table></figure><p>优化之后</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.groupby.skewindata = true;</span><br><span class="line">hive (default)&gt; select deptno from emp group by deptno;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 28.53 sec   HDFS Read: 18209 HDFS Write: 534 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 1  Reduce: 5   Cumulative CPU: 38.32 sec   HDFS Read: 15014 HDFS Write: 9 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line">10</span><br><span class="line">20</span><br><span class="line">30</span><br></pre></td></tr></table></figure></li></ol><h5 id="9-3-5-Count-distinct-去重统计—-会内存溢出"><a href="#9-3-5-Count-distinct-去重统计—-会内存溢出" class="headerlink" title="9.3.5 Count(distinct) 去重统计—-会内存溢出"></a>9.3.5 Count(distinct) 去重统计—-会内存溢出</h5><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT的全聚合操作，即使设定了reduce task个数，set mapred.reduce.tasks=100；hive也只会启动一个reducer。，这就造成一个Reduce处理的数据量太大，导致整个Job很难完成，<font color="red">一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</font></p><p><strong>案例实操</strong></p><ol><li><p>创建一张大表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table bigtable(id bigint, time bigint, uid string, keyword</span><br><span class="line">string, url_rank int, click_num int, click_url string) row format delimited</span><br><span class="line">fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>加载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/bigtable&#x27; into table</span><br><span class="line"> bigtable;</span><br></pre></td></tr></table></figure></li><li><p>设置5个reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces = <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li><li><p>执行去重id查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(distinct id) from bigtable;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.12 sec   HDFS Read: 120741990 HDFS Write: 7 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 7 seconds 120 msec</span><br><span class="line">OK</span><br><span class="line">c0</span><br><span class="line">100001</span><br><span class="line">Time taken: 23.607 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li><li><p>采用group by去重id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(id) from (select id from bigtable group by id) a;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 17.53 sec   HDFS Read: 120752703 HDFS Write: 580 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.29 sec   HDFS Read: 9409 HDFS Write: 7 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 21 seconds 820 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">100001</span><br><span class="line">Time taken: 50.795 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p></li></ol><h5 id="9-3-6-笛卡尔积"><a href="#9-3-6-笛卡尔积" class="headerlink" title="9.3.6 笛卡尔积"></a>9.3.6 笛卡尔积</h5><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h5 id="9-3-7-行列过滤"><a href="#9-3-7-行列过滤" class="headerlink" title="9.3.7 行列过滤"></a>9.3.7 行列过滤</h5><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p><p><strong>案例实操：</strong></p><ol><li><p>测试先关联两张表，再用where条件过滤</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select o.id from bigtable b</span><br><span class="line">join ori o on o.id = b.id</span><br><span class="line">where o.id &lt;= 10;</span><br></pre></td></tr></table></figure><p>Time taken: 34.406 seconds, Fetched: 100 row(s)</p></li><li><p>通过子查询后，再关联表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select b.id from bigtable b</span><br><span class="line">join (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> ori <span class="keyword">where</span> <span class="keyword">id</span> &lt;= <span class="number">10</span> ) o <span class="keyword">on</span> b.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 30.058 seconds, Fetched: 100 row(s)</p></li></ol><h5 id="9-3-8-动态分区调整"><a href="#9-3-8-动态分区调整" class="headerlink" title="9.3.8 动态分区调整"></a>9.3.8 动态分区调整</h5><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p><ol><li><p>开启动态分区参数设置</p><p>(1)开启动态分区功能（默认true，开启）</p><p><font color="red">hive.exec.dynamic.partition=true</font></p><p>(2)设置为非严格模式（动态分区的模式，默认strict,表示必须指定一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区）</p><p>注：在严格模式下，插入数据必须指定一个分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode&#x3D;nonstrict</span><br></pre></td></tr></table></figure><p>(3)在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure><p>(4)在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。与上面哪个设置成一样大。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode&#x3D;100</span><br></pre></td></tr></table></figure><p>(5)整个MR job中，最大可以创建多少个HDFS文件，默认100000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files&#x3D;100000</span><br></pre></td></tr></table></figure><p>(6)当有空分区生成时，是否抛出异常。一般不需要设置。默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition&#x3D;false</span><br></pre></td></tr></table></figure></li><li><p><strong>案例实操</strong></p><p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p><p>(1)创建目标分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(id int, name string) partitioned</span><br><span class="line">by (location int) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>(2)设置动态分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">hive (default)&gt; insert into table dept_partition partition(location) select deptno, dname, loc from dept; /*最后一个字段loc来的*/</span><br></pre></td></tr></table></figure><p>(3)查看目标分区表的分区情况</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure><p><font color="red">思考：目标分区表是如何匹配到分区字段的？</font></p></li></ol><h5 id="9-3-9-分桶"><a href="#9-3-9-分桶" class="headerlink" title="9.3.9 分桶"></a>9.3.9 分桶</h5><p>详见6.6章。</p><h5 id="9-3-10-分区"><a href="#9-3-10-分区" class="headerlink" title="9.3.10 分区"></a>9.3.10 分区</h5><p>详见4.6章。</p><h4 id="9-4-MR优化"><a href="#9-4-MR优化" class="headerlink" title="9.4 MR优化"></a>9.4 MR优化</h4><h5 id="9-4-1-合理设置Map数"><a href="#9-4-1-合理设置Map数" class="headerlink" title="9.4.1 合理设置Map数"></a>9.4.1 合理设置Map数</h5><ol><li><p>通常情况下，作业会通过input的目录产生一个或者多个map任务</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p></li><li><p>是不是map数越多越好</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p></li><li><p>是不是保证每个map处理接近128m的文件块，就高枕无忧了?</p><p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p></li></ol><h5 id="9-4-1-复杂文件增加map数"><a href="#9-4-1-复杂文件增加map数" class="headerlink" title="9.4.1 复杂文件增加map数"></a>9.4.1 复杂文件增加map数</h5><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p><strong>案例实操</strong></p><ol><li><p>执行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from emp;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br></pre></td></tr></table></figure></li><li><p>设置最大切片值为100个字节</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;</span><br><span class="line">hive (default)&gt; select count(*) from emp;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</span><br></pre></td></tr></table></figure></li></ol><h5 id="9-4-2-小文件进行合并"><a href="#9-4-2-小文件进行合并" class="headerlink" title="9.4.2 小文件进行合并"></a>9.4.2 小文件进行合并</h5><p>（1）在map执行前合并小文件，减少map数：  CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><p>（2）在Map-Reduce的任务结束时合并小文件的设置：</p><p>在map-only任务结束时合并小文件，默认true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.mapfiles &#x3D; true;</span><br></pre></td></tr></table></figure><p>在map-reduce任务结束时合并小文件，默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.mapredfiles &#x3D; true;</span><br></pre></td></tr></table></figure><p>合并文件的大小，默认256M</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.size.per.task &#x3D; 268435456;</span><br></pre></td></tr></table></figure><p>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.smallfiles.avgsize &#x3D; 16777216;</span><br></pre></td></tr></table></figure><h5 id="9-4-3-合理设置Reduce数"><a href="#9-4-3-合理设置Reduce数" class="headerlink" title="9.4.3 合理设置Reduce数"></a>9.4.3 合理设置Reduce数</h5><p>1．调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer&#x3D;256000000</span><br></pre></td></tr></table></figure><p>（2）每个任务最大的reduce数，默认为1009</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.max&#x3D;1009</span><br></pre></td></tr></table></figure><p>（3）计算reducer数的公式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N&#x3D;min(参数2，总输入数据量&#x2F;参数1)</span><br></pre></td></tr></table></figure><p>2．调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces &#x3D; 15;</span><br></pre></td></tr></table></figure><p>3．reduce个数并不是越多越好</p><p>1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：<font color="red">处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</font></p><h4 id="9-5-并行执行"><a href="#9-5-并行执行" class="headerlink" title="9.5 并行执行"></a>9.5 并行执行</h4><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;              //打开任务并行执行</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>;  //同一个sql允许最大并行度，默认为8。</span><br></pre></td></tr></table></figure><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h4 id="9-6-严格模式"><a href="#9-6-严格模式" class="headerlink" title="9.6 严格模式"></a>9.6 严格模式</h4><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p><p>通过设置属性hive.mapred.mode值为默认是非严格模式<font color="red">nonstrict</font> 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The mode in which the Hive operations are being performed. </span><br><span class="line">      In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">        Cartesian Product.</span><br><span class="line">        No partition being picked up for a query.</span><br><span class="line">        Comparing bigints and strings.</span><br><span class="line">        Comparing bigints and doubles.</span><br><span class="line">        Orderby without limit.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li><p>对于分区表，<font color="red">除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。</font>换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p></li><li><p>对于<font color="red">使用了order by语句的查询，要求必须使用limit语句。</font>因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p></li><li><p><font color="red">限制笛卡尔积的查询</font>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p></li></ol><h4 id="9-7-JVM重用"><a href="#9-7-JVM重用" class="headerlink" title="9.7 JVM重用"></a>9.7 JVM重用</h4><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;mapreduce.job.jvm.numtasks&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;10&lt;&#x2F;value&gt;</span><br><span class="line"> &lt;description&gt;How many tasks to run per jvm. If set to -1, there is</span><br><span class="line"> no limit. </span><br><span class="line"> &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h4 id="9-8-推测执行"><a href="#9-8-推测执行" class="headerlink" title="9.8 推测执行"></a>9.8 推测执行</h4><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置，默认是true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>不过hive本身也提供了配置项来控制reduce-side的推测执行：默认是true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h4 id="9-9-压缩"><a href="#9-9-压缩" class="headerlink" title="9.9 压缩"></a>9.9 压缩</h4><p>详见第8章</p><h4 id="9-10-执行计划（Explain）"><a href="#9-10-执行计划（Explain）" class="headerlink" title="9.10 执行计划（Explain）"></a>9.10 执行计划（Explain）</h4><p>1．基本语法</p><p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p><p>2．案例实操</p><p>（1）查看下面这条语句的执行计划</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; explain select * from emp;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure><p>（2）查看详细执行计划</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; explain extended select * from emp;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure><h3 id="第十章-Hive实战之谷粒影音"><a href="#第十章-Hive实战之谷粒影音" class="headerlink" title="第十章 Hive实战之谷粒影音"></a>第十章 Hive实战之谷粒影音</h3><h4 id="10-1-需求描述"><a href="#10-1-需求描述" class="headerlink" title="10.1 需求描述"></a>10.1 需求描述</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><p>–统计视频观看数Top10</p><p>–统计视频类别热度Top10</p><p>–统计视频观看数Top20所属类别以及类别包含的Top20的视频个数</p><p>–统计视频观看数Top50所关联视频的所属类别Rank</p><p>–统计每个类别中的视频热度Top10</p><p>–统计每个类别中视频流量Top10</p><p>–统计上传视频最多的用户Top10以及他们上传的观看次数前20视频</p><p>–统计每个类别视频观看数Top10</p><h4 id="10-2-项目"><a href="#10-2-项目" class="headerlink" title="10.2 项目"></a>10.2 项目</h4><h5 id="10-2-1-数据结构"><a href="#10-2-1-数据结构" class="headerlink" title="10.2.1 数据结构"></a>10.2.1 数据结构</h5><ol><li><p>视频表</p><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>video id</td><td>视频唯一id</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别</td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分</td><td>满分5分</td></tr><tr><td>Ratings</td><td>流量</td><td>视频的流量，整型数字</td></tr><tr><td>conments</td><td>评论数</td><td>一个视频的整数评论数</td></tr><tr><td>related ids</td><td>相关视频id</td><td>相关视频的id，最多20个</td></tr></tbody></table></li><li><p>用户表</p><table><thead><tr><th>字段</th><th>备注</th><th>字段类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table></li></ol><h5 id="10-2-2-ETL原始数据"><a href="#10-2-2-ETL原始数据" class="headerlink" title="10.2.2 ETL原始数据"></a>10.2.2 ETL原始数据</h5>]]></content>
    
    
    <summary type="html">&lt;p&gt;第七章 函数 / 第八章 压缩和存储 &lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive入门</title>
    <link href="http://luo6656.github.io/2020/07/01/BigDataFrame/Hive%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/07/01/BigDataFrame/Hive%E5%85%A5%E9%97%A8/</id>
    <published>2020-06-30T16:00:00.000Z</published>
    <updated>2020-10-27T05:11:12.942Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 / 第二章 Hive安装 / 第三章 Hive数据类型 /<br>第四章 DDL数据定义 / 第五章 DML数据操作 / 第六章 查询</p><a id="more"></a><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="第一章-Hive基本概念"><a href="#第一章-Hive基本概念" class="headerlink" title="第一章 Hive基本概念"></a>第一章 Hive基本概念</h3><h4 id="1-1-什么是Hive-仅仅是一个客户端"><a href="#1-1-什么是Hive-仅仅是一个客户端" class="headerlink" title="1.1 什么是Hive(仅仅是一个客户端)"></a>1.1 什么是Hive(仅仅是一个客户端)</h4><p>​        Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p><p>​        Hive是基于Hadoop的一个数据仓库工具（本身不存储数据），可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p><p>​        <font color="red">本质是：将HQL转化为MapReduce程序</font></p><p><img src="https://i.loli.net/2020/10/27/6FuaSCvWrAP3fkV.png" alt="img"></p><ul><li>​        Hive处理的数据存储在HDFS</li><li>​        Hive分析数据底层的<font color="red">默认实现是MapReducer</font>，也可使用     Spark来作为底层实现。</li><li>​        执行程序运行在Yarn上</li></ul><h4 id="1-2-Hive的优缺点"><a href="#1-2-Hive的优缺点" class="headerlink" title="1.2 Hive的优缺点"></a>1.2 Hive的优缺点</h4><h5 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h5><ul><li>操作接口采用类SQL语法，提供快速开发的能力（简答、容易上手）。</li><li>避免了去写MapReduce，减少了开发人员的学习成本。</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li></ul><h5 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h5><ol><li>Hive的HQL表达能力有限<ul><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长，由于MapReducer数据处理流程的限制，效率更高的算法却无法实现。</li></ul></li><li>Hive的效率比较低<ul><li>Hive自动生成的MapReduce作业，通常情况下不够智能化。</li><li>Hive调优比较困难，粒度较粗</li></ul></li></ol><h4 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h4><p><img src="https://i.loli.net/2020/10/27/paMvQozwPeyS6Vf.png" alt="img"></p><ol><li><p>用户接口：Client</p><p>CLI(command-line interface)、JDBC/ODBC(jdbc访问hive)、WEBUI(浏览器访问hive)</p></li><li><p>元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在的目录等；</p><p><font color="red">默认存储在自带的derby数据库中，推荐使用MySql存储Metastore</font></p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReducer进行计算。</p></li><li><p>驱动器：Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><p><img src="https://i.loli.net/2020/10/27/pESwWXNvCPoy6gk.png" alt="img"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p></li></ol><h4 id="1-4-Hive和数据库比较"><a href="#1-4-Hive和数据库比较" class="headerlink" title="1.4 Hive和数据库比较"></a>1.4 Hive和数据库比较</h4><p>​        由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h5 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h5><p>​        由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h5 id="1-4-2-数据存储位置"><a href="#1-4-2-数据存储位置" class="headerlink" title="1.4.2 数据存储位置"></a>1.4.2 数据存储位置</h5><p>​        Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p><h5 id="1-4-3-数据更新"><a href="#1-4-3-数据更新" class="headerlink" title="1.4.3 数据更新"></a>1.4.3 数据更新</h5><p>​        由于Hive是针对数据仓库应用设计的，而<font color="red">数据仓库的内容是读多写少的。</font>因此，<font color="red">Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</font>而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h5 id="1-4-4-执行"><a href="#1-4-4-执行" class="headerlink" title="1.4.4 执行"></a>1.4.4 执行</h5><p>​        Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。</p><h5 id="1-4-5-执行延迟"><a href="#1-4-5-执行延迟" class="headerlink" title="1.4.5 执行延迟"></a>1.4.5 执行延迟</h5><p>​        Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h5 id="1-4-6-可扩展性"><a href="#1-4-6-可扩展性" class="headerlink" title="1.4.6 可扩展性"></a>1.4.6 可扩展性</h5><p>​        由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle在理论上的扩展能力也只有100台左右。</p><h5 id="1-4-7-数据规模"><a href="#1-4-7-数据规模" class="headerlink" title="1.4.7 数据规模"></a>1.4.7 数据规模</h5><p>​        由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h3 id="第二章-Hive安装"><a href="#第二章-Hive安装" class="headerlink" title="第二章 Hive安装"></a>第二章 Hive安装</h3><h4 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h4><ul><li>Hive官网地址：<a href="http://hive.apache.org/">http://hive.apache.org/</a></li><li>文档查看地址：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li><li>下载地址：<a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></li><li>github地址：<a href="https://github.com/apache/hive">https://github.com/apache/hive</a></li></ul><h4 id="2-2-Hive安装部署"><a href="#2-2-Hive安装部署" class="headerlink" title="2.2 Hive安装部署"></a>2.2 Hive安装部署</h4><h5 id="2-2-1-Hive安装及配置"><a href="#2-2-1-Hive安装及配置" class="headerlink" title="2.2.1 Hive安装及配置"></a>2.2.1 Hive安装及配置</h5><ol><li><p>把apche-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure></li><li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure></li><li><p>配置hive-env.sh文件</p><ol><li><p>配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure></li><li><p>配置HIVE_CONF_DIR路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure></li></ol></li></ol><h5 id="2-2-2-Hadoop集群配置"><a href="#2-2-2-Hadoop集群配置" class="headerlink" title="2.2.2 Hadoop集群配置"></a>2.2.2 Hadoop集群配置</h5><ol><li><p>必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-2-3-Hive基本操作"><a href="#2-2-3-Hive基本操作" class="headerlink" title="2.2.3 Hive基本操作"></a>2.2.3 Hive基本操作</h5><ol><li><p>启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>打开默认数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use default;</span></span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>创建一张表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table student(id int, name string);</span></span><br></pre></td></tr></table></figure></li><li><p>显示数据库中有几张表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>查看表的结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc student;</span></span><br></pre></td></tr></table></figure></li><li><p>向表中插入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert into student values(1000,<span class="string">&quot;ss&quot;</span>);</span></span><br></pre></td></tr></table></figure></li><li><p>查询表中数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from student;</span></span><br></pre></td></tr></table></figure></li><li><p>退出hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> quit;</span></span><br></pre></td></tr></table></figure></li></ol><p>说明：（查看hive在hdfs中的结构）</p><p><font color="red">数据库</font>：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹</p><p> <font color="red"> 表</font>：在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据。</p><h4 id="2-3-将本地文件导入Hive案例"><a href="#2-3-将本地文件导入Hive案例" class="headerlink" title="2.3 将本地文件导入Hive案例"></a>2.3 将本地文件导入Hive案例</h4><h5 id="2-3-1-需求"><a href="#2-3-1-需求" class="headerlink" title="2.3.1 需求"></a>2.3.1 需求</h5><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p><h5 id="2-3-2-数据准备"><a href="#2-3-2-数据准备" class="headerlink" title="2.3.2 数据准备"></a>2.3.2 数据准备</h5><p>在/opt/module/datas这个目录下创建datas</p><ol><li><p>在/opt/module/目录下创建datas</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure></li><li><p>在/opt/module/datas/目录下创建student.txt文件并添加数据</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch student.txt</span><br><span class="line">[atguigu@hadoop102 datas]$ vi student.txt</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意以tab键间隔</font>。</p><h5 id="2-3-3-Hive实际操作"><a href="#2-3-3-Hive实际操作" class="headerlink" title="2.3.3 Hive实际操作"></a>2.3.3 Hive实际操作</h5><ol><li><p>启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>显示数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>使用default数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use default;</span></span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>删除已创建的student表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop table student;</span></span><br></pre></td></tr></table></figure></li><li><p>创建student表，并声明文件分隔符’\t’</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED</span></span><br><span class="line"> BY &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>加载/opt/module/datas/student.txt文件到student数据库表中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> into table student;</span></span><br></pre></td></tr></table></figure></li><li><p>Hive查询结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from student;</span></span><br><span class="line">OK</span><br><span class="line">1001zhangshan</span><br><span class="line">1002lishi</span><br><span class="line">1003zhaoliu</span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-3-4-遇到的问题"><a href="#2-3-4-遇到的问题" class="headerlink" title="2.3.4 遇到的问题"></a>2.3.4 遇到的问题</h5><p><font color="red">再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常</font></p><p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p><p><font color="red">为什么表在/user/hive/warehouse里呢：因为/user/hive/warehouse是创建的default数据库的路径。</font></p><h4 id="2-4-MySql安装"><a href="#2-4-MySql安装" class="headerlink" title="2.4 MySql安装"></a>2.4 MySql安装</h4><h5 id="2-4-1-安装包准备"><a href="#2-4-1-安装包准备" class="headerlink" title="2.4.1 安装包准备"></a>2.4.1 安装包准备</h5><ol><li><p>查看mysql是否安装，如果安装了，卸载mysql</p><ol><li><p>查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -qa|grep mysql</span><br><span class="line"></span><br><span class="line">mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure></li><li><p>卸载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure></li></ol></li><li><p>解压mysql-libs.zip文件到当前目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# unzip mysql-libs.zip</span><br><span class="line">[root@hadoop102 software]# ls</span><br><span class="line">mysql-libs.zip</span><br><span class="line">mysql-libs</span><br></pre></td></tr></table></figure></li><li><p>进入到mysql-libs文件夹下</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# ll</span><br><span class="line"></span><br><span class="line">总用量 76048</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><h5 id="2-4-2-安装MySql服务器"><a href="#2-4-2-安装MySql服务器" class="headerlink" title="2.4.2 安装MySql服务器"></a>2.4.2 安装MySql服务器</h5><ol><li><p>安装mysql服务端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>查看产生的随机密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# cat /root/.mysql_secret</span><br><span class="line"></span><br><span class="line">OEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure></li><li><p>查看mysql状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql status</span><br></pre></td></tr></table></figure></li><li><p>启动mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql start</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-4-3-安装Mysql客户端"><a href="#2-4-3-安装Mysql客户端" class="headerlink" title="2.4.3 安装Mysql客户端"></a>2.4.3 安装Mysql客户端</h5><ol><li><p>安装mysql客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>链接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure></li><li><p>修改密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">SET PASSWORD=PASSWORD(<span class="string">&#x27;000000&#x27;</span>);</span></span><br></pre></td></tr></table></figure></li><li><p>退出mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"><span class="built_in">exit</span></span></span><br></pre></td></tr></table></figure></li></ol><h5 id="2-4-4-MySql中user表中主机配置"><a href="#2-4-4-MySql中user表中主机配置" class="headerlink" title="2.4.4 MySql中user表中主机配置"></a>2.4.4 MySql中user表中主机配置</h5><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。</p><ol><li><p>进入mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -p123456789</span><br></pre></td></tr></table></figure></li><li><p>显示数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>使用mysql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br></pre></td></tr></table></figure></li><li><p>展示mysql数据库中的所有表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show tables;</span><br></pre></td></tr></table></figure></li><li><p>展示user表的结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;desc user;</span><br></pre></td></tr></table></figure></li><li><p>查询user表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;select User, Host, Password from user;</span><br></pre></td></tr></table></figure></li><li><p>修改user表，把host表内容修改为%</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;update user set host&#x3D;&#39;%&#39; where host&#x3D;&#39;localhost&#39;;</span><br></pre></td></tr></table></figure></li><li><p>删除root用户的其他host</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;hadoop102&#39;;</span><br><span class="line"></span><br><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;127.0.0.1&#39;;</span><br><span class="line"></span><br><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;::1&#39;;</span><br></pre></td></tr></table></figure></li><li><p>刷新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure></li><li><p>退出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;quit;</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-5-Hive元数据配置到Mysql"><a href="#2-5-Hive元数据配置到Mysql" class="headerlink" title="2.5 Hive元数据配置到Mysql"></a>2.5 Hive元数据配置到Mysql</h4><h5 id="2-5-1-驱动拷贝"><a href="#2-5-1-驱动拷贝" class="headerlink" title="2.5.1 驱动拷贝"></a>2.5.1 驱动拷贝</h5><ol><li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line"></span><br><span class="line"> /opt/module/hive/lib/</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-5-2-配置Metastore到Mysql"><a href="#2-5-2-配置Metastore到Mysql" class="headerlink" title="2.5.2 配置Metastore到Mysql"></a>2.5.2 配置Metastore到Mysql</h5><ol><li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动Hadoop集群）</p></li></ol><h5 id="2-5-3-多窗口启动Hive测试"><a href="#2-5-3-多窗口启动Hive测试" class="headerlink" title="2.5.3 多窗口启动Hive测试"></a>2.5.3 多窗口启动Hive测试</h5><ol><li><p>先启动MySql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 mysql-libs]$ mysql -uroot -p123456789</span><br></pre></td></tr></table></figure></li><li><p>再次打开多个窗口，分别启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>启动hive后，回到MySql窗口查看数据库，显示<font color="red">增加了metastore数据库</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| metastore          |</span><br><span class="line">| mysql             |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test               |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-6-HiveJDBC访问"><a href="#2-6-HiveJDBC访问" class="headerlink" title="2.6 HiveJDBC访问"></a>2.6 HiveJDBC访问</h4><h5 id="2-6-1-启动hiveserver2服务"><a href="#2-6-1-启动hiveserver2服务" class="headerlink" title="2.6.1 启动hiveserver2服务"></a>2.6.1 启动hiveserver2服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure><h5 id="2-6-2-启动beeline"><a href="#2-6-2-启动beeline" class="headerlink" title="2.6.2 启动beeline"></a>2.6.2 启动beeline</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2-6-3-连接hiveserver2"><a href="#2-6-3-连接hiveserver2" class="headerlink" title="2.6.3 连接hiveserver2"></a>2.6.3 连接hiveserver2</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop102:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://hadoop102:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车）</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop102:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| hive_db2       |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure><h4 id="2-7-Hive常用交互命令"><a href="#2-7-Hive常用交互命令" class="headerlink" title="2.7 Hive常用交互命令"></a>2.7 Hive常用交互命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--help                        Print help information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure><ol><li><p>“e” 不进入hive的交互窗口执行sql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>“f” 执行脚本中的sql语句</p><ol><li><p>在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>文件中写入正确的sql语句</p><p>select * from student;</p></li><li><p>执行文件中的sql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句并将结果写入文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="2-8-Hive其他命令操作"><a href="#2-8-Hive其他命令操作" class="headerlink" title="2.8 Hive其他命令操作"></a>2.8 Hive其他命令操作</h4><ol><li><p>退出hive窗口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls &#x2F;;</span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure></li><li><p>查看在hive中输入的所有历史命令</p><ol><li>进入到当前用户的根目录/root或/home/atguigu</li><li>查看.hivehistory文件</li></ol></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure><h4 id="2-9-Hive常见配置属性"><a href="#2-9-Hive常见配置属性" class="headerlink" title="2.9 Hive常见配置属性"></a>2.9 Hive常见配置属性</h4><h5 id="2-9-1-Hive数据仓库位置的配置"><a href="#2-9-1-Hive数据仓库位置的配置" class="headerlink" title="2.9.1 Hive数据仓库位置的配置"></a>2.9.1 Hive数据仓库位置的配置</h5><ol><li><p>Default数据仓库的最原始位置是在hdfs上的:/user/hive/warehouse路径下。</p></li><li><p><font color="red">在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹</font></p></li><li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-9-2-查询后信息显示的配置"><a href="#2-9-2-查询后信息显示的配置" class="headerlink" title="2.9.2 查询后信息显示的配置"></a>2.9.2 查询后信息显示的配置</h5><ol><li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>重新启动hive，对比配置前后差异。</p><ol><li><p>配置前，如图所示</p><p><img src="https://i.loli.net/2020/10/27/wZdMS2s5pQ4ulaR.png"></p></li><li><p>配置后，如图所示</p></li></ol></li></ol><p><img src="https://i.loli.net/2020/10/27/1c4uR3ZUpnmDJfz.png"></p><h5 id="2-9-3-Hive运行日志信息的配置"><a href="#2-9-3-Hive运行日志信息的配置" class="headerlink" title="2.9.3 Hive运行日志信息的配置"></a>2.9.3 Hive运行日志信息的配置</h5><ol><li><p>Hive的log默认存放在/tmp/atguigu/hive.log目录下（当前用户名下）</p></li><li><p>修改hive的log存放日志到/opt/module/hive/logs</p><ol><li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure></li><li><p>在hive-log4j.properties文件中修改log存放位置</p></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-9-4-参数配置方式"><a href="#2-9-4-参数配置方式" class="headerlink" title="2.9.4 参数配置方式"></a>2.9.4 参数配置方式</h5><ol><li><p>查看当前所有的配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure></li><li><p>参数的配置三种方式</p><p>（1）配置文件方式</p><p>​        默认配置文件：hive-default.xml</p><p>​        用户自定义配置文件：hive-site.xlm</p><p><font color="red">注意</font>：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p><p>（2）命令行参数方式</p><p>​        启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p><p>​        例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意：仅对本次hive启动有效</font></p><p>​        查看参数设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>（3）参数声明方式</p><p>​        可以在HQL中使用SET关键字设定参数</p><p>​        例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks&#x3D;100;</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意：仅对本次hive启动有效</font></p><p>​        查看参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>​        上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p></li></ol><h3 id="第三章-Hive数据类型"><a href="#第三章-Hive数据类型" class="headerlink" title="第三章 Hive数据类型"></a>第三章 Hive数据类型</h3><h4 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h4><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE  FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>​        对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h4 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h4><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()例如struct&lt;street:string, city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()例如array<string></td></tr></tbody></table><p>​        Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>​        <strong>案例实操</strong></p><p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问格式为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据</p><p>​    创建本地测试文件test.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line"></span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>​        <font color="red">注意</font>：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p><p>3）Hive上创建测试表test</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table test(</span><br><span class="line">name string,</span><br><span class="line">friends array&lt;string&gt;,</span><br><span class="line">children map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;street:string, city:string&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;</span><br><span class="line">collection items terminated by &#39;_&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><p>字段解释：</p><p>row format delimited fields terminated by ‘,’  – 列分隔符</p><p>collection items terminated by ‘_’  –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’                – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’;                    – 行分隔符</p><p>4）导入文本数据到测试表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘&#x2F;opt&#x2F;module&#x2F;datas&#x2F;test.txt’into table test</span><br></pre></td></tr></table></figure><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from test</span><br><span class="line">where name&#x3D;&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><h4 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h4><p>​        Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><h5 id="3-3-1-隐式类型转换规则如下"><a href="#3-3-1-隐式类型转换规则如下" class="headerlink" title="3.3.1 隐式类型转换规则如下"></a>3.3.1 隐式类型转换规则如下</h5><ol><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li><li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li><li>TINYINT、SMALLINT、INT都可以转换成FLOAT</li><li>BOOLEAN类型不可以转换为任何其他地类型</li></ol><h5 id="3-3-2-可以使用CAST操作显示进行数据类型转换"><a href="#3-3-2-可以使用CAST操作显示进行数据类型转换" class="headerlink" title="3.3.2 可以使用CAST操作显示进行数据类型转换"></a>3.3.2 可以使用CAST操作显示进行数据类型转换</h5><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt; select &#39;1&#39;+2, cast(&#39;1&#39;as int) + 2;</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br><span class="line"></span><br><span class="line">| _c0  | _c1  |</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br><span class="line"></span><br><span class="line">| 3.0  | 3   |</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br></pre></td></tr></table></figure><h3 id="第四章-DDL数据定义"><a href="#第四章-DDL数据定义" class="headerlink" title="第四章 DDL数据定义"></a>第四章 DDL数据定义</h3><p><font color="red">DDL都是操作的元数据</font></p><h4 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><ol><li><p>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure></li><li><p>避免要创建的数据库已经存在的错误，增加if not exists判断(标准写法)</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure></li><li><p>创建一个数据库，指定数据库在HDFS上存放的位置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h4><h5 id="4-2-1-显示数据库"><a href="#4-2-1-显示数据库" class="headerlink" title="4.2.1 显示数据库"></a>4.2.1 显示数据库</h5><ol><li><p>显示数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>过滤显示查询数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like &#39;db_hive*&#39;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h5><ol><li><p>显示数据库信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbatguiguUSER</span><br></pre></td></tr></table></figure></li><li><p>显示数据库详细信息，extended</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbatguiguUSER</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-2-3-切换当前数据库"><a href="#4-2-3-切换当前数据库" class="headerlink" title="4.2.3 切换当前数据库"></a>4.2.3 切换当前数据库</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h4 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h4><p>​        用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。<font color="red">数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</font></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line"></span><br><span class="line">db_name <span class="keyword">comment</span> location     owner_name    owner_type    <span class="keyword">parameters</span></span><br><span class="line"></span><br><span class="line">db_hive     hdfs://hadoop102:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db   atguigu <span class="keyword">USER</span>   &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h4><ol><li><p>删除空数据库</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果删除的数据库不存在，最好采用if exists判断数据库是否存在</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line"></span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line"></span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果数据库不为空，可以采用cascade命令，强制删除</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h4><h5 id="4-5-1-建表语句以及建表参数"><a href="#4-5-1-建表语句以及建表参数" class="headerlink" title="4.5.1 建表语句以及建表参数"></a>4.5.1 建表语句以及建表参数</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">/*注释*/</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]  <span class="comment">/*分区，大部分都是分区表，分的是文件夹*/</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">/*分桶，分的是文件*/</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure><p><strong>字段解释说明</strong> </p><p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p><p>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），<font color="red">在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</font></p><p>（3）COMMENT：为表和列添加注释。</p><p>（4）PARTITIONED BY创建分区表</p><p>（5）CLUSTERED BY创建分桶表</p><p>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</p><p>（7）ROW FORMAT </p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p><p>​    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] </p><p>  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，<font color="red">Hive通过SerDe确定表的具体的列的数据。</font></p><p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p><p>（8）STORED AS指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p><p><font color="red">（9）LOCATION ：指定表在HDFS上的存储位置。</font></p><p><font color="red">（10）AS：后跟查询语句</font>，根据查询结果创建表。</p><p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h5 id="4-5-2-管理表（内部表）"><a href="#4-5-2-管理表（内部表）" class="headerlink" title="4.5.2 管理表（内部表）"></a>4.5.2 管理表（内部表）</h5><ol><li><p>理论</p><p>默认创建的表都是所谓的管理表，有时也被称为<font color="red">内部表</font>。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。    <font color="red">当我们删除一个管理表时，Hive也会删除这个表中数据</font>。管理表不适合和其他工具共享数据。</p></li><li><p>案例操作</p><ol><li><p>普通创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student2&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据查询结果创建表（查询的结果会添加到新创建的表中)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>根据已经存在的表结构创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE  </span><br></pre></td></tr></table></figure></li></ol></li></ol><h5 id="4-5-3-外部表"><a href="#4-5-3-外部表" class="headerlink" title="4.5.3 外部表"></a>4.5.3 外部表</h5><ol><li><p>理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<font color="red">删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</font></p></li><li><p>管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p></li><li><p>案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据</p><ol><li><p>上传数据到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>建表语句（创建外部表）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table stu_external(</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27; </span><br><span class="line">location &#x27;/student&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>查看创建的表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br><span class="line">1003    zhaoliu</span><br></pre></td></tr></table></figure></li><li><p>查看表格式化数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</font></p><p>​    </p></li></ol><h5 id="4-5-4-管理表与外部表的互相转换（注意大小写）"><a href="#4-5-4-管理表与外部表的互相转换（注意大小写）" class="headerlink" title="4.5.4 管理表与外部表的互相转换（注意大小写）"></a>4.5.4 管理表与外部表的互相转换（注意大小写）</h5><ol><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li><li><p>查询内部表student2为外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span>=<span class="string">&#x27;TRUE&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改外部表student2为内部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span>=<span class="string">&#x27;FALSE&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p><font color="red">注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</font></p></li></ol><h4 id="4-6-分区表"><a href="#4-6-分区表" class="headerlink" title="4.6 分区表"></a>4.6 分区表</h4><p>​        分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<font color="red">Hive中的分区就是分目录</font>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h5 id="4-6-1-分区表基本操作"><a href="#4-6-1-分区表基本操作" class="headerlink" title="4.6.1 分区表基本操作"></a>4.6.1 分区表基本操作</h5><ol><li><p>引入分区表（需要根据日期对日志进行管理）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170702&#x2F;20170702.log</span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170703&#x2F;20170703.log</span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170704&#x2F;20170704.log</span><br></pre></td></tr></table></figure></li><li><p>创建分区表语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p></li><li><p>加载数据到分区表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201709&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201708&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201707’);</span><br></pre></td></tr></table></figure><p>注意：分区表加载数据时，必须指定分区</p><p><img src="https://i.loli.net/2020/10/27/W7BVeQ6LygNRiKH.png"></p><p>​                                    加载数据到分区表</p><p><img src="https://i.loli.net/2020/10/27/hmvedFiYBXGtyZn.png"></p><p>​                                           分区表</p></li></ol><ol start="4"><li><p>查询分区表中数据</p><p>（1）单分区查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>（2）多分区联合查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;</span><br><span class="line">              union</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201708&#x27;</span></span><br><span class="line">              <span class="keyword">union</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201707&#x27;</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure></li><li><p>增加分区</p><p>（1）创建单个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201706&#x27;) ;</span><br></pre></td></tr></table></figure><p>（2）同时创建多个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201705&#x27;) partition(month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure></li><li><p>删除分区</p><p>（1）删除单个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>（2）同时删除多个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201705&#x27;), partition (month=&#x27;201706&#x27;);</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>查看分区表有多少分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>查看分区表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string    </span><br></pre></td></tr></table></figure></li></ol><h5 id="4-6-2-分区表注意事项"><a href="#4-6-2-分区表注意事项" class="headerlink" title="4.6.2 分区表注意事项"></a>4.6.2 分区表注意事项</h5><ol><li><p>创建二级分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, dname string, loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>正常的加载数据</p><p>（1）加载数据到二级分区表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> default.dept_partition2 partition(month=&#x27;201709&#x27;, day=&#x27;13&#x27;);</span><br></pre></td></tr></table></figure><p>（2）查询分区数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><p>（1）方式一：上传数据后修复</p><p>​        a.上传数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><p>​        b.查询数据（不能查到，元数据里没有）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>​        c.执行修复命令</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>​        d.再次查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>（2）方式二：上传数据后添加分区</p><p>​        a.上传数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>​        b.执行添加分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month=&#x27;201709&#x27;,</span><br><span class="line"> day=&#x27;11&#x27;);</span><br></pre></td></tr></table></figure><p>​        c.查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;</span><br></pre></td></tr></table></figure><p>（3）方式三：创建文件夹后load数据到分区</p><p>​        a.创建目录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>​        b.上传数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);</span><br></pre></td></tr></table></figure><p>​        c.查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-7-修改表"><a href="#4-7-修改表" class="headerlink" title="4.7 修改表"></a>4.7 修改表</h4><h5 id="4-7-1-重命名表"><a href="#4-7-1-重命名表" class="headerlink" title="4.7.1 重命名表"></a>4.7.1 重命名表</h5><ol><li><p>语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li><li><p>实操案例</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-7-2-增加、修改和删除分区表"><a href="#4-7-2-增加、修改和删除分区表" class="headerlink" title="4.7.2 增加、修改和删除分区表"></a>4.7.2 增加、修改和删除分区表</h5><p><font color="red">详见4.6.1分区表基本操作</font></p><h5 id="4-7-3-增加-修改-替换列信息"><a href="#4-7-3-增加-修改-替换列信息" class="headerlink" title="4.7.3 增加/修改/替换列信息"></a>4.7.3 增加/修改/替换列信息</h5><ol><li><p>语法</p><ol><li><p>更新列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure></li><li><p>增加和替换列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...) </span><br></pre></td></tr></table></figure><p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，<font color="red">REPLACE则是表示替换表中所有字段。</font></p></li></ol></li><li><p>实操案例</p><p>（1）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（2）添加列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure><p>（3）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（4）更新列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure><p>（5）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（6）替换列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure><p>（7）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-8-删除表"><a href="#4-8-删除表" class="headerlink" title="4.8 删除表"></a>4.8 删除表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h3 id="第五章-DML数据操作"><a href="#第五章-DML数据操作" class="headerlink" title="第五章 DML数据操作"></a>第五章 DML数据操作</h3><h4 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h4><h5 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h5><ol><li><p>语法</p><p>hive&gt; load data [local] inpath ‘/opt/module/datas/student.txt’ [overwrite] into table student [partition (partcol1=val1,…)];</p><p>（1）load data:表示加载数据</p><p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p>（3）inpath:表示加载数据的路径</p><p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p><p>（5）into table:表示加载到哪张表</p><p>（6）student:表示具体的表</p><p>（7）partition:表示上传到指定分区</p></li></ol><ol start="2"><li><p>实操案例</p><p>（1）创建一张表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（2）加载本地文件到hive</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（3）加载HDFS文件到hive中</p><p>​        a.上传文件到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;</span><br></pre></td></tr></table></figure><p>​        b.加载HDFS上数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（4）加载数据覆盖表中已有的数据</p><p>​        a.上传文件到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;</span><br></pre></td></tr></table></figure><p>​        b.加载数据覆盖表中已有的数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h5><p>（1）创建一张分区表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（2）基本插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month=&#x27;201709&#x27;) values(1,&#x27;wangwu&#x27;),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure><p>（3）基本模式插入（根据单张表查询结果）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month=&#x27;201708&#x27;)</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure><p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p><p>insert overwrite：会覆盖表或分区中已存在的数据</p><p>（4）多表（多分区）插入模式（根据多张表查询结果）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">&#x27;201707&#x27;</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">&#x27;201706&#x27;</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="5-1-3-查询语句中创建表并加载数据（as-select）"><a href="#5-1-3-查询语句中创建表并加载数据（as-select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（as select）"></a>5.1.3 查询语句中创建表并加载数据（as select）</h5><p>​    <font color="red">详见4.5.1 章创建表</font></p><p>​    根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h5 id="5-1-4-创建表时通过location指定加载数据路径"><a href="#5-1-4-创建表时通过location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过location指定加载数据路径"></a>5.1.4 创建表时通过location指定加载数据路径</h5><ol><li><p>上传数据到hdfs上</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>创建表，并指定载hdfs上的位置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">              location &#x27;/student;</span><br></pre></td></tr></table></figure></li><li><p>查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-1-5-import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）"><a href="#5-1-5-import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）" class="headerlink" title="5.1.5 import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）"></a>5.1.5 import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）</h5><p>​        <font color="red">注意：先用export导出后，再将数据导入</font></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month=&#x27;201709&#x27;) from</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><h4 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h4><h5 id="5-2-1-insert导出"><a href="#5-2-1-insert导出" class="headerlink" title="5.2.1 insert导出"></a>5.2.1 insert导出</h5><ol><li><p>将查询的结果导出到本地</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;</span><br><span class="line">            <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果格式化导出到本地</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;</span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果导出到HDFS上（没有local）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/atguigu/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-2-2-Hadoop命令导出到本地"><a href="#5-2-2-Hadoop命令导出到本地" class="headerlink" title="5.2.2 Hadoop命令导出到本地"></a>5.2.2 Hadoop命令导出到本地</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h5 id="5-2-3-Hive-Shell命令导出"><a href="#5-2-3-Hive-Shell命令导出" class="headerlink" title="5.2.3 Hive Shell命令导出"></a>5.2.3 Hive Shell命令导出</h5><p>​        基本语法：（hive -f/-e  执行语句或者脚本 &gt;file）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h5 id="5-2-4-export导出到HDFS上"><a href="#5-2-4-export导出到HDFS上" class="headerlink" title="5.2.4 export导出到HDFS上"></a>5.2.4 export导出到HDFS上</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><p>​        export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h5 id="5-2-5-sqoop导出"><a href="#5-2-5-sqoop导出" class="headerlink" title="5.2.5 sqoop导出"></a>5.2.5 sqoop导出</h5><p>​        后续课程专门讲</p><h4 id="5-3-清除表中数据（truncate"><a href="#5-3-清除表中数据（truncate" class="headerlink" title="5.3 清除表中数据（truncate)"></a>5.3 清除表中数据（truncate)</h4><p>​        <font color="red">注意：truncate只能删除管理表，不能删除外部表中数据</font></p><h3 id="第六章-查询"><a href="#第六章-查询" class="headerlink" title="第六章 查询"></a>第六章 查询</h3><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><p>查询顺序：from–&gt; join on–&gt; where–&gt; group by–&gt; select|having –&gt; order by–&gt; limit</p><p>查询语句语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*]    (Note: <span class="keyword">Only</span> available</span><br><span class="line"> <span class="keyword">starting</span> <span class="keyword">with</span> Hive <span class="number">0.13</span><span class="number">.0</span>)</span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">    | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><h4 id="6-1-基本查询"><a href="#6-1-基本查询" class="headerlink" title="6.1 基本查询"></a>6.1 基本查询</h4><h5 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h5><p>​    创建部门表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>​    创建员工表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>​    导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line">dept;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table emp;</span><br></pre></td></tr></table></figure><ol><li><p>全表查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br></pre></td></tr></table></figure></li><li><p>选择特定列查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, ename from emp;</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意</font>：（1）SQL语言大小写不敏感</p><p>​            （2）SQL可以写在一行或者多行</p><p>​            （3）关键字不能被缩写也不能分行</p><p>​            （4）各子句一般要分行写</p><p>​            （5）使用缩进提高语句的可读性</p><h5 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h5><ol><li><p>重命名一个列</p></li><li><p>便于计算</p></li><li><p>紧跟列名，<font color="red">也可以在列名和别名之间加入关键字 as</font></p></li><li><p>案例实操</p><p>查询名称和部门</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h5><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B 相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B 相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A|B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table><p>​    案例实操</p><p>​        查询出所有员工的薪水后加1显示</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sal +1 from emp;</span><br></pre></td></tr></table></figure><h5 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h5><ol><li><p>求总行数（count）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的最大值（max）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的最小值（min）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的总和（sum）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure></li><li><p>求工资的平均值（avg）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-1-5-limit语句"><a href="#6-1-5-limit语句" class="headerlink" title="6.1.5 limit语句"></a>6.1.5 limit语句</h5><p>​        典型的查询会返回多行数据。limit子句用于限制返回的行数。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br></pre></td></tr></table></figure><h4 id="6-2-where语句"><a href="#6-2-where语句" class="headerlink" title="6.2 where语句"></a>6.2 where语句</h4><ol><li><p>使用where子句，将不满足条件的行过滤掉</p></li><li><p>where子句紧跟from子句</p></li><li><p>案例实操</p><p>查询出薪水大于1000的所有员工</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000;</span><br></pre></td></tr></table></figure><p><font color="red">注意：where子句中不能使用字段别名</font></p></li></ol><h5 id="6-2-1-比较运算符（between-in-is-null）"><a href="#6-2-1-比较运算符（between-in-is-null）" class="headerlink" title="6.2.1 比较运算符（between/in/is null）"></a>6.2.1 比较运算符（between/in/is null）</h5><p>(1) 下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中.</p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><p>(2) <strong>案例实操</strong></p><ol><li><p>查询出薪水等于5000的所有员工</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal=<span class="number">5000</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询工资在500到1000的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">between</span> <span class="number">500</span> <span class="keyword">and</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询comm为空的所有员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询工资是1500或5000的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">in</span> (<span class="number">1500</span>,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-2-2-like和rlike"><a href="#6-2-2-like和rlike" class="headerlink" title="6.2.2 like和rlike"></a>6.2.2 like和rlike</h5><ol><li><p>使用like运算选择类似的值</p></li><li><p>选择条件可以包含字符或数字</p></li><li><p>rlike子句是Hive中这个功能的一个扩展,其可以通过<font color="red">Java的正则表达式</font>这个更强大的语言来指定匹配条件</p></li><li><p><strong>案例实操</strong></p><p>(1) 查找以2开头薪水的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">like</span> <span class="string">&#x27;2%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>(2) 查找第二个数值为2的薪水的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">like</span> <span class="string">&#x27;_2%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>(3) 查找薪水中含有2的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">rlike</span> <span class="string">&#x27;[2]&#x27;</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-2-3-逻辑运算符（and-or-not）"><a href="#6-2-3-逻辑运算符（and-or-not）" class="headerlink" title="6.2.3 逻辑运算符（and/or/not）"></a>6.2.3 逻辑运算符（and/or/not）</h5><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><p>​    <strong>案例实操</strong></p><ol><li><p>查询薪水大于1000，部门是30</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">and</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询薪水大于1000，或者部门是304</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">or</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询除了20部门和30部门以外的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">in</span>(<span class="number">30</span>,<span class="number">200</span>);</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-3-分组"><a href="#6-3-分组" class="headerlink" title="6.3 分组"></a>6.3 分组</h4><h5 id="6-3-1-group-by语句"><a href="#6-3-1-group-by语句" class="headerlink" title="6.3.1 group by语句"></a>6.3.1 group by语句</h5><p>​        group by语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p><p>​    <strong>案例实操</strong></p><ol><li><p>计算emp表每个部门的平局工资</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno,<span class="keyword">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure></li><li><p>计算emp每个部门中每个岗位的最高薪水</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno,t.job,<span class="keyword">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno,t.job;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-3-2-Having语句"><a href="#6-3-2-Having语句" class="headerlink" title="6.3.2 Having语句"></a>6.3.2 Having语句</h5><ol><li><p>having与where不同点</p><p>（1）where后面不能写分组函数，而having后面可以使用分组函数。</p><p>（2）having只用于group by分组统计语句</p></li><li><p><strong>案例实操</strong></p><p>求每个部门的平均薪水大于2000的部门</p><p>（1）求每个部门的平均工资</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> deptno,<span class="keyword">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p>（2）求每个部门的平均薪水大于2000的部门</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> deptno,<span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal&gt;<span class="number">2000</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-4-join语句"><a href="#6-4-join语句" class="headerlink" title="6.4 join语句"></a>6.4 join语句</h4><h5 id="6-4-1-等值join"><a href="#6-4-1-等值join" class="headerlink" title="6.4.1 等值join"></a>6.4.1 等值join</h5><p>​    Hive支持通常的SQL join语句，但是<font color="red">只支持等值连接，不支持非等值连接</font></p><p>​    <strong>案例实操</strong></p><p>​    根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-2-表的别名"><a href="#6-4-2-表的别名" class="headerlink" title="6.4.2 表的别名"></a>6.4.2 表的别名</h5><ol><li><p>好处</p><p>（1）使用别名可以简化查询</p><p>（2）使用表名前缀可以提高执行效率</p></li><li><p>案例实操</p><p>合并员工表和部门表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-4-3-内连接"><a href="#6-4-3-内连接" class="headerlink" title="6.4.3 内连接"></a>6.4.3 内连接</h5><p>​    <font color="red">内连接</font>：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-4-左外连接"><a href="#6-4-4-左外连接" class="headerlink" title="6.4.4 左外连接"></a>6.4.4 左外连接</h5><p>​    <font color="red">左外连接</font>：join操作符左边表中符合where子句的所有记录将会被返回</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-5-右外连接"><a href="#6-4-5-右外连接" class="headerlink" title="6.4.5 右外连接"></a>6.4.5 右外连接</h5><p>​    <font color="red">右外连接</font>：join操作符右边表中符合where子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-6-满外连接"><a href="#6-4-6-满外连接" class="headerlink" title="6.4.6 满外连接"></a>6.4.6 满外连接</h5><p>​    <font color="red">满外连接</font>：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值得话，那么就使用null值替代。</p><h5 id="6-4-7-多表连接"><a href="#6-4-7-多表连接" class="headerlink" title="6.4.7 多表连接"></a>6.4.7 多表连接</h5><p>​    <font color="red">注意：连接n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</font></p><p>​    <strong>案例实操</strong></p><ol><li><p>​    创建位置表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> location(</span><br><span class="line">loc <span class="built_in">int</span>,</span><br><span class="line">loc_name <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>​    导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span><br></pre></td></tr></table></figure></li><li><p>​    多表连接查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename, d.dname, l.loc_name <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> d.deptno = e.deptno <span class="keyword">join</span> location l <span class="keyword">on</span> d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>​        大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p><p>​        注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</p><p>​        <font color="red">优化：当对3个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。</font></p></li></ol><h5 id="6-4-8-笛卡尔积（少用）"><a href="#6-4-8-笛卡尔积（少用）" class="headerlink" title="6.4.8 笛卡尔积（少用）"></a>6.4.8 笛卡尔积（少用）</h5><ol><li><p>笛卡尔积会在下面条件下产生</p><p>（1）省略连接条件</p><p>（2）连接条件无效</p><p>（3）所有表中得所有行互相连接</p></li><li><p>案例实操</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> empno,dname <span class="keyword">from</span> emp,dept;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-4-9-连接谓词中不支持or"><a href="#6-4-9-连接谓词中不支持or" class="headerlink" title="6.4.9 连接谓词中不支持or"></a>6.4.9 连接谓词中不支持or</h5><p>​        hive join目前不支持在on子句中使用谓词or</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno <span class="keyword">or</span> e.ename=d.ename;   错误的</span><br></pre></td></tr></table></figure><h4 id="6-5-排序"><a href="#6-5-排序" class="headerlink" title="6.5 排序"></a>6.5 排序</h4><h5 id="6-5-1-全局排序"><a href="#6-5-1-全局排序" class="headerlink" title="6.5.1 全局排序"></a>6.5.1 全局排序</h5><p><font color="red">order by：全局排序，只有一个reducer</font></p><ol><li><p>使用order by子句排序</p><p><font color="red">（1）asc（ascend）：升序（默认）</font></p><p><font color="red">（2）desc（descend）：降序</font></p></li><li><p>order by子句在select语句的结尾</p></li><li><p><strong>案例实操</strong></p><p>（1）查询员工信息按工资升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span><br></pre></td></tr></table></figure><p>（2）查询员工信息按工资降序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-5-2-按照别名排序"><a href="#6-5-2-按照别名排序" class="headerlink" title="6.5.2 按照别名排序"></a>6.5.2 按照别名排序</h5><p><strong>案例实操</strong></p><p>按照员工薪水的2倍排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename, sal*<span class="number">2</span> doublesal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> doublesal;</span><br></pre></td></tr></table></figure><h5 id="6-5-3-多个列排序"><a href="#6-5-3-多个列排序" class="headerlink" title="6.5.3 多个列排序"></a>6.5.3 多个列排序</h5><p><strong>案例实操</strong></p><p>按照部门和工资升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno,dal;</span><br></pre></td></tr></table></figure><h5 id="6-5-4-每个MapReduce内部排序（sort-by）"><a href="#6-5-4-每个MapReduce内部排序（sort-by）" class="headerlink" title="6.5.4 每个MapReduce内部排序（sort by）"></a>6.5.4 每个MapReduce内部排序（sort by）</h5><p>​        Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p><p>​        Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure></li><li><p>查看设置reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure></li><li><p>根据部门编号降序查看员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件夹中（按照部门编号降序排序）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">&#x27;/opt/module/datas/sortby-result&#x27;</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-5-5-分区排序（distribute-by）"><a href="#6-5-5-分区排序（distribute-by）" class="headerlink" title="6.5.5 分区排序（distribute by）"></a>6.5.5 分区排序（distribute by）</h5><p>​        Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。*<strong>*distribute by**</strong> 子句可以做这件事。*<strong>*distribute by**</strong>类似MR中partition（自定义分区），进行分区，结合sort by使用。 </p><p>​        对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p><strong>案例实操</strong></p><p>先按照部门编号分区，再按照员工编号降序排序</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>distrubute by 的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li><li><font color="red">Hive要求distribute by语句要写在sort by语句之前</font></li></ol><h5 id="6-5-6-cluster-by"><a href="#6-5-6-cluster-by" class="headerlink" title="6.5.6 cluster by"></a>6.5.6 cluster by</h5><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>1）以下两种写法等价</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p><h5 id="6-5-6-几个排序函数的区别（重要）"><a href="#6-5-6-几个排序函数的区别（重要）" class="headerlink" title="6.5.6 几个排序函数的区别（重要）"></a>6.5.6 几个排序函数的区别（重要）</h5><ol><li>order by：全局排序，reduce个数为1，设置多个也没用</li><li>sort by：区内排序，通常结合distribute by使用，reduce多个，会产生数据倾斜</li><li>cluster by：当分区字段和排序字段相同的时候，可以使用cluster by代替。</li></ol><h4 id="6-6-分桶及抽样查询"><a href="#6-6-分桶及抽样查询" class="headerlink" title="6.6 分桶及抽样查询"></a>6.6 分桶及抽样查询</h4><h5 id="6-6-1-分桶表数据存储"><a href="#6-6-1-分桶表数据存储" class="headerlink" title="6.6.1 分桶表数据存储"></a>6.6.1 分桶表数据存储</h5><p>​        1. 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p><p>​        2. 分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p><p>​        3. <font color="red">分区针对的是数据的存储路径；分桶针对的是数据文件。</font></p><p><strong>案例实操</strong></p><ol><li><p>先创建分桶表，通过直接导入数据文件的方式</p><p>（1）数据准备</p><p>​        准备student.txt文件</p><p>（2）创建分桶表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）查看表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted stu_buck;</span><br><span class="line">Num Buckets:4</span><br></pre></td></tr></table></figure><p>（4）导入数据到分桶表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br></pre></td></tr></table></figure><p>（5）查看创建的分桶表中是否分成4个桶，如下图</p><p>​        发现并未分桶，那是什么原因呢（load相当于是put，所以不会分桶）</p></li></ol><ol start="2"><li><p>创建分桶表时，数据通过子查询的方式导入</p><p>（1）先创建一个普通的stu表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（2）先普通的stu表中导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><p>（3）清空stu_buck表中数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck;</span><br></pre></td></tr></table></figure><p>（4）导入数据到分桶表，通过子查询的方式（走MR）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>（5）发现还是只有一个分桶</p><p>（6）需要设置一个属性</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>; <span class="comment">/*开启分桶功能*/</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">-1</span>; <span class="comment">/*-1表示reduce会根据桶的个数分配reduce个数*/</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/jurRfc7bzdG6IiP.png"></p><p>（7）查询分桶的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_buck;</span><br><span class="line">OK</span><br><span class="line">stu_buck.id     stu_buck.name</span><br><span class="line">1004    ss4</span><br><span class="line">1008    ss8</span><br><span class="line">1012    ss12</span><br><span class="line">1016    ss16</span><br><span class="line">1001    ss1</span><br><span class="line">1005    ss5</span><br><span class="line">1009    ss9</span><br><span class="line">1013    ss13</span><br><span class="line">1002    ss2</span><br><span class="line">1006    ss6</span><br><span class="line">1010    ss10</span><br><span class="line">1014    ss14</span><br><span class="line">1003    ss3</span><br><span class="line">1007    ss7</span><br><span class="line">1011    ss11</span><br><span class="line">1015    ss15</span><br></pre></td></tr></table></figure><p><font color="red">分桶规则：</font></p><p>根据结果可知：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p></li></ol><h5 id="6-6-2-分桶抽样查询"><a href="#6-6-2-分桶抽样查询" class="headerlink" title="6.6.2 分桶抽样查询"></a>6.6.2 分桶抽样查询</h5><p>​        对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p><p>​        查询表stu_buck中的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p>​        注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>​        y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>​        <font color="red">x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。</font>例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p>​        <font color="red">注意：x的值必须小于等于y的值，否则</font></p><p>​        FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p><h4 id="6-7-其他常用查询函数"><a href="#6-7-其他常用查询函数" class="headerlink" title="6.7 其他常用查询函数"></a>6.7 其他常用查询函数</h4><h5 id="6-7-1-空字段赋值"><a href="#6-7-1-空字段赋值" class="headerlink" title="6.7.1 空字段赋值"></a>6.7.1 空字段赋值</h5><ol><li><p>函数说明</p><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。</p></li><li><p>数据准备：采用员工表</p></li><li><p>查询：如果员工的comm为NULL，则用-1代替</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive(default) &gt; select comm,nvl(comm, -1) from emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    -1.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br></pre></td></tr></table></figure></li><li><p>查询：如果员工的comm为NULL，则用领导id代替</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> comm, nvl(comm, mgr) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    7902.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    NULL</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    7788.0</span><br><span class="line">NULL    7698.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    7782.0</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-2-case-when"><a href="#6-7-2-case-when" class="headerlink" title="6.7.2 case when"></a>6.7.2 case when</h5><ol><li><p>数据准备</p><table><thead><tr><th>name</th><th>dept_id</th><th>sex</th></tr></thead><tbody><tr><td>悟空</td><td>A</td><td>男</td></tr><tr><td>大海</td><td>A</td><td>男</td></tr><tr><td>宋宋</td><td>B</td><td>男</td></tr><tr><td>凤姐</td><td>A</td><td>女</td></tr><tr><td>婷姐</td><td>B</td><td>女</td></tr><tr><td>婷婷</td><td>B</td><td>女</td></tr></tbody></table></li><li><p>需求</p><p>求出不同部门男女各多少人，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A     2       1</span><br><span class="line">B     1       2</span><br></pre></td></tr></table></figure></li><li><p>创建本地emp_sex.txt，导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi emp_sex.txt</span><br><span class="line">悟空A男</span><br><span class="line">大海A男</span><br><span class="line">宋宋B男</span><br><span class="line">凤姐A女</span><br><span class="line">婷姐B女</span><br><span class="line">婷婷B女</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">dept_id <span class="keyword">string</span>, </span><br><span class="line">sex <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp_sex.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> dept_id,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;女&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span> emp_sex <span class="keyword">group</span> <span class="keyword">by</span> dept_id;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-3-行转列（对多个列进行合并）"><a href="#6-7-3-行转列（对多个列进行合并）" class="headerlink" title="6.7.3 行转列（对多个列进行合并）"></a>6.7.3 行转列（对多个列进行合并）</h5><ol><li><p>相关函数说明</p><p>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p></li><li><p>数据准备</p><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table></li><li><p>需求</p><p>把星座和血型一样的人归类到一起。结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure></li><li><p>创建本地constellation.txt，导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi constellation.txt</span><br><span class="line">孙悟空白羊座A</span><br><span class="line">大海     射手座A</span><br><span class="line">宋宋     白羊座B</span><br><span class="line">猪八戒    白羊座A</span><br><span class="line">凤姐     射手座A</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">constellation <span class="keyword">string</span>, </span><br><span class="line">blood_type <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/constellation.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">&#x27;|&#x27;</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">name</span>,</span><br><span class="line">        <span class="keyword">concat</span>(constellation, <span class="string">&quot;,&quot;</span>, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-4-列转行（把列拆分）"><a href="#6-7-4-列转行（把列拆分）" class="headerlink" title="6.7.4 列转行（把列拆分）"></a>6.7.4 列转行（把列拆分）</h5><ol><li><p>函数说明</p><p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p></li><li><p>数据准备</p><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table></li><li><p>需求</p><p>将电影分类中的数组数据展开，结果如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure></li><li><p>创建本地movie.txt，导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi movie.txt</span><br><span class="line">《疑犯追踪》悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》战争,动作,灾难</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">    movie <span class="keyword">string</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;,&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/movie.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-5-窗口函数（开窗函数）"><a href="#6-7-5-窗口函数（开窗函数）" class="headerlink" title="6.7.5 窗口函数（开窗函数）"></a>6.7.5 窗口函数（开窗函数）</h5><ol><li><p>相关函数说明</p><p>（1）OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。</p><p>（2）CURRENT ROW：当前行</p><p>（3）n PRECEDING：往前n行数据</p><p>（4）n FOLLOWING：往后n行数据</p><p>（5）UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>（6）LAG(col,n,default_val)：往前第n行数据</p><p>（7）LEAD(col,n, default_val)：往后第n行数据</p><p>（8）NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<font color="red">注意：n必须为int类型。</font></p></li><li><p>数据准备：name，orderdate，cost</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure></li><li><p>需求</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><p>（2）查询顾客的购买明细及月购买总额</p><p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p><p>（4）查询每个顾客上次的购买时间</p><p>（5）查询前20%时间的订单信息</p></li><li><p>创建本地business.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi business.txt</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">orderdate <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">cost</span> <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/business.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">count</span>(*) <span class="keyword">over</span> () </span><br><span class="line"><span class="keyword">from</span> business </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">&#x27;2017-04&#x27;</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>（2）查询顾客的购买明细及月购买总额</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,<span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) <span class="keyword">from</span></span><br><span class="line"> business;</span><br></pre></td></tr></table></figure><p>（3）上述的场景，将每个顾客的cost按照日期进行累加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行 </span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>rows必须跟在order by子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量。</p><p>（4）查看顾客上次的购买时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line">lag(orderdate,<span class="number">1</span>,<span class="string">&#x27;1900-01-01&#x27;</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1, lag(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2 </span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>（5）查询前20%时间的订单信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> sorted = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-6-rank"><a href="#6-7-6-rank" class="headerlink" title="6.7.6 rank"></a>6.7.6 rank</h5><ol><li><p>函数说明</p><p>（1）RANK() 排序相同时会重复，总数不会变</p><p>（2）DENSE_RANK() 排序相同时会重复，总数会减少</p><p>（3）ROW_NUMBER() 会根据顺序计算</p></li><li><p>数据准备</p><table><thead><tr><th>name</th><th>subject</th><th>score</th></tr></thead><tbody><tr><td>孙悟空</td><td>语文</td><td>87</td></tr><tr><td>孙悟空</td><td>数学</td><td>95</td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td></tr><tr><td>大海</td><td>语文</td><td>94</td></tr><tr><td>大海</td><td>数学</td><td>56</td></tr><tr><td>大海</td><td>英语</td><td>84</td></tr><tr><td>宋宋</td><td>语文</td><td>64</td></tr><tr><td>宋宋</td><td>数学</td><td>86</td></tr><tr><td>宋宋</td><td>英语</td><td>84</td></tr><tr><td>婷婷</td><td>语文</td><td>65</td></tr><tr><td>婷婷</td><td>数学</td><td>85</td></tr><tr><td>婷婷</td><td>英语</td><td>78</td></tr></tbody></table></li><li><p>需求</p><p>计算每门学科成绩排名</p></li><li><p>创建本地score.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi score.txt</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subject <span class="keyword">string</span>, </span><br><span class="line">score <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/score.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,</span><br><span class="line">subject,</span><br><span class="line">score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rp,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) drp,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rmp</span><br><span class="line"><span class="keyword">from</span> score;</span><br><span class="line"></span><br><span class="line">name    subject score   rp      drp     rmp</span><br><span class="line">孙悟空  数学    95      1       1       1</span><br><span class="line">宋宋    数学    86      2       2       2</span><br><span class="line">婷婷    数学    85      3       3       3</span><br><span class="line">大海    数学    56      4       4       4</span><br><span class="line">宋宋    英语    84      1       1       1</span><br><span class="line">大海    英语    84      1       1       2</span><br><span class="line">婷婷    英语    78      3       2       3</span><br><span class="line">孙悟空  英语    68      4       3       4</span><br><span class="line">大海    语文    94      1       1       1</span><br><span class="line">孙悟空  语文    87      2       2       2</span><br><span class="line">婷婷    语文    65      3       3       3</span><br><span class="line">宋宋    语文    64      4       4       4</span><br></pre></td></tr></table></figure><p><font color="red">扩展：求出每门学科前三名的学生</font></p></li></ol><h5 id="6-7-7-时间类"><a href="#6-7-7-时间类" class="headerlink" title="6.7.7 时间类"></a>6.7.7 时间类</h5><ol><li><p>date_format:格式化时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">&#x27;1987-5-23&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>date_add:时间跟天数相加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select date_add(&#x27;2019-06-05&#x27;,5);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">2019-06-10</span><br></pre></td></tr></table></figure></li><li><p>date_sub:时间跟天数相减</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select date_sub(&#x27;2019-06-05&#x27;,5);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">2019-05-31</span><br><span class="line">Time taken: 0.343 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li><li><p>datediff:两个时间相减</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select datediff(&#x27;2019-06-29&#x27;,&#x27;2019-07-05&#x27;);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">-6</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 / 第二章 Hive安装 / 第三章 Hive数据类型 /&lt;br&gt;第四章 DDL数据定义 / 第五章 DML数据操作 / 第六章 查询&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka入门</title>
    <link href="http://luo6656.github.io/2020/06/25/BigDataFrame/Kafka%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/06/25/BigDataFrame/Kafka%E5%85%A5%E9%97%A8/</id>
    <published>2020-06-24T16:00:00.000Z</published>
    <updated>2020-10-27T05:11:00.271Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 概述 / 第二章 快速入门 / 第三章 Kafka架构深入 / 第四章 Kafka API /</p><a id="more"></a><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h3 id="第一章-Kafka概述"><a href="#第一章-Kafka概述" class="headerlink" title="第一章 Kafka概述"></a>第一章 Kafka概述</h3><h4 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h4><p>​    Kafka是一个分布式的基于<font color="red">发布/订阅模式</font>的消息队列，主要应用于大数据实时处理领域。</p><h4 id="1-2-消息队列（Message-Queue）"><a href="#1-2-消息队列（Message-Queue）" class="headerlink" title="1.2 消息队列（Message Queue）"></a>1.2 消息队列（Message Queue）</h4><h5 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h5><p><img src="https://i.loli.net/2020/10/27/jGKtykBrihl8VCg.png"></p><p><img src="https://i.loli.net/2020/10/27/rYhFNLJfH8bjtD3.png"></p><p><strong>消息队列的优点</strong></p><p>（1）解耦</p><p>（2）可恢复性</p><p>（3）缓冲</p><p>（4）灵活性&amp;峰值处理能力</p><p>（5）异步通信</p><h5 id="1-2-2-消息队列的模式"><a href="#1-2-2-消息队列的模式" class="headerlink" title="1.2.2 消息队列的模式"></a>1.2.2 消息队列的模式</h5><ol><li><p>点对点模式（<font color="red">一对一</font>，消费者主动拉取数据，消息收到后消息清除）</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。</p><p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p><p><img src="https://i.loli.net/2020/10/27/wR8EXKGuqZWr9fV.png"></p></li></ol><ol start="2"><li><p>发布/订阅模式（<font color="red">一对多</font>，消费者消费数据之后不会清除消息）</p><p>消息生产者（发布）将消息发布到topic中，同时有多个消息消9费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。kafka是消费者主动拉取模式的，需要一直轮询，看topic中是否有新消息</p><p><img src="https://i.loli.net/2020/10/27/D7rPpGY1o9FjUQW.png"></p></li></ol><h4 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h4><p><img src="https://i.loli.net/2020/10/27/6nHYkWQar4Fx17q.png"></p><p><img src="https://i.loli.net/2020/10/27/CxTMcGgbJZQXtUe.png"></p><p>Producer：消息生产者，就是向kafka broker发消息的客户端；</p><p>Consumer：消息消费者，向kafka broker取出消息的客户端</p><p>Consumer Group（CG）：消费者组，由多个consumer组成。<font color="red">消费者组内每个消费者负责消费不同区分的数据，一个分区只能由一个组里面的一个消费者消费；消费者组之间互不影响。</font>所有的消费者都属于某个消费者组，即<font color="red">消费者组是逻辑上的一个订阅者</font> 。同一个消费者组里面的消费者不能同时消费相同的分区数据，其他的消费者组可以消费相同分区。消费者组就是为了提高消费能力。消费者组里的消费者个数大于分区数，回浪费资源。并发度最好就是消费者组的消费者个数=分区数。</p><p>Broker：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p><p>Topic：可以理解为一个队列，<font color="red">生产者和消费者面向的都是一个topic</font></p><p>Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，<font color="red">一个topic可以分为多个partition</font>，每个partition是一个有序的队列。</p><p>Replica：副本，为保证集群中的某个节点发生故障，<font color="red">该节点上的partition数据不丢失，且kafka仍能够继续工作，</font>kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p><p>Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p><p>Follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower（应该是leader吧）。</p><p>Zookeeper：管理Kafka集群，保存消费信息（offset偏移量）0.9之前。0.9版本之后为了效率就不保存在zk上了保存在磁盘上</p><h3 id="第二章-Kafka快速入门"><a href="#第二章-Kafka快速入门" class="headerlink" title="第二章 Kafka快速入门"></a>第二章 Kafka快速入门</h3><h4 id="2-1-安装部署"><a href="#2-1-安装部署" class="headerlink" title="2.1 安装部署"></a>2.1 安装部署</h4><h5 id="2-1-1-集群规划"><a href="#2-1-1-集群规划" class="headerlink" title="2.1.1 集群规划"></a>2.1.1 集群规划</h5><table><thead><tr><th>hadoop103</th><th>hadoop104</th><th>hadoop105</th></tr></thead><tbody><tr><td>zk</td><td>zk</td><td>zk</td></tr><tr><td>kafka</td><td>kafka</td><td>kafka</td></tr></tbody></table><h5 id="2-1-2-jar包下载"><a href="#2-1-2-jar包下载" class="headerlink" title="2.1.2 jar包下载"></a>2.1.2 jar包下载</h5><blockquote><p><a href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></p></blockquote><p><img src="https://i.loli.net/2020/10/27/JDKgMhUoRVQyNGp.png"></p><h5 id="2-1-3-集群部署"><a href="#2-1-3-集群部署" class="headerlink" title="2.1.3 集群部署"></a>2.1.3 集群部署</h5><ol><li><p>解压安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改解压后的文件名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv kafka_2.11-0.11.0.0/ kafka</span><br></pre></td></tr></table></figure></li><li><p>在/opt/module/kafka目录下创建logs文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure></li><li><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ cd config/</span><br><span class="line">[atguigu@hadoop102 config]$ vi server.properties</span><br></pre></td></tr></table></figure><p>输入以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta">#</span><span class="bash">设置可以删除topic</span></span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span><span class="bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta">#</span><span class="bash">用来处理磁盘IO的现成数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta">#</span><span class="bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta">#</span><span class="bash">kafka运行日志存放的路径,实际就是消息数据</span></span><br><span class="line">log.dirs=/opt/module/kafka/logs</span><br><span class="line"><span class="meta">#</span><span class="bash">topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta">#</span><span class="bash">用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta">#</span><span class="bash">segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta">#</span><span class="bash">配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=hadoop103:2181,hadoop104:2181,hadoop105:2181</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ sudo vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 module]$ source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>分发安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync kafka/</span><br></pre></td></tr></table></figure><p><font color="red">注意：分发之后记得配置其他机器的环境变量</font></p></li><li><p>分别在hadoop104和hadoop105上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p><p><font color="red">注意：broker.id不得重复</font></p></li><li><p>启动集群</p><p>依次在hadoop103、hadoop104、hadoop105节点上启动kafka。</p><p><font color="red">中间加一个-daemon使他成为守护进程，就不需要阻塞了</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon  config/server.properties</span><br><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon  config/server.properties</span><br></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>关闭集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-server-stop.sh stop</span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-server-stop.sh stop</span><br><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure></li></ol><ol start="10"><li><p>kafka群起脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for i in &#96;cat &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;slaves&#96;</span><br><span class="line">do</span><br><span class="line">echo &quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; $i &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot; </span><br><span class="line">ssh $i &#39;source &#x2F;etc&#x2F;profile&amp;&amp;&#x2F;opt&#x2F;module&#x2F;kafka_2.11-0.11.0.2&#x2F;bin&#x2F;kafka-server-start.sh -daemon &#x2F;opt&#x2F;module&#x2F;kafka_2.11-0.11.0.2&#x2F;config&#x2F;server.properties&#39;</span><br><span class="line">echo $?</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h4><ol><li><p>查看当前服务器中的所有topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--list</span><br></pre></td></tr></table></figure></li><li><p>创建topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--create </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--topic first</span><br><span class="line">--partitions 2</span><br><span class="line">--replication-factor 2</span><br></pre></td></tr></table></figure><p><font color="red">选项说明：</font></p><p>–topic 定义topic名</p><p>–replication-factor  定义副本数，<font color="red">副本为1那么就只有leader</font></p><p>–partitions  定义分区数</p><p><font color="red">注意：</font></p><p>副本数不能超过集群数，因为相同分区不能出现在同一个broker上。</p><p>分区数量可以超过集群，因为在一个broker上可以有多个分区。</p></li><li><p>删除topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--delete </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--topic first</span><br></pre></td></tr></table></figure><p>需要<font color="red">servier.properties中设置delete.topic.enable=true</font>否则只是标记删除。</p></li><li><p>查看某个Topic的详情</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh </span><br><span class="line">--describe </span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--topic first</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>发送消息，生产者不需要使用zk</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh</span><br><span class="line">--topic first</span><br><span class="line">--broker-list hadoop102:9092 </span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">atguigu  atguigu</span></span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>消费消息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh </span><br><span class="line">--bootstrap-server hadoop102:9092 </span><br><span class="line">--from-beginning </span><br><span class="line">--topic first</span><br><span class="line"><span class="meta">#</span><span class="bash"> 新版本，断点是存在kafka中的。offset</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 旧版本,断点是存在zookeeper中的，所以调用zookeeper</span></span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh</span><br><span class="line">--topic first</span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--from-beginning </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>–from-beginning：会把主题中以往所有的数据都读取出来</p></li></ol><ol start="7"><li><p>修改分区数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$bin/kafka-topics.sh </span><br><span class="line">--zookeeper hadoop102:2181 </span><br><span class="line">--alter </span><br><span class="line">--topic first </span><br><span class="line">--partitions 6</span><br></pre></td></tr></table></figure></li></ol><h3 id="第三章-Kafka架构深入"><a href="#第三章-Kafka架构深入" class="headerlink" title="第三章 Kafka架构深入"></a>第三章 Kafka架构深入</h3><h4 id="3-1-Kafka工作流程及文件存储机制"><a href="#3-1-Kafka工作流程及文件存储机制" class="headerlink" title="3.1 Kafka工作流程及文件存储机制"></a>3.1 Kafka工作流程及文件存储机制</h4><p><img src="https://i.loli.net/2020/10/27/e5PozwXQNnUsOKa.png"></p><p>Kafka中消息是以<font color="red">topic</font>进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p><p>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错回复，从上次的位置继续消费。</p><p><img src="https://i.loli.net/2020/10/27/L4kAVj9H5XuDstG.png"></p><p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了<strong>分片</strong>和<strong>索引</strong>机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure><p>index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。</p><p><img src="https://i.loli.net/2020/10/27/BN8QW9cGoCL4FOf.png"></p><p><font color="red">“.index”文件存储大量的索引信息，“.log”文件存储大量的数据</font>，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p><h4 id="3-2-Kafka生产者"><a href="#3-2-Kafka生产者" class="headerlink" title="3.2 Kafka生产者"></a>3.2 Kafka生产者</h4><h5 id="3-2-1-分区策略"><a href="#3-2-1-分区策略" class="headerlink" title="3.2.1 分区策略"></a>3.2.1 分区策略</h5><ol><li><p><strong>分区的原因</strong></p><p>（1）<font color="red">方便在集群中扩展</font>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p><p>（2）<font color="red">可以提高并发</font>，因为可以以Partition为单位读写了。</p></li></ol><ol start="2"><li><p><strong>分区的原则</strong></p><p>我们需要将producer发送的数据封装成一个<font color="red"><strong>ProducerRecord</strong></font>对象。</p><p><img src="https://i.loli.net/2020/10/27/O6aNlcfEwsuGFTA.png"></p><p>（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；</p><p>（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</p><p>（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin（轮询） 算法。</p></li></ol><h5 id="3-2-2-数据可靠性保证"><a href="#3-2-2-数据可靠性保证" class="headerlink" title="3.2.2 数据可靠性保证"></a>3.2.2 数据可靠性保证</h5><p><font color="red">为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</font></p><p><img src="https://i.loli.net/2020/10/27/gPXBCs2tM5KWqE8.png"></p><p><img src="https://i.loli.net/2020/10/27/yPjFMk2lRhT7aSu.png"></p><ol><li><p>副本数据同步策略</p><table><thead><tr><th>方案</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>半数以上完成同步，就发送ack</td><td>延迟低</td><td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr><tr><td>全部完成同步，才发送ack</td><td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td><td>延迟高</td></tr></tbody></table><p>Kafka选择了第二种方案，原因如下：</p><p>1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p><p>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p></li><li><p>ISR（同步队列，为了选出新的Leader）</p><p>采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p><p><font color="red">Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由*<strong>*replica.lag.time.max.ms**</strong>参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</font></p></li></ol><ol start="3"><li><p>ack应答机制</p><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p><p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p><p><strong><font color="red">acks参数配置：</font></strong></p><p>(1) 0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能<font color="red">丢失数据</font></p><p><img src="https://i.loli.net/2020/10/27/kMvyXUoQbd2ZAC9.png"></p><p><img src="https://i.loli.net/2020/10/27/2vFlIECj4wgM3Ok.png"></p><p>(2) 1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<font color="red">丢失数据</font></p><p><img src="https://i.loli.net/2020/10/27/kMvyXUoQbd2ZAC9.png"></p><p><img src="https://i.loli.net/2020/10/27/2vFlIECj4wgM3Ok.png"></p><p>(3) -1(all)：producer等待broker的ack，partition的leader和follower(ISR里面的)全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<font color="red">数据重复</font>。</p><p><img src="https://i.loli.net/2020/10/27/ja5weIKLB4RVsXk.png"></p><p><img src="https://i.loli.net/2020/10/27/jJhk89QpGm4q3A7.png"></p></li><li><p>故障处理细节</p><p><img src="https://i.loli.net/2020/10/27/T8uFe5sNpKrZMw7.png"></p><p>（1）follower故障</p><p>​        follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该，<font color="red">follower的LEO大于等于该Partition的HW</font>，即follower追上leader之后，就可以重新加入ISR了。</p><p>（2）leader故障</p><p>​        leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p><p><font color="red">注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</font></p></li></ol><h5 id="3-2-3-Exactly-Once语义（解决数据重复问题）"><a href="#3-2-3-Exactly-Once语义（解决数据重复问题）" class="headerlink" title="3.2.3 Exactly Once语义（解决数据重复问题）"></a>3.2.3 Exactly Once语义（解决数据重复问题）</h5><p>将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 <font color="red">At Least Once 语义</font>。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 <font color="red">At Most Once 语义。 </font></p><p>At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once 可以保证数据不重复，但是不能保证数据不丢失。<font color="red">但是，对于一些非常重要的信息，比如说 交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。</font>在 0.11 版 本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局 去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 </p><p>0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论 向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语 义，就构成了 Kafka 的 Exactly Once 语义。即：</p><p>​                       At Least Once + <font color="red">幂等性</font> = Exactly Once </p><p>要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker 只 会持久化一条。 </p><p>但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨 分区跨会话的 Exactly Once。 </p><p>使用时，只需将enable.idempotence属性设置为true，kafka自动将acks属性设为-1。</p><h4 id="3-3-Kafka消费者"><a href="#3-3-Kafka消费者" class="headerlink" title="3.3 Kafka消费者"></a>3.3 Kafka消费者</h4><h5 id="3-3-1-消费方式"><a href="#3-3-1-消费方式" class="headerlink" title="3.3.1 消费方式"></a>3.3.1 消费方式</h5><p>consumer采用pull（拉）模式从broker中读取数据。</p><p><font color="red">push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。</font>它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p><font color="red">pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。</font>针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</p><h5 id="3-3-2-分区分配策略"><a href="#3-3-2-分区分配策略" class="headerlink" title="3.3.2 分区分配策略"></a>3.3.2 分区分配策略</h5><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p><p>Kafka有两种分配策略，</p><p>一是roundrobin（轮询）：面向组，</p><p>一是range，面向主题</p><p><strong>（1）roundrobin（轮询）</strong></p><p><img src="https://i.loli.net/2020/10/27/AzubjqYyR5cNhXU.png"></p><p><img src="https://i.loli.net/2020/10/27/7yMa4OVPBswWgQY.png"></p><p><strong>（2）range（范围）</strong><font color="red">默认策略</font></p><p>​    <img src="https://i.loli.net/2020/10/27/AzubjqYyR5cNhXU.png"></p><p><img src="https://i.loli.net/2020/10/27/zVYlLwQhXFfdDix.png"></p><h5 id="3-3-3-offset的维护"><a href="#3-3-3-offset的维护" class="headerlink" title="3.3.3 offset的维护"></a>3.3.3 offset的维护</h5><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p><font color="red">Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</font></p><p>（1）保存在zk中时，是由组+topic+分区来保存的</p><p><img src="https://i.loli.net/2020/10/27/ZDjcShKlJ74qO5Q.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get &#x2F;consumers&#x2F;console-consumer-88502&#x2F;offsets&#x2F;bigdata&#x2F;0</span><br><span class="line">                     组名                       topic 分区</span><br></pre></td></tr></table></figure><p>（2）保存在本地</p><p>​        1.修改配置文件consumer.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exclude.internal.topic&#x3D;false &#x2F;&#x2F;可以让消费者消费内部主题</span><br></pre></td></tr></table></figure><p>​        2.读取offset</p><p>​            0.11.0.0之前的版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><p>​            0.11.0.0之后的版本（含）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh </span><br><span class="line">--topic __consumer_offsets</span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OddsetsMessageFormatter&quot;</span><br><span class="line">--consumer.config config/consumer.properties</span><br><span class="line">--from-beginning</span><br></pre></td></tr></table></figure><p>组+topic+分区唯一确定一个offset</p><h5 id="3-3-4-消费者组案例"><a href="#3-3-4-消费者组案例" class="headerlink" title="3.3.4 消费者组案例"></a>3.3.4 消费者组案例</h5><p><strong>需求</strong></p><p>​    测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费</p><p><strong>案例实操</strong></p><p>​    （1）在hadoop102、hadoop103上修改/opt/module/kafka/config/consumer.properties配置文件中的group.id属性为任意组名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 config]$ vi consumer.properties</span><br><span class="line">group.id=atguigu</span><br></pre></td></tr></table></figure><p>（2）在hadoop102、hadoop103上分别启动消费者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh </span><br><span class="line">--zookeeper hadoop102:2181</span><br><span class="line">--topic first </span><br><span class="line">--consumer.config config/consumer.properties</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh </span><br><span class="line">--bootstrap-server hadoop102:9092 </span><br><span class="line">--topic first </span><br><span class="line">--consumer.config config/consumer.properties</span><br></pre></td></tr></table></figure><p>（3）在hadoop104上启动生产者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh </span><br><span class="line">--broker-list hadoop102:9092 --topic first</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br></pre></td></tr></table></figure><p>（4）查看hadoop102和Hadoop103的接收者。</p><p>​        同一时刻同一个组中只有一个消费者接收到数据</p><p>​        不同组可以同时消费</p><h4 id="3-4-Kafka高效读写数据"><a href="#3-4-Kafka高效读写数据" class="headerlink" title="3.4 Kafka高效读写数据"></a>3.4 Kafka高效读写数据</h4><p><strong>（1）顺序写磁盘</strong></p><p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p><p><strong>（2）零复制技术</strong></p><p>​    原始</p><p>​    <img src="https://i.loli.net/2020/10/27/vNZFec9DIRwf67M.png"></p><p>​    零拷贝</p><p>​    <img src="https://i.loli.net/2020/10/27/lISnkgHLW4cdjeA.png"></p><h4 id="3-5-Zookeeper在Kafka中的作用"><a href="#3-5-Zookeeper在Kafka中的作用" class="headerlink" title="3.5 Zookeeper在Kafka中的作用"></a>3.5 Zookeeper在Kafka中的作用</h4><p>Kafka集群中有一个broker会被选举为Controller，<font color="red">负责管理集群broker的上下线</font>，所有topic的<font color="red">分区副本分配</font>和<font color="red">leader选举</font>等工作。</p><p>Controller的管理工作都依赖于Zookeeper。</p><p>以下是partition的leader选举过程：</p><p><img src="https://i.loli.net/2020/10/27/aBtgI3Rc462uLeS.png"></p><h4 id="3-6-Kafka事务"><a href="#3-6-Kafka事务" class="headerlink" title="3.6 Kafka事务"></a>3.6 Kafka事务</h4><p>Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><h5 id="3-6-1-Producer事务"><a href="#3-6-1-Producer事务" class="headerlink" title="3.6.1 Producer事务"></a>3.6.1 Producer事务</h5><p>为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。这样当Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。</p><p> 为了管理 Transaction，Kafka 引入了一个新的组件 Transaction Coordinator。Producer 就 是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于 事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 </p><h5 id="3-6-2-Consumer事务"><a href="#3-6-2-Consumer事务" class="headerlink" title="3.6.2 Consumer事务"></a>3.6.2 Consumer事务</h5><p>上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对 较弱，尤其时无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访 问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后删除的情况。 </p><h3 id="第四章-Kafka-API"><a href="#第四章-Kafka-API" class="headerlink" title="第四章 Kafka API"></a>第四章 Kafka API</h3><h4 id="4-1-Producer-API"><a href="#4-1-Producer-API" class="headerlink" title="4.1 Producer API"></a>4.1 Producer API</h4><h5 id="4-1-1-消息发送流程"><a href="#4-1-1-消息发送流程" class="headerlink" title="4.1.1 消息发送流程"></a>4.1.1 消息发送流程</h5><p>Kafka的producer发送消息采用的是<font color="red">异步发送</font>的方式。在消息发送的过程中，涉及到了<font color="red">两个线程——main线程和Sender线程</font>，以及<font color="red">一个线程共享变量——RecordAccumulator</font>。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p><p><img src="https://i.loli.net/2020/10/27/Sg5CBiAxHpc4J1f.png"></p><p><font color="red">相关参数：</font></p><p>batch.size：只有数据积累到batch.size之后，sender才会发送数据</p><p>linger.ms：如果数据迟迟未到达batch.size，sender等待linger.time之后就会发送数据。</p><h5 id="4-1-2-异步发送API"><a href="#4-1-2-异步发送API" class="headerlink" title="4.1.2 异步发送API"></a>4.1.2 异步发送API</h5><ol><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>编写代码</p><p>需要用到的类：</p><p><strong>KafkaProducer：</strong>需要创建一个生产者对象，用来发送数据</p><p><strong>ProducerConfig：</strong>获取所需的一系列配置参数</p><p><strong>ProducerRecord：</strong>每条数据都要封装成一个ProducerRecord对象</p></li><li><p>不带回调函数的API</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        <span class="comment">// ack应答级别</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小，16k</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间，1ms</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小，32M</span></span><br><span class="line">        <span class="comment">// key的序列化类</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        <span class="comment">// value的序列化类</span></span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close(); <span class="comment">//资源要关</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>带回调函数的API</p><p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p><p><font color="red">注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//ProduceConfig</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;atguigu--&quot;</span>+i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span>    </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;success-&gt;&quot;</span> + metadata.offset());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-1-3-同步发送API"><a href="#4-1-3-同步发送API" class="headerlink" title="4.1.3 同步发送API"></a>4.1.3 同步发送API</h5><p>同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</p><p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需要在调用Futrue对象的get方法即可。</p><p>send线程进行时阻塞main线程实现同步</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);<span class="comment">//kafka集群，broker-list</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i)).get();</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-2-Consumer-API"><a href="#4-2-Consumer-API" class="headerlink" title="4.2 Consumer API"></a>4.2 Consumer API</h4><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p><h5 id="4-2-1-手动提交offset"><a href="#4-2-1-手动提交offset" class="headerlink" title="4.2.1 手动提交offset"></a>4.2.1 手动提交offset</h5><ol><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.11.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>编写代码</p><p>需要用到的类</p><p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p><p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p><p><strong>ConsumerRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// ConsumerConfig</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//自动提交offset</span></span><br><span class="line">       </span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">            &#125;</span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>代码分析</p><p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将**<font color="red">本次poll的一批数据最高的偏移量提交</font>**；不同点是，commitSync会失败重试，一直到提交成功（如果由于不可恢复原因导致，也会提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p></li><li><p>数据重复消费问题</p><p><img src="https://i.loli.net/2020/10/27/BJAinMYoOmVGvxP.png"></p></li></ol><h5 id="4-2-2-自动提交offset"><a href="#4-2-2-自动提交offset" class="headerlink" title="4.2.2 自动提交offset"></a>4.2.2 自动提交offset</h5><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。</p><p>自动提交offset的相关参数：</p><p>enable.auto.commit：是否开启自动提交offset功能</p><p>auto.commit.interval.ms：自动提交offset的时间间隔</p><p>以下为自动提交offset的代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-自定义Interceptor"><a href="#4-3-自定义Interceptor" class="headerlink" title="4.3 自定义Interceptor"></a>4.3 自定义Interceptor</h4><h5 id="4-3-1-拦截器原理"><a href="#4-3-1-拦截器原理" class="headerlink" title="4.3.1 拦截器原理"></a>4.3.1 拦截器原理</h5><p>​        Producer拦截器(interceptor)是在Kafka0.10版本中被引入的，主要用于实现clients端的定制化控制逻辑。</p><p>​        对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是<code>org.apache.kafka.clients.producer.ProducerInterceptor</code>，其定义的方法包括：</p><p>（1）configure(configs)</p><p>​        获取配置信息和初始化数据时调用。</p><p>（2）onSend(ProducerRecord)</p><p>​        该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。<font color="red">用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区</font>，否则会影响目标分区的计算。</p><p>（3）onAcknowledgement(RecordMetadata,Exception)</p><p>​        <font color="red">该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。</font>并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。</p><p>（4）close</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 概述 / 第二章 快速入门 / 第三章 Kafka架构深入 / 第四章 Kafka API /&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Flume入门</title>
    <link href="http://luo6656.github.io/2020/06/17/BigDataFrame/Flume%E5%85%A5%E9%97%A8/"/>
    <id>http://luo6656.github.io/2020/06/17/BigDataFrame/Flume%E5%85%A5%E9%97%A8/</id>
    <published>2020-06-16T16:00:00.000Z</published>
    <updated>2020-10-27T06:24:12.189Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 概述 / 第二章 快速入门 / 第三章 企业开发案例 / 第四章 监测控制 / 第五章 自定义Interceptor /<br>第六章 自定义Source / 第七章 自定义Sink / 第八章 企业真实面试题</p><a id="more"></a><h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h3 id="第一章-概述"><a href="#第一章-概述" class="headerlink" title="第一章 概述"></a>第一章 概述</h3><h4 id="1-1-Flume定义"><a href="#1-1-Flume定义" class="headerlink" title="1.1 Flume定义"></a>1.1 Flume定义</h4><p>Flume是Cloudera提供的一个高可用的，高可靠的，<font color="red">分布式的海量日志采集、聚合和传输的系统</font>。Flume基于流式架构，灵活简单。</p><p><img src="https://i.loli.net/2020/10/27/ADfw2GVRl5voePH.png"></p><h4 id="1-2-Flume的优点"><a href="#1-2-Flume的优点" class="headerlink" title="1.2 Flume的优点"></a>1.2 Flume的优点</h4><ol><li>可以和任意存储进程集成。</li><li>输入的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</li><li>flume中的事务基于channel，使用两个事务模型(sender+receiver),确保消息被可靠发送。</li></ol><p>Flume使用两个独立的事务分别负责从source到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p><h4 id="1-3-Flume组成架构"><a href="#1-3-Flume组成架构" class="headerlink" title="1.3 Flume组成架构"></a>1.3 Flume组成架构</h4><p><img src="https://i.loli.net/2020/10/27/1kzlx3aqAoQXPts.png"><br><img src="https://i.loli.net/2020/10/27/xQqZr2XVidByMsw.png"></p><p>Channel是被动的，只能被put和take</p><p>​    下面我们来详细介绍一下Flume架构中的组件</p><h5 id="1-3-1-Agent"><a href="#1-3-1-Agent" class="headerlink" title="1.3.1 Agent"></a>1.3.1 Agent</h5><p>​        Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p><p>​        Agent主要有3个部分组成，source，channel，sink</p><h5 id="1-3-2-Source"><a href="#1-3-2-Source" class="headerlink" title="1.3.2 Source"></a>1.3.2 Source</h5><p><font color="red">Source是负责接收数据到Flume Agent的组件。</font>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、<font color="red">exec</font>、jms、<font color="red">spooling directory</font>、netcat、sequence generator、syslog、http、legacy。</p><h5 id="1-3-3-Channel"><a href="#1-3-3-Channel" class="headerlink" title="1.3.3 Channel"></a>1.3.3 Channel</h5><p><font color="red">Channel是位于Source和Sink之间的缓冲区。</font>因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel</p><p><font color="red">Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。</font>如果需要关系数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p><p><font color="red">File Channel将所有事件写到磁盘。</font>因此在程序关闭或机器宕机的情况下不会丢失数据。</p><h5 id="1-3-4-Sink"><a href="#1-3-4-Sink" class="headerlink" title="1.3.4 Sink"></a>1.3.4 Sink</h5><p><font color="red">Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</font></p><p><font color="red">Sink是完全事务性的。</font>在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p><h5 id="1-3-5-Event"><a href="#1-3-5-Event" class="headerlink" title="1.3.5 Event"></a>1.3.5 Event</h5><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。Event由可选的header和载有数据的一个byte array构成。Header是容纳了key-value字符串对的HashMap</p><p><img src="https://i.loli.net/2020/10/27/q1BEiWyTMe5oQZf.png"></p><h4 id="1-4-Flume拓扑结构"><a href="#1-4-Flume拓扑结构" class="headerlink" title="1.4 Flume拓扑结构"></a>1.4 Flume拓扑结构</h4><p><img src="https://i.loli.net/2020/10/27/Gp8SyUnYzgBjNsQ.png"></p><p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量，flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p><p><img src="https://i.loli.net/2020/10/27/2hmk4axYNT3gRie.png"></p><p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送不同的目的地。</p><p><img src="https://i.loli.net/2020/10/27/zXwGm3ZHBsIWeKR.png"></p><p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p><p><img src="https://i.loli.net/2020/10/27/9265on7NXimGptc.png"></p><p>这种模式是我们最常见的，也是非常实用，日常web应用通常分布在上百个服务，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p><h4 id="1-5-Flume-Agent内部原理"><a href="#1-5-Flume-Agent内部原理" class="headerlink" title="1.5 Flume Agent内部原理"></a>1.5 Flume Agent内部原理</h4><p><img src="https://i.loli.net/2020/10/27/ALNnw4ZD3EUWci6.png"></p><h3 id="第二章-快速入门"><a href="#第二章-快速入门" class="headerlink" title="第二章 快速入门"></a>第二章 快速入门</h3><h4 id="2-1-Flume安装地址"><a href="#2-1-Flume安装地址" class="headerlink" title="2.1 Flume安装地址"></a>2.1 Flume安装地址</h4><ol><li><p>Flume官网地址</p><blockquote><p><a href="http://flume.apache.org/">http://flume.apache.org/</a></p></blockquote></li><li><p>文档查看地址</p><blockquote><p><a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p></blockquote></li><li><p>下载地址</p><blockquote><p><a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></p></blockquote></li></ol><h4 id="2-2-安装部署"><a href="#2-2-安装部署" class="headerlink" title="2.2 安装部署"></a>2.2 安装部署</h4><ol><li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-flume-1.7.0-bin的名称为flume</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li><li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[atguigu@hadoop102 conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure></li></ol><h3 id="第三章-企业开发案例"><a href="#第三章-企业开发案例" class="headerlink" title="第三章 企业开发案例"></a>第三章 企业开发案例</h3><h4 id="3-1-监控端口数据官方案例（netcat）"><a href="#3-1-监控端口数据官方案例（netcat）" class="headerlink" title="3.1 监控端口数据官方案例（netcat）"></a>3.1 监控端口数据官方案例（netcat）</h4><ol><li><p>案例需求</p><p>（1）首先启动Flume任务，监控本机44444端口，服务器</p><p>（2）然后通过netcat工具向本机44444端口发送消息，客户端。</p><p>（3）最后Flume将监听的数据实时显示在控制台。</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/KQRVy8miANSlcOn.png"></p></li><li><p>实现步骤</p><p>（1）安装netcat工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure><p>（2）判断44444端口是否被占用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure><p>功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 </p><p>基本语法：netstat [选项]</p><p>选项参数：</p><p>​    -t或–tcp：显示TCP传输协议的连线状况； </p><p>​    -u或–udp：显示UDP传输协议的连线状况；</p><p>​    -n或–numeric：直接使用ip地址，而不通过域名服务器； </p><p>​    -l或–listening：显示监控中的服务器的Socket； </p><p>​    -p或–programs：显示正在使用Socket的程序识别码（PID）和程序名称；</p></li></ol><p>   （3）创建Flume Agent配置文件flume-netcat-logger.conf</p><p>   ​        a.在flume目录下创建job文件夹并进入job文件夹</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir job</span><br><span class="line">[atguigu@hadoop102 flume]$ cd job/</span><br></pre></td></tr></table></figure><p>   ​        b.在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-netcat-logger.conf</span><br></pre></td></tr></table></figure><p>   ​        c.在flume-netcat-logger.conf文件中添加如下</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><p>   <font color="red">注：配置文件来源于官方手册</font><a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p><p>   <img src="https://i.loli.net/2020/10/27/w5gbOoZCneYFyRE.png"></p><p>   ​        <font color="red">注意：最后一行channel，说明一个sink只能绑定一个channel，但是一个channel可以绑定多个sink</font></p><p>   （4）先开启flume监听端口</p><p>   ​        第一种写法：</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>   ​        第二种写法（简写）：</p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>   ​        参数说明（<font color="red">–XX表示参数，空格后面表示参数值</font>）：</p><p>   ​    –conf conf/  ：表示配置文件存储在conf/目录</p><p>   ​    –name a1    ：表示给agent起名为a1</p><p>   ​    –conf-file job/flume-netcat.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p><p>   ​    -Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p><p>   （5）使用netcat工具向本机的44444端口发送内容</p>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure><p>   （6）在Flume监听页面观察接收数据情况</p><p>   <img src="https://i.loli.net/2020/10/27/pVhIxjlGvEPsKa5.png"></p><p>   <font color="red">nc hadoop102 44444 ,flume能否接收到</font></p><h4 id="3-2-实时读取本地文件到HDFS案例（exec）"><a href="#3-2-实时读取本地文件到HDFS案例（exec）" class="headerlink" title="3.2 实时读取本地文件到HDFS案例（exec）"></a>3.2 实时读取本地文件到HDFS案例（exec）</h4><ol><li><p>案例需求：实时监控Hive日志，并上传到HDFS中</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/VMhIEBTSuDaGzrl.png"></p></li></ol><ol start="3"><li><p>实现步骤</p><p>（1）Flume要想将数据输出到HDFS，必须持有Hadoop相关jar包</p><p>​        将</p><p>​            commons-configuration-1.6.jar、</p><p>​            hadoop-auth-2.7.2.jar、</p><p>​            hadoop-common-2.7.2.jar、</p><p>​            hadoop-hdfs-2.7.2.jar、</p><p>​            commons-io-2.4.jar、 </p><p>​            htrace-core-3.1.0-incubating.jar</p><p>​        拷贝到/opt/module/flume/lib文件夹下。</p><p>（2）创建flume-file-hdfs.conf文件</p><p>​    创建文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>​    注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources &#x3D; r2</span><br><span class="line">a2.sinks &#x3D; k2</span><br><span class="line">a2.channels &#x3D; c2</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a2.sources.r2.type &#x3D; exec</span><br><span class="line">a2.sources.r2.command &#x3D; tail -F &#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs&#x2F;hive.log</span><br><span class="line">a2.sources.r2.shell &#x3D; &#x2F;bin&#x2F;bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type &#x3D; hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F;flume&#x2F;%Y%m%d&#x2F;%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix &#x3D; logs-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.round &#x3D; true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue &#x3D; 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit &#x3D; hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.batchSize &#x3D; 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval &#x3D; 60</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollSize &#x3D; 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k2.hdfs.rollCount &#x3D; 0</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type &#x3D; memory</span><br><span class="line">a2.channels.c2.capacity &#x3D; 1000</span><br><span class="line">a2.channels.c2.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels &#x3D; c2</span><br><span class="line">a2.sinks.k2.channel &#x3D; c2</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>对于所有与时间相关的转义序列，Event Header中必须存在以”timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p><p><font color="red">a3.sinks.k3.hdfs.useLocalTimeStamp=true</font></p><p><img src="https://i.loli.net/2020/10/27/vQKeJ659tTcCqsL.png"></p><p>（3）执行监控配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>（4）开启Hadoop和Hive并操作Hive产生日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure><p>（5）在HDFS上查看文件</p><p>​    <img src="https://i.loli.net/2020/10/27/tYWwDTFCxQy7bM1.png"></p></li></ol><h4 id="3-3-实时读取目录文件到HDFS案例（spooldir）"><a href="#3-3-实时读取目录文件到HDFS案例（spooldir）" class="headerlink" title="3.3 实时读取目录文件到HDFS案例（spooldir）"></a>3.3 实时读取目录文件到HDFS案例（spooldir）</h4><ol><li><p>案例需求：使用Flume监听整个目录的文件</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/XzdJSZQjA2aBlwg.png"></p></li><li><p>实现步骤</p><p>（1）创建配置文件flume-dir-hdfs.conf</p><p>​        a.创建一个文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>​        b.打开文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>​        c.添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash">忽略所有以.tmp结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/t1FcnYeX6jvJOIx.png"></p><p>（2）启动监控文件夹命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p><font color="red">说明：在使用Spooling Directory Source时</font></p><p>a. 不要在监控目录中创建并持续修改文件(不能监控动态变化的文件)</p><p>b. 上传完成的文件会以.COMPLETED结尾（默认）</p><p>c. 被监控文件夹每500毫秒扫描一次文件变动</p><p>（3）向upload文件夹中添加文件</p><p>​        在/opt/module/flume目录下创建upload文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir upload</span><br></pre></td></tr></table></figure><p>​        向upload文件夹中添加文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.txt</span><br><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.tmp</span><br><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.log</span><br></pre></td></tr></table></figure><p>（4）查看HDFS上的数据</p><p><img src="https://i.loli.net/2020/10/27/hBistWC9Gu5aFx6.png"></p><p>（5）等待1s，再次查询upload文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.tmp</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.txt.COMPLETED</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-3-实时监控目录下的多个追加文件（tailDir）"><a href="#3-3-实时监控目录下的多个追加文件（tailDir）" class="headerlink" title="3.3 实时监控目录下的多个追加文件（tailDir）"></a>3.3 实时监控目录下的多个追加文件（tailDir）</h4><p>​        Exec source适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir中source能保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而Taildir Source既能实现断点续传，又可以保证数据不丢失，还能够进行实时监控。</p><p><strong>案例需求</strong></p><p>使用Flume监听整个目录的实时追加文件，并上传至HDFS</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/iebAFwkufNgUISD.png"></p><p><strong>实现步骤</strong></p><p>创建配置文件taildir-flume-hdfs.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r1.filegroups &#x3D; f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;files&#x2F;file1.txt</span><br><span class="line">a1.sources.r1.filegroups.f2 &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;files&#x2F;file2.txt</span><br><span class="line">a1.sources.r1.positionFile &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;position&#x2F;position.json</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动监控命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flume-ng agent -c conf&#x2F; -n a1 -f job&#x2F;files-flume-logger.conf -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure><h4 id="3-4-单数据源多出口案例（选择器）"><a href="#3-4-单数据源多出口案例（选择器）" class="headerlink" title="3.4 单数据源多出口案例（选择器）"></a>3.4 单数据源多出口案例（选择器）</h4><p><img src="https://i.loli.net/2020/10/27/8fqLvsIX12VHimW.png"></p><p><strong>案例需求</strong></p><p>​    使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到LocalFileSystem。</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/GlJQz5OHbW3I7FV.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group1文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ cd group1/</span><br></pre></td></tr></table></figure><p>在/opt/module/datas/目录下创建flume3文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ mkdir flume3</span><br></pre></td></tr></table></figure></li><li><p>创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-file-flume.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-file-flume.conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将数据流复制给所有channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink端的avro是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p><font color="red">注：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架</font></p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li></ol><ol start="3"><li><p>创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-flume-hdfs.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-flume-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>端的avro是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-flume-dir.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-flume-dir.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>启动Hadoop和Hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>检查HDFS上数据</p><p><img src="https://i.loli.net/2020/10/27/CWbqTAVaHB6ye7n.png"></p></li></ol><ol start="8"><li><p>检查/opt/module/datas/flume3目录中数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume3]$ ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 5942 5月  22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-5-单数据源多出口案例（Sink组）（负载均衡和故障转移）"><a href="#3-5-单数据源多出口案例（Sink组）（负载均衡和故障转移）" class="headerlink" title="3.5 单数据源多出口案例（Sink组）（负载均衡和故障转移）"></a>3.5 单数据源多出口案例（Sink组）（负载均衡和故障转移）</h4><p>​    单Source、Channel多Sink（负载均衡），如图</p><p>​    <img src="https://i.loli.net/2020/10/27/d4SW7w2nmeo83x5.png"></p><p><strong>案例需求</strong></p><p>​    使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/YngpWLfw7h5IDUj.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group2文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ cd group2&#x2F;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建flume-netcat-flume</p><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-netcat-flume.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-netcat-flume.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">/*这里是负载均衡，故障转移的type是failover*/</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p><font color="red">注</font>：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li></ol><ol start="3"><li><p>创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-flume-console1.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-flume-console1.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume-flume-console2.conf</p><p>配置上机Flume输出的Source，输出是到本地控制台</p><p>创建配置文件并打开</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-flume-console2.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-flume-console2.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>使用netcat工具向本机的4444端口发送内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 44444</span></span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li>查看Flume2及Flume3的控制台打印日志</li></ol><h4 id="3-6-多数据源汇总案例"><a href="#3-6-多数据源汇总案例" class="headerlink" title="3.6 多数据源汇总案例"></a>3.6 多数据源汇总案例</h4><p>多Source汇总数据到单Flume如图</p><p><img src="https://i.loli.net/2020/10/27/Cc5oJmHfTSNPyrA.png"></p><p><strong>案例需求</strong></p><p>​    hadoop103上的Flume-1监控文件/opt/module/group.log</p><p>​    hadoop102上的Flume-2监控某一个端口的数据流</p><p>​    Flume-1与Flume-2将数据发送给hadoop104上的Flume-3，Flume-3将最终数据打印到控制台。</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/DWFKlntXPGQjqA5.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>分发Flume</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync flume</span><br></pre></td></tr></table></figure><p>在hadoop102、hadoop103以及hadoop104的/opt/module/flume/job目录下创建一个group3文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ mkdir group3</span><br><span class="line">[atguigu@hadoop103 job]$ mkdir group3</span><br><span class="line">[atguigu@hadoop104 job]$ mkdir group3</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建flume1-logger-flume.conf</p><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume</p><p>在hadoop103上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 group3]$ touch flume1-logger-flume.conf</span><br><span class="line">[atguigu@hadoop103 group3]$ vim flume1-logger-flume.conf </span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>创建flume2-netcat-flume.conf</p><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在hadoop102上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group3]$ touch flume2-netcat-flume.conf</span><br><span class="line">[atguigu@hadoop102 group3]$ vim flume2-netcat-flume.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = hadoop104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume3-flume-logger.conf</p><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在hadoop104上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 group3]$ touch flume3-flume-logger.conf</span><br><span class="line">[atguigu@hadoop104 group3]$ vim flume3-flume-logger.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>在hadoop103上向/opt/module目录下的group.log追加内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 module]$ echo &#x27;hello&#x27; &gt; group.log</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>在hadoop102上向44444端口发送数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ telnet hadoop102 44444</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>检查hadoop104上数据</p><p><img src="G:\截图\flume\3_6_2.png"></p></li></ol><h3 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h3><h4 id="4-1-Ganglia的安装与部署"><a href="#4-1-Ganglia的安装与部署" class="headerlink" title="4.1 Ganglia的安装与部署"></a>4.1 Ganglia的安装与部署</h4><ol><li><p>安装httpd服务与php</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install httpd php</span><br></pre></td></tr></table></figure></li><li><p>安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure></li><li><p>安装ganglia</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install ganglia-gmetad </span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install ganglia-web</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum install -y ganglia-gmond</span><br></pre></td></tr></table></figure><p>Ganglia由gmond、gmetad和gweb三部分组成。</p><p><font color="red">gmond（Ganglia Monitoring Daemon）</font>是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等。</p><p><font color="red">gmetad（Ganglia Meta Daemon）</font>整合所有信息，并将其以RRD格式存储至磁盘的服务。</p><p><font color="red">gweb（Ganglia Web）</font>Ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。</p></li><li><p>修改配置文件/etc/httpd/conf.d/ganglia.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure><p><font color="red">修改为红颜色的配置：</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Ganglia monitoring system php web frontend</span><br><span class="line">Alias &#x2F;ganglia &#x2F;usr&#x2F;share&#x2F;ganglia</span><br><span class="line">&lt;Location &#x2F;ganglia&gt;</span><br><span class="line">  Order deny,allow</span><br><span class="line">  #Deny from all</span><br><span class="line">  Allow from all</span><br><span class="line">  # Allow from 127.0.0.1</span><br><span class="line">  # Allow from ::1</span><br><span class="line">  # Allow from .example.com</span><br><span class="line">&lt;&#x2F;Location&gt;</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>修改配置文件/etc/ganglia/gmetad.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/ganglia/gmetad.conf</span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_source &quot;hadoop102&quot; 192.168.1.102</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>修改配置文件/etc/ganglia/gmond.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim &#x2F;etc&#x2F;ganglia&#x2F;gmond.conf </span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">  name = &quot;hadoop102&quot;</span><br><span class="line">  owner = &quot;unspecified&quot;</span><br><span class="line">  latlong = &quot;unspecified&quot;</span><br><span class="line">  url = &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">bind_hostname = yes <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line">                       # This option tells gmond to use a source address</span><br><span class="line">                       # that resolves to the machine&#x27;s hostname.  Without</span><br><span class="line">                       # this, the metrics may appear to come from any</span><br><span class="line">                       # interface and the DNS names associated with</span><br><span class="line">                       # those IPs will be used to create the RRDs.</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  host = 192.168.1.102</span><br><span class="line">  port = 8649</span><br><span class="line">  ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  port = 8649</span><br><span class="line">  bind = 192.168.1.102</span><br><span class="line">  retry_bind = true</span><br><span class="line"><span class="meta">  #</span><span class="bash"> Size of the UDP buffer. If you are handling lots of metrics you really</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> buffer = 10485760</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>修改配置文件/etc/selinux/config</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo setenforce 0</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>启动ganglia</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo service httpd start</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo service gmetad start</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo service gmond start</span><br></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>打开网页浏览ganglia页面</p><p><a href="http://192.168.1.102/ganglia">http://192.168.1.102/ganglia</a></p><p><font color="red">提示</font>：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo chmod -R 777 &#x2F;var&#x2F;lib&#x2F;ganglia</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-2-操作Flume测试监控"><a href="#4-2-操作Flume测试监控" class="headerlink" title="4.2 操作Flume测试监控"></a>4.2 操作Flume测试监控</h4><ol><li><p>修改/opt/module/flume/conf目录下的flume-env.sh配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS&#x3D;&quot;-Dflume.monitoring.type&#x3D;ganglia</span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.1.102:8649</span><br><span class="line">-Xms100m</span><br><span class="line">-Xmx200m&quot;</span><br></pre></td></tr></table></figure></li><li><p>启动Flume任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin&#x2F;flume-ng agent \</span><br><span class="line">--conf conf&#x2F; \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf-file job&#x2F;flume-netcat-logger.conf \</span><br><span class="line">-Dflume.root.logger&#x3D;&#x3D;INFO,console \</span><br><span class="line">-Dflume.monitoring.type&#x3D;ganglia \</span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.1.102:8649</span><br></pre></td></tr></table></figure></li><li><p>发送数据观察ganglia监测图</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ nc localhost 44444</span><br></pre></td></tr></table></figure><p><strong>样式如图：</strong></p><p><img src="https://i.loli.net/2020/10/27/dBG5X83zocSWuOI.png"></p><p><strong>图例说明：</strong></p><table><thead><tr><th>字段（图表名称）</th><th>字段含义</th></tr></thead><tbody><tr><td>EventPutAttemptCount</td><td>source尝试写入channel的事件总数量</td></tr><tr><td>EventPutSuccessCount</td><td>成功写入channel且提交的事件总数量</td></tr><tr><td>EventTakeAttemptCount</td><td>sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。</td></tr><tr><td>EventTakeSuccessCount</td><td>sink成功读取的事件的总数量</td></tr><tr><td>StartTime</td><td>channel启动的时间（毫秒）</td></tr><tr><td>StopTime</td><td>channel停止的时间（毫秒）</td></tr><tr><td>ChannelSize</td><td>目前channel中事件的总数量</td></tr><tr><td>ChannelFillPercentage</td><td>channel占用百分比</td></tr><tr><td>ChannelCapacity</td><td>channel的容量</td></tr></tbody></table></li></ol><h3 id="第五章-自定义Interceptor"><a href="#第五章-自定义Interceptor" class="headerlink" title="第五章 自定义Interceptor"></a>第五章 自定义Interceptor</h3><h3 id="第六章-自定义Source"><a href="#第六章-自定义Source" class="headerlink" title="第六章 自定义Source"></a>第六章 自定义Source</h3><h4 id="6-1-介绍"><a href="#6-1-介绍" class="headerlink" title="6.1 介绍"></a>6.1 介绍</h4><p><font color="red">Source是负责接收数据到Flume Agent的组件。</font>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。</p><p>官方也提供了自定义source的接口：</p><p><a href="#source">https://flume.apache.org/FlumeDeveloperGuide.html#source</a>根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p><p>实现相应方法：</p><p><code>getBackOffSleepIncrement()</code>//暂不用</p><p><code>getMaxBackOffSleepInterval()</code>//暂不用</p><p><code>configure(Context context)</code>//初始化context（读取配置文件内容）</p><p><code>process()</code>//获取数据封装成event并写入channel，<font color="red">这个方法将被循环调用。</font></p><p>使用场景：读取MySQL数据或者其他文件系统。</p><h4 id="6-2-需求"><a href="#6-2-需求" class="headerlink" title="6.2 需求"></a>6.2 需求</h4><p>使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。</p><p><img src="https://i.loli.net/2020/10/27/HkaSKRlrtq4hbLo.png"></p><h4 id="6-3-分析"><a href="#6-3-分析" class="headerlink" title="6.3 分析"></a>6.3 分析</h4><p><img src="https://i.loli.net/2020/10/27/92KqrxYAdecPwDs.png"></p><h4 id="6-4-编码"><a href="#6-4-编码" class="headerlink" title="6.4 编码"></a>6.4 编码</h4><ol><li><p>导入pom依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>写java代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义配置文件将来要读取的字段</span></span><br><span class="line">    <span class="keyword">private</span> Long delay;</span><br><span class="line">    <span class="keyword">private</span> String field;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化配置信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        delay = context.getLong(<span class="string">&quot;delay&quot;</span>);</span><br><span class="line">        field = context.getString(<span class="string">&quot;field&quot;</span>, <span class="string">&quot;Hello!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建事件头信息</span></span><br><span class="line">            HashMap&lt;String, String&gt; hearderMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="comment">//创建事件</span></span><br><span class="line">            SimpleEvent event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">            <span class="comment">//循环封装事件</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                <span class="comment">//给事件设置头信息</span></span><br><span class="line">                event.setHeaders(hearderMap);</span><br><span class="line">                <span class="comment">//给事件设置内容</span></span><br><span class="line">                event.setBody((field + i).getBytes());</span><br><span class="line">                <span class="comment">//将事件写入channel</span></span><br><span class="line">                getChannelProcessor().processEvent(event);</span><br><span class="line">                Thread.sleep(delay);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-5-测试"><a href="#6-5-测试" class="headerlink" title="6.5 测试"></a>6.5 测试</h4><ol><li><p>打包</p><p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume)下。</p></li><li><p>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; com.atguigu.MySource</span><br><span class="line">a1.sources.r1.delay &#x3D; 1000</span><br><span class="line">#a1.sources.r1.field &#x3D; atguigu</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>开启任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ pwd</span><br><span class="line">/opt/module/flume</span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysource.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><p><img src="https://i.loli.net/2020/10/27/1uVWsDKybXSUk3B.png"></p></li></ol><h3 id="第七章-自定义Sink"><a href="#第七章-自定义Sink" class="headerlink" title="第七章 自定义Sink"></a>第七章 自定义Sink</h3><h4 id="7-1-介绍"><a href="#7-1-介绍" class="headerlink" title="7.1 介绍"></a>7.1 介绍</h4><p><font color="red">Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent.</font></p><p>Sink是完全事务性地。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写入到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。</p><p><font color="red">官方也提供了自定义source的接口:</font></p><p><a href="#sink">https://flume.apache.org/FlumeDeveloperGuide.html#sink</a>根据官方说明自定义MySink需要继承AbstractSink类并实现Configurable接口。</p><p>实现相应方法：</p><p><code>configure(Context context)</code>//初始化context（读取配置文件内容）</p><p><code>process()</code>//从Channel读取获取数据（event），这个方法将被循环调用。</p><p>使用场景：读取Channel数据写入MySQL或者其他文件系统。</p><h4 id="7-2-需求"><a href="#7-2-需求" class="headerlink" title="7.2 需求"></a>7.2 需求</h4><p>使用flume接收数据，并在Sink端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume任务配置文件中配置。</p><p>流程分析</p><p><img src="https://i.loli.net/2020/10/27/e4PE23m7rh9nKyg.png"></p><h4 id="7-3-编码"><a href="#7-3-编码" class="headerlink" title="7.3 编码"></a>7.3 编码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Logger对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(AbstractSink.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前Sink绑定的Channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取Channel中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> String(event.getBody()) + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefix&quot;</span>, <span class="string">&quot;hello:&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">&quot;suffix&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="7-4-测试"><a href="#7-4-测试" class="headerlink" title="7.4 测试"></a>7.4 测试</h4><ol><li><p>打包</p><p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p></li><li><p>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; com.atguigu.MySink</span><br><span class="line">#a1.sinks.k1.prefix &#x3D; atguigu:</span><br><span class="line">a1.sinks.k1.suffix &#x3D; :atguigu</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></li><li><p>开启任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ pwd</span><br><span class="line">/opt/module/flume</span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">[atguigu@hadoop102 ~]$ nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br><span class="line">atguigu</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><p><img src="https://i.loli.net/2020/10/27/ILe48pEPrfyzbus.png"></p></li></ol><h3 id="第八章-企业真实面试题（重点）"><a href="#第八章-企业真实面试题（重点）" class="headerlink" title="第八章 企业真实面试题（重点）"></a>第八章 企业真实面试题（重点）</h3><h4 id="8-1-你是如何实现Flume数据传输的监控的"><a href="#8-1-你是如何实现Flume数据传输的监控的" class="headerlink" title="8.1 你是如何实现Flume数据传输的监控的"></a>8.1 你是如何实现Flume数据传输的监控的</h4><p>​    使用第三方框架Ganglia实时监控Flume</p><h4 id="8-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？"><a href="#8-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？" class="headerlink" title="8.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？"></a>8.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？</h4><p><strong>1.作用</strong></p><p>（1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</p><p>（2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中</p><p>（3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</p><p>2.我们公司采用的Source类型为：</p><p>（1）监控后台日志：exec</p><p>（2）监控后台产生日志的端口：netcat</p><p><font color="red">exec spooldir</font></p><h4 id="8-3-Flume的Channel-Selectors"><a href="#8-3-Flume的Channel-Selectors" class="headerlink" title="8.3 Flume的Channel Selectors"></a>8.3 Flume的Channel Selectors</h4><p><img src="https://i.loli.net/2020/10/27/b3ym6Jj5s7rlxTq.png"></p><h4 id="8-4-Flume参数调优"><a href="#8-4-Flume参数调优" class="headerlink" title="8.4 Flume参数调优"></a>8.4 Flume参数调优</h4><ol><li><strong>Source</strong></li></ol><p>增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</p><p>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</p><ol start="2"><li><strong>Channel</strong> </li></ol><p>type 选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。</p><p>使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</p><p>Capacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</p><ol start="3"><li><strong>Sink</strong> </li></ol><p>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</p><p>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</p><h4 id="8-5-Flume的事务机制"><a href="#8-5-Flume的事务机制" class="headerlink" title="8.5 Flume的事务机制"></a>8.5 Flume的事务机制</h4><p>Flume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。</p><h4 id="8-6-Flume采集数据会丢失吗"><a href="#8-6-Flume采集数据会丢失吗" class="headerlink" title="8.6 Flume采集数据会丢失吗"></a>8.6 Flume采集数据会丢失吗</h4><p>不会，Channel存储可以存储在File中，数据传输自身有事务。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章 概述 / 第二章 快速入门 / 第三章 企业开发案例 / 第四章 监测控制 / 第五章 自定义Interceptor /&lt;br&gt;第六章 自定义Source / 第七章 自定义Sink / 第八章 企业真实面试题&lt;/p&gt;</summary>
    
    
    
    <category term="大数据框架" scheme="http://luo6656.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Tebleau</title>
    <link href="http://luo6656.github.io/2020/05/10/BI/Tebleau/"/>
    <id>http://luo6656.github.io/2020/05/10/BI/Tebleau/</id>
    <published>2020-05-09T16:00:00.000Z</published>
    <updated>2020-10-30T02:56:36.975Z</updated>
    
    
    
    
    <category term="BI工具" scheme="http://luo6656.github.io/categories/BI%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
  <entry>
    <title>PowerBI</title>
    <link href="http://luo6656.github.io/2020/05/05/BI/PowerBI/"/>
    <id>http://luo6656.github.io/2020/05/05/BI/PowerBI/</id>
    <published>2020-05-04T16:00:00.000Z</published>
    <updated>2020-10-30T02:56:37.042Z</updated>
    
    
    
    
    <category term="BI工具" scheme="http://luo6656.github.io/categories/BI%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
  <entry>
    <title>Kettle</title>
    <link href="http://luo6656.github.io/2020/05/03/ETL/Kettle/"/>
    <id>http://luo6656.github.io/2020/05/03/ETL/Kettle/</id>
    <published>2020-05-02T16:00:00.000Z</published>
    <updated>2020-10-30T02:56:37.009Z</updated>
    
    
    
    
    <category term="ETL工具" scheme="http://luo6656.github.io/categories/ETL%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
</feed>
