<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="author" content="Eric"><meta name="keywords" content=""><meta name="description" content="决策树认识决策树
决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法
信息的单位：比特bite



信息熵和香农定理
https://blog.csdn.net/dyx810601/article/details/..."><meta name="Robots" content="all"><title>Eric个人博客 | 机器学习之决策树</title><link rel="icon" href="/images/icon.svg"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/atom-one-dark.css"><link rel="stylesheet" href="/css/style.css"><script src="/js/highlight.min.js"></script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Eric个人博客" type="application/atom+xml"></head><body><div class="main-container"><header class="header"><div class="global-width"><nav class="nav-box"><a class="nav-item" href="/">主页</a> <a class="nav-item" href="/resume">项目</a> <a class="nav-item" href="/mood" target="_blank">热点观点</a> <a class="nav-item" href="/2018/01/01/introduce/" target="_blank">个人介绍</a> <a class="nav-item" href="/fuye.md">关于</a></nav></div></header><section class="content global-width"><div class="main"><article class="box post"><div class="post-title align-center detail-title">机器学习之决策树</div><div class="post-meta align-center"><span class="label">原创</span> <span class="dotted">|</span> <i class="fa fa-calendar"></i> <time>2019-07-25</time> <span class="dotted">|</span> <i class="fa fa-user"></i> Eric <span class="dotted">|</span> <i class="fa fa-folder-open-o"></i> <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-content"><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="认识决策树"><a href="#认识决策树" class="headerlink" title="认识决策树"></a>认识决策树</h4><ul><li>决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法</li><li>信息的单位：比特bite</li></ul><a id="more"></a><h4 id="信息熵和香农定理"><a href="#信息熵和香农定理" class="headerlink" title="信息熵和香农定理"></a>信息熵和香农定理</h4><ul><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dyx810601/article/details/82226456">https://blog.csdn.net/dyx810601/article/details/82226456</a><ul><li>信息熵在信息传递和压缩中常见<img src="https://img-blog.csdnimg.cn/20190907100247908.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE3MDg2Mw==,size_16,color_FFFFFF,t_70" alt="信息熵"></li></ul></li><li>当得到一些信息时信息熵就减小了</li><li>信息和消除不确定性是相联系的：<font color="red">信息熵越大，不确定性越大</font></li><li>决策树中把能减少更多的不确定性(信息熵)放在根上</li></ul><h4 id="决策树的划分依据之一-信息增益"><a href="#决策树的划分依据之一-信息增益" class="headerlink" title="决策树的划分依据之一:信息增益"></a>决策树的划分依据之一:<font color="red">信息增益</font></h4><ul><li>信息增益：当得知一个特征条件之后，减少的信息熵的大小</li><li>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：<font color="red">g(D,A)=H(D)-H(D|A)</font></li></ul><h4 id="常见决策树使用的算法"><a href="#常见决策树使用的算法" class="headerlink" title="常见决策树使用的算法"></a>常见决策树使用的算法</h4><ul><li>ID3<ul><li><font color="red">信息增益 - 最大的准则</font></li></ul></li><li>C4.5<ul><li>信息增益比 - 最大的准则</li></ul></li><li>CART<ul><li>回归树：平方误差最小</li><li>分类树：基尼系数 最小的准则 在sklearn中可以选择划分的默认原则 划分更加仔细</li></ul></li><li>基尼系数<br><img src="https://img-blog.csdnimg.cn/20190915164223928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE3MDg2Mw==,size_16,color_FFFFFF,t_70" alt="基尼系数"><h4 id="sklearn决策树API"><a href="#sklearn决策树API" class="headerlink" title="sklearn决策树API"></a>sklearn决策树API</h4></li><li><code>sklearn.tree.DecisionTreeClassifier(criterion=&#39;gini&#39;,max_depth=None,random_state=None)</code></li><li>决策树分类器</li><li><code>criterion</code>:默认是’gini’系数，也可以选择信息增益的熵‘entropy’</li><li><code>max_depth</code>:树的深度大小</li><li><code>random_state</code>:随机数种子</li><li><code>method</code>：</li><li><code>decision_path</code>:返回决策树的路径</li></ul><h4 id="决策树的结构、本地保存"><a href="#决策树的结构、本地保存" class="headerlink" title="决策树的结构、本地保存"></a>决策树的结构、本地保存</h4><ul><li><code>sklearn.tree.export_graphviz()</code> 该函数能够导出DOT格式</li><li><code>tree,export_graphviz(estimator,out_flie=&#39;tree.dot&#39;,feature_names=[&quot;,&quot;])</code></li><li>查看dot文件的工具：graphviz</li></ul><h4 id="信息论基础–银行贷款分析"><a href="#信息论基础–银行贷款分析" class="headerlink" title="信息论基础–银行贷款分析"></a>信息论基础–银行贷款分析</h4><ul><li>特征<ul><li>年龄；是否有工作；是否有房子；信贷情况；类别</li></ul></li></ul><h4 id="决策树的优缺点以及改进"><a href="#决策树的优缺点以及改进" class="headerlink" title="决策树的优缺点以及改进"></a>决策树的优缺点以及改进</h4><ul><li><p>优点：</p><ul><li>简单的理解和解释，树木可视化。</li><li>需要很少的数据准备，<font color="red">技术通常需要数据归一化</font></li><li>在企业重要决策中，由于决策树很好的分析能力，在决策过程中应用较多，可以找到主要决策因素</li></ul></li><li><p>缺点</p><ul><li>决策树学习者可以创建不能很好地推广数据地过于复杂的树，这被称为过拟合</li></ul></li><li><p>改进</p><ul><li>减枝cart算法(<font color="red">决策树API当中已经实现，随机森林参数调优</font>):小于参数的sample分支不要</li><li><font color="red">随机森林</font></li></ul></li></ul><h4 id="集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林"><a href="#集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林" class="headerlink" title="集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林"></a>集成学习方法之一：‘’‘’‘’‘’‘’‘’‘’‘’‘’随机森林</h4><ul><li>集成学习方法：通过建立几个模型组合来解决单一预测问题。它的工作原理是<font color="red">生成多个分类/模型</font>,各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类地做出预测</li><li>定义：在机器学习中，<font color="red">随机森林</font>是一个包含多个决策树地分类器，并且其输出地类别是由个别输出的类别的众数而定。</li><li>例如：如果你训练了5个树，其中有4个数的结果是True，1个是False，那么最终结果会是True</li></ul><h4 id="随机森林建立多个决策树的过程"><a href="#随机森林建立多个决策树的过程" class="headerlink" title="随机森林建立多个决策树的过程"></a>随机森林建立多个决策树的过程</h4><ul><li>单个树的建立过程<ul><li>1.随机在N个样本中选择一个样本,重复N次(样本有可能重复，<font color="red">有放回</font>)</li><li>2.随机在M个特征中选择m个特征，m&lt;&lt;M</li></ul></li><li>建立多颗决策树，样本特征大多不一样</li><li>bootstrap:随机有放回抽样</li><li>为什么要随机抽样训练集<ul><li>如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出来的决策树的分类结果也完全一样</li></ul></li><li>为什么要有放回地抽样<ul><li>如果不是有放回地抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”,都是绝对”片面的“，也就是说每棵树训练出来都是有很大差异的；而随机森林最后分类取决于多棵树(弱分类器)的投票表决。</li></ul></li></ul><h4 id="随机森林API"><a href="#随机森林API" class="headerlink" title="随机森林API"></a>随机森林API</h4><ul><li><code>sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion=&#39;gini&#39;,max_depth=None,bootstrap=True,random_state=None)</code></li><li><code>n_estimators</code>:森林里的树木的数量，default=10,有120,200,300,500,800,1200</li><li><code>criteria</code>:分割特征的测量方法，default=”gini”</li><li><code>max_depth</code>:树的最大深度，default为无</li><li><code>max_features=&quot;auto&quot;</code>,每个决策树的最大特征数量，有”sqrt”,”log2”,”None”</li><li><code>boostrap</code>:是否在构建树时放回抽样，default=True<h4 id="随机森林的优点"><a href="#随机森林的优点" class="headerlink" title="随机森林的优点"></a>随机森林的优点</h4></li><li>在当前所有算法中，具有极好的准确率</li><li>能够有效地运行在大数据集上:样本多，特征多</li><li>能够处理具有高维特征的输入样本，而且不需要降维</li><li>能够评估各个特征在分类问题上的重要性</li></ul></div></article></div><div class="aside"><div class="box widget"><div class="introduction"><p><img src="/images/ironman.jpg" alt="head-sculpture"></p><p class="name">罗明辉Eric</p><p class="slogan">个人博客，分享经验，分享快乐</p></div></div><div class="box widget"><div class="title">最新</div><ul class="item-box"><li><a href="/2020/11/13/BigDataFrame/Flink/">Flink</a></li><li><a href="/2020/11/05/BigDataFrame/canal/">Canal</a></li><li><a href="/2020/09/26/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1/">数据仓库-建模</a></li><li><a href="/2020/09/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/">数据仓库-架构</a></li><li><a href="/2020/09/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/">数据仓库-概述</a></li><li><a href="/2020/09/18/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a></li><li><a href="/2020/09/10/BigDataFrame/YARN/">YARN</a></li><li><a href="/2020/09/06/BigDataFrame/HDFS%E8%81%94%E9%82%A6%E6%9C%BA%E5%88%B6/">HDFS联邦机制</a></li></ul></div><div class="box widget"><div class="title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BI%E5%B7%A5%E5%85%B7/">BI工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ETL%E5%B7%A5%E5%85%B7/">ETL工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div><div class="box widget"><div class="title">归档</div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">2020-11</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">2020-09</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">2020-07</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">2020-06</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">2020-05</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">2020-01</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">2019-12</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">2019-07</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">2019-03</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">2018-09</a><span class="archive-list-count">1</span></li></ul></div></div></section><footer class="footer"><div class="global-width footer-box"><div class="copyright"><span>Copyright &copy; 2020</span> <span class="dotted">|</span> <span>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a></span> <span class="dotted">|</span></div></div></footer></div><script>hljs.initHighlightingOnLoad()</script></body></html>