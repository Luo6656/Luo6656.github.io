<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="author" content="Eric"><meta name="keywords" content=""><meta name="description" content="第一章 概述 / 第二章 快速入门 / 第三章 企业开发案例 / 第四章 监测控制 / 第五章 自定义Interceptor /第六章 自定义Source / 第七章 自定义Sink / 第八章 企业真实面试题


Flume第一章 概述1.1 Flume定义Flume是Cloudera提供的..."><meta name="Robots" content="all"><title>Eric个人博客 | Flume入门</title><link rel="icon" href="/images/icon.svg"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/atom-one-dark.css"><link rel="stylesheet" href="/css/style.css"><script src="/js/highlight.min.js"></script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Eric个人博客" type="application/atom+xml"></head><body><div class="main-container"><header class="header"><div class="global-width"><nav class="nav-box"><a class="nav-item" href="/">主页</a> <a class="nav-item" href="/resume">项目</a> <a class="nav-item" href="/mood" target="_blank">热点观点</a> <a class="nav-item" href="/2018/01/01/introduce/" target="_blank">个人介绍</a> <a class="nav-item" href="/fuye.md">关于</a></nav></div></header><section class="content global-width"><div class="main"><article class="box post"><div class="post-title align-center detail-title">Flume入门</div><div class="post-meta align-center"><span class="label">原创</span> <span class="dotted">|</span> <i class="fa fa-calendar"></i> <time>2020-06-17</time> <span class="dotted">|</span> <i class="fa fa-user"></i> Eric <span class="dotted">|</span> <i class="fa fa-folder-open-o"></i> <a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a></div><div class="post-content"><p>第一章 概述 / 第二章 快速入门 / 第三章 企业开发案例 / 第四章 监测控制 / 第五章 自定义Interceptor /<br>第六章 自定义Source / 第七章 自定义Sink / 第八章 企业真实面试题</p><a id="more"></a><h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h3 id="第一章-概述"><a href="#第一章-概述" class="headerlink" title="第一章 概述"></a>第一章 概述</h3><h4 id="1-1-Flume定义"><a href="#1-1-Flume定义" class="headerlink" title="1.1 Flume定义"></a>1.1 Flume定义</h4><p>Flume是Cloudera提供的一个高可用的，高可靠的，<font color="red">分布式的海量日志采集、聚合和传输的系统</font>。Flume基于流式架构，灵活简单。</p><p><img src="https://i.loli.net/2020/10/27/ADfw2GVRl5voePH.png"></p><h4 id="1-2-Flume的优点"><a href="#1-2-Flume的优点" class="headerlink" title="1.2 Flume的优点"></a>1.2 Flume的优点</h4><ol><li>可以和任意存储进程集成。</li><li>输入的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</li><li>flume中的事务基于channel，使用两个事务模型(sender+receiver),确保消息被可靠发送。</li></ol><p>Flume使用两个独立的事务分别负责从source到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p><h4 id="1-3-Flume组成架构"><a href="#1-3-Flume组成架构" class="headerlink" title="1.3 Flume组成架构"></a>1.3 Flume组成架构</h4><p><img src="https://i.loli.net/2020/10/27/1kzlx3aqAoQXPts.png"><br><img src="https://i.loli.net/2020/10/27/xQqZr2XVidByMsw.png"></p><p>Channel是被动的，只能被put和take</p><p>​ 下面我们来详细介绍一下Flume架构中的组件</p><h5 id="1-3-1-Agent"><a href="#1-3-1-Agent" class="headerlink" title="1.3.1 Agent"></a>1.3.1 Agent</h5><p>​ Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p><p>​ Agent主要有3个部分组成，source，channel，sink</p><h5 id="1-3-2-Source"><a href="#1-3-2-Source" class="headerlink" title="1.3.2 Source"></a>1.3.2 Source</h5><p><font color="red">Source是负责接收数据到Flume Agent的组件。</font>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、<font color="red">exec</font>、jms、<font color="red">spooling directory</font>、netcat、sequence generator、syslog、http、legacy。</p><h5 id="1-3-3-Channel"><a href="#1-3-3-Channel" class="headerlink" title="1.3.3 Channel"></a>1.3.3 Channel</h5><p><font color="red">Channel是位于Source和Sink之间的缓冲区。</font>因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel</p><p><font color="red">Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。</font>如果需要关系数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p><p><font color="red">File Channel将所有事件写到磁盘。</font>因此在程序关闭或机器宕机的情况下不会丢失数据。</p><h5 id="1-3-4-Sink"><a href="#1-3-4-Sink" class="headerlink" title="1.3.4 Sink"></a>1.3.4 Sink</h5><p><font color="red">Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</font></p><p><font color="red">Sink是完全事务性的。</font>在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p><h5 id="1-3-5-Event"><a href="#1-3-5-Event" class="headerlink" title="1.3.5 Event"></a>1.3.5 Event</h5><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。Event由可选的header和载有数据的一个byte array构成。Header是容纳了key-value字符串对的HashMap</p><p><img src="https://i.loli.net/2020/10/27/q1BEiWyTMe5oQZf.png"></p><h4 id="1-4-Flume拓扑结构"><a href="#1-4-Flume拓扑结构" class="headerlink" title="1.4 Flume拓扑结构"></a>1.4 Flume拓扑结构</h4><p><img src="https://i.loli.net/2020/10/27/Gp8SyUnYzgBjNsQ.png"></p><p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量，flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p><p><img src="https://i.loli.net/2020/10/27/2hmk4axYNT3gRie.png"></p><p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送不同的目的地。</p><p><img src="https://i.loli.net/2020/10/27/zXwGm3ZHBsIWeKR.png"></p><p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p><p><img src="https://i.loli.net/2020/10/27/9265on7NXimGptc.png"></p><p>这种模式是我们最常见的，也是非常实用，日常web应用通常分布在上百个服务，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p><h4 id="1-5-Flume-Agent内部原理"><a href="#1-5-Flume-Agent内部原理" class="headerlink" title="1.5 Flume Agent内部原理"></a>1.5 Flume Agent内部原理</h4><p><img src="https://i.loli.net/2020/10/27/ALNnw4ZD3EUWci6.png"></p><h3 id="第二章-快速入门"><a href="#第二章-快速入门" class="headerlink" title="第二章 快速入门"></a>第二章 快速入门</h3><h4 id="2-1-Flume安装地址"><a href="#2-1-Flume安装地址" class="headerlink" title="2.1 Flume安装地址"></a>2.1 Flume安装地址</h4><ol><li><p>Flume官网地址</p><blockquote><p><a target="_blank" rel="noopener" href="http://flume.apache.org/">http://flume.apache.org/</a></p></blockquote></li><li><p>文档查看地址</p><blockquote><p><a target="_blank" rel="noopener" href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p></blockquote></li><li><p>下载地址</p><blockquote><p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></p></blockquote></li></ol><h4 id="2-2-安装部署"><a href="#2-2-安装部署" class="headerlink" title="2.2 安装部署"></a>2.2 安装部署</h4><ol><li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-flume-1.7.0-bin的名称为flume</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li><li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[atguigu@hadoop102 conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure></li></ol><h3 id="第三章-企业开发案例"><a href="#第三章-企业开发案例" class="headerlink" title="第三章 企业开发案例"></a>第三章 企业开发案例</h3><h4 id="3-1-监控端口数据官方案例（netcat）"><a href="#3-1-监控端口数据官方案例（netcat）" class="headerlink" title="3.1 监控端口数据官方案例（netcat）"></a>3.1 监控端口数据官方案例（netcat）</h4><ol><li><p>案例需求</p><p>（1）首先启动Flume任务，监控本机44444端口，服务器</p><p>（2）然后通过netcat工具向本机44444端口发送消息，客户端。</p><p>（3）最后Flume将监听的数据实时显示在控制台。</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/KQRVy8miANSlcOn.png"></p></li><li><p>实现步骤</p><p>（1）安装netcat工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure><p>（2）判断44444端口是否被占用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure><p>功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。</p><p>基本语法：netstat [选项]</p><p>选项参数：</p><p>​ -t或–tcp：显示TCP传输协议的连线状况；</p><p>​ -u或–udp：显示UDP传输协议的连线状况；</p><p>​ -n或–numeric：直接使用ip地址，而不通过域名服务器；</p><p>​ -l或–listening：显示监控中的服务器的Socket；</p><p>​ -p或–programs：显示正在使用Socket的程序识别码（PID）和程序名称；</p></li></ol><p>（3）创建Flume Agent配置文件flume-netcat-logger.conf</p><p>​ a.在flume目录下创建job文件夹并进入job文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir job</span><br><span class="line">[atguigu@hadoop102 flume]$ cd job/</span><br></pre></td></tr></table></figure><p>​ b.在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-netcat-logger.conf</span><br></pre></td></tr></table></figure><p>​ c.在flume-netcat-logger.conf文件中添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><p><font color="red">注：配置文件来源于官方手册</font><a target="_blank" rel="noopener" href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p><p><img src="https://i.loli.net/2020/10/27/w5gbOoZCneYFyRE.png"></p><p>​ <font color="red">注意：最后一行channel，说明一个sink只能绑定一个channel，但是一个channel可以绑定多个sink</font></p><p>（4）先开启flume监听端口</p><p>​ 第一种写法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>​ 第二种写法（简写）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>​ 参数说明（<font color="red">–XX表示参数，空格后面表示参数值</font>）：</p><p>​ –conf conf/ ：表示配置文件存储在conf/目录</p><p>​ –name a1 ：表示给agent起名为a1</p><p>​ –conf-file job/flume-netcat.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p><p>​ -Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p><p>（5）使用netcat工具向本机的44444端口发送内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">atguigu</span><br></pre></td></tr></table></figure><p>（6）在Flume监听页面观察接收数据情况</p><p><img src="https://i.loli.net/2020/10/27/pVhIxjlGvEPsKa5.png"></p><p><font color="red">nc hadoop102 44444 ,flume能否接收到</font></p><h4 id="3-2-实时读取本地文件到HDFS案例（exec）"><a href="#3-2-实时读取本地文件到HDFS案例（exec）" class="headerlink" title="3.2 实时读取本地文件到HDFS案例（exec）"></a>3.2 实时读取本地文件到HDFS案例（exec）</h4><ol><li><p>案例需求：实时监控Hive日志，并上传到HDFS中</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/VMhIEBTSuDaGzrl.png"></p></li></ol><ol start="3"><li><p>实现步骤</p><p>（1）Flume要想将数据输出到HDFS，必须持有Hadoop相关jar包</p><p>​ 将</p><p>​ commons-configuration-1.6.jar、</p><p>​ hadoop-auth-2.7.2.jar、</p><p>​ hadoop-common-2.7.2.jar、</p><p>​ hadoop-hdfs-2.7.2.jar、</p><p>​ commons-io-2.4.jar、</p><p>​ htrace-core-3.1.0-incubating.jar</p><p>​ 拷贝到/opt/module/flume/lib文件夹下。</p><p>（2）创建flume-file-hdfs.conf文件</p><p>​ 创建文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>​ 注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources &#x3D; r2</span><br><span class="line">a2.sinks &#x3D; k2</span><br><span class="line">a2.channels &#x3D; c2</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a2.sources.r2.type &#x3D; exec</span><br><span class="line">a2.sources.r2.command &#x3D; tail -F &#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs&#x2F;hive.log</span><br><span class="line">a2.sources.r2.shell &#x3D; &#x2F;bin&#x2F;bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type &#x3D; hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F;flume&#x2F;%Y%m%d&#x2F;%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix &#x3D; logs-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.round &#x3D; true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue &#x3D; 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit &#x3D; hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.batchSize &#x3D; 1000</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval &#x3D; 60</span><br><span class="line">#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollSize &#x3D; 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k2.hdfs.rollCount &#x3D; 0</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type &#x3D; memory</span><br><span class="line">a2.channels.c2.capacity &#x3D; 1000</span><br><span class="line">a2.channels.c2.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels &#x3D; c2</span><br><span class="line">a2.sinks.k2.channel &#x3D; c2</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>对于所有与时间相关的转义序列，Event Header中必须存在以”timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p><p><font color="red">a3.sinks.k3.hdfs.useLocalTimeStamp=true</font></p><p><img src="https://i.loli.net/2020/10/27/vQKeJ659tTcCqsL.png"></p><p>（3）执行监控配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>（4）开启Hadoop和Hive并操作Hive产生日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure><p>（5）在HDFS上查看文件</p><p>​ <img src="https://i.loli.net/2020/10/27/tYWwDTFCxQy7bM1.png"></p></li></ol><h4 id="3-3-实时读取目录文件到HDFS案例（spooldir）"><a href="#3-3-实时读取目录文件到HDFS案例（spooldir）" class="headerlink" title="3.3 实时读取目录文件到HDFS案例（spooldir）"></a>3.3 实时读取目录文件到HDFS案例（spooldir）</h4><ol><li><p>案例需求：使用Flume监听整个目录的文件</p></li><li><p>需求分析</p><p><img src="https://i.loli.net/2020/10/27/XzdJSZQjA2aBlwg.png"></p></li><li><p>实现步骤</p><p>（1）创建配置文件flume-dir-hdfs.conf</p><p>​ a.创建一个文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>​ b.打开文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>​ c.添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash">忽略所有以.tmp结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/t1FcnYeX6jvJOIx.png"></p><p>（2）启动监控文件夹命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p><font color="red">说明：在使用Spooling Directory Source时</font></p><p>a. 不要在监控目录中创建并持续修改文件(不能监控动态变化的文件)</p><p>b. 上传完成的文件会以.COMPLETED结尾（默认）</p><p>c. 被监控文件夹每500毫秒扫描一次文件变动</p><p>（3）向upload文件夹中添加文件</p><p>​ 在/opt/module/flume目录下创建upload文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir upload</span><br></pre></td></tr></table></figure><p>​ 向upload文件夹中添加文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.txt</span><br><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.tmp</span><br><span class="line">[atguigu@hadoop102 upload]$ touch atguigu.log</span><br></pre></td></tr></table></figure><p>（4）查看HDFS上的数据</p><p><img src="https://i.loli.net/2020/10/27/hBistWC9Gu5aFx6.png"></p><p>（5）等待1s，再次查询upload文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.tmp</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 5月  20 22:31 atguigu.txt.COMPLETED</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-3-实时监控目录下的多个追加文件（tailDir）"><a href="#3-3-实时监控目录下的多个追加文件（tailDir）" class="headerlink" title="3.3 实时监控目录下的多个追加文件（tailDir）"></a>3.3 实时监控目录下的多个追加文件（tailDir）</h4><p>​ Exec source适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir中source能保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而Taildir Source既能实现断点续传，又可以保证数据不丢失，还能够进行实时监控。</p><p><strong>案例需求</strong></p><p>使用Flume监听整个目录的实时追加文件，并上传至HDFS</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/iebAFwkufNgUISD.png"></p><p><strong>实现步骤</strong></p><p>创建配置文件taildir-flume-hdfs.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r1.filegroups &#x3D; f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;files&#x2F;file1.txt</span><br><span class="line">a1.sources.r1.filegroups.f2 &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;files&#x2F;file2.txt</span><br><span class="line">a1.sources.r1.positionFile &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;position&#x2F;position.json</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动监控命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flume-ng agent -c conf&#x2F; -n a1 -f job&#x2F;files-flume-logger.conf -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure><h4 id="3-4-单数据源多出口案例（选择器）"><a href="#3-4-单数据源多出口案例（选择器）" class="headerlink" title="3.4 单数据源多出口案例（选择器）"></a>3.4 单数据源多出口案例（选择器）</h4><p><img src="https://i.loli.net/2020/10/27/8fqLvsIX12VHimW.png"></p><p><strong>案例需求</strong></p><p>​ 使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到LocalFileSystem。</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/GlJQz5OHbW3I7FV.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group1文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ cd group1/</span><br></pre></td></tr></table></figure><p>在/opt/module/datas/目录下创建flume3文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ mkdir flume3</span><br></pre></td></tr></table></figure></li><li><p>创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-file-flume.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-file-flume.conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将数据流复制给所有channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink端的avro是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p><font color="red">注：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架</font></p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li></ol><ol start="3"><li><p>创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-flume-hdfs.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-flume-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>端的avro是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group1]$ touch flume-flume-dir.conf</span><br><span class="line">[atguigu@hadoop102 group1]$ vim flume-flume-dir.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p><font color="red">提示：</font>输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>启动Hadoop和Hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>检查HDFS上数据</p><p><img src="https://i.loli.net/2020/10/27/CWbqTAVaHB6ye7n.png"></p></li></ol><ol start="8"><li><p>检查/opt/module/datas/flume3目录中数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume3]$ ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 5942 5月  22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure></li></ol><h4 id="3-5-单数据源多出口案例（Sink组）（负载均衡和故障转移）"><a href="#3-5-单数据源多出口案例（Sink组）（负载均衡和故障转移）" class="headerlink" title="3.5 单数据源多出口案例（Sink组）（负载均衡和故障转移）"></a>3.5 单数据源多出口案例（Sink组）（负载均衡和故障转移）</h4><p>​ 单Source、Channel多Sink（负载均衡），如图</p><p>​ <img src="https://i.loli.net/2020/10/27/d4SW7w2nmeo83x5.png"></p><p><strong>案例需求</strong></p><p>​ 使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/YngpWLfw7h5IDUj.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group2文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ cd group2&#x2F;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建flume-netcat-flume</p><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-netcat-flume.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-netcat-flume.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line">/*这里是负载均衡，故障转移的type是failover*/</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p><font color="red">注</font>：Avro是由Hadoop创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li></ol><ol start="3"><li><p>创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台</p><p>创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-flume-console1.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-flume-console1.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume-flume-console2.conf</p><p>配置上机Flume输出的Source，输出是到本地控制台</p><p>创建配置文件并打开</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group2]$ touch flume-flume-console2.conf</span><br><span class="line">[atguigu@hadoop102 group2]$ vim flume-flume-console2.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>使用netcat工具向本机的4444端口发送内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 44444</span></span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li>查看Flume2及Flume3的控制台打印日志</li></ol><h4 id="3-6-多数据源汇总案例"><a href="#3-6-多数据源汇总案例" class="headerlink" title="3.6 多数据源汇总案例"></a>3.6 多数据源汇总案例</h4><p>多Source汇总数据到单Flume如图</p><p><img src="https://i.loli.net/2020/10/27/Cc5oJmHfTSNPyrA.png"></p><p><strong>案例需求</strong></p><p>​ hadoop103上的Flume-1监控文件/opt/module/group.log</p><p>​ hadoop102上的Flume-2监控某一个端口的数据流</p><p>​ Flume-1与Flume-2将数据发送给hadoop104上的Flume-3，Flume-3将最终数据打印到控制台。</p><p><strong>需求分析</strong></p><p><img src="https://i.loli.net/2020/10/27/DWFKlntXPGQjqA5.png"></p><p><strong>实现步骤</strong></p><ol><li><p>准备工作</p><p>分发Flume</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync flume</span><br></pre></td></tr></table></figure><p>在hadoop102、hadoop103以及hadoop104的/opt/module/flume/job目录下创建一个group3文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 job]$ mkdir group3</span><br><span class="line">[atguigu@hadoop103 job]$ mkdir group3</span><br><span class="line">[atguigu@hadoop104 job]$ mkdir group3</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>创建flume1-logger-flume.conf</p><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume</p><p>在hadoop103上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 group3]$ touch flume1-logger-flume.conf</span><br><span class="line">[atguigu@hadoop103 group3]$ vim flume1-logger-flume.conf </span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>创建flume2-netcat-flume.conf</p><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在hadoop102上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 group3]$ touch flume2-netcat-flume.conf</span><br><span class="line">[atguigu@hadoop102 group3]$ vim flume2-netcat-flume.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = hadoop102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = hadoop104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>创建flume3-flume-logger.conf</p><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在hadoop104上创建配置文件并打开</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 group3]$ touch flume3-flume-logger.conf</span><br><span class="line">[atguigu@hadoop104 group3]$ vim flume3-flume-logger.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = hadoop104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>执行配置文件</p><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>在hadoop103上向/opt/module目录下的group.log追加内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 module]$ echo &#x27;hello&#x27; &gt; group.log</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>在hadoop102上向44444端口发送数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ telnet hadoop102 44444</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>检查hadoop104上数据</p><p><img src="G:\截图\flume\3_6_2.png"></p></li></ol><h3 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h3><h4 id="4-1-Ganglia的安装与部署"><a href="#4-1-Ganglia的安装与部署" class="headerlink" title="4.1 Ganglia的安装与部署"></a>4.1 Ganglia的安装与部署</h4><ol><li><p>安装httpd服务与php</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install httpd php</span><br></pre></td></tr></table></figure></li><li><p>安装其他依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure></li><li><p>安装ganglia</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install ganglia-gmetad </span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum -y install ganglia-web</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo yum install -y ganglia-gmond</span><br></pre></td></tr></table></figure><p>Ganglia由gmond、gmetad和gweb三部分组成。</p><p><font color="red">gmond（Ganglia Monitoring Daemon）</font>是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等。</p><p><font color="red">gmetad（Ganglia Meta Daemon）</font>整合所有信息，并将其以RRD格式存储至磁盘的服务。</p><p><font color="red">gweb（Ganglia Web）</font>Ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。</p></li><li><p>修改配置文件/etc/httpd/conf.d/ganglia.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure><p><font color="red">修改为红颜色的配置：</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Ganglia monitoring system php web frontend</span><br><span class="line">Alias &#x2F;ganglia &#x2F;usr&#x2F;share&#x2F;ganglia</span><br><span class="line">&lt;Location &#x2F;ganglia&gt;</span><br><span class="line">  Order deny,allow</span><br><span class="line">  #Deny from all</span><br><span class="line">  Allow from all</span><br><span class="line">  # Allow from 127.0.0.1</span><br><span class="line">  # Allow from ::1</span><br><span class="line">  # Allow from .example.com</span><br><span class="line">&lt;&#x2F;Location&gt;</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>修改配置文件/etc/ganglia/gmetad.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/ganglia/gmetad.conf</span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_source &quot;hadoop102&quot; 192.168.1.102</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li><p>修改配置文件/etc/ganglia/gmond.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim &#x2F;etc&#x2F;ganglia&#x2F;gmond.conf </span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">  name = &quot;hadoop102&quot;</span><br><span class="line">  owner = &quot;unspecified&quot;</span><br><span class="line">  latlong = &quot;unspecified&quot;</span><br><span class="line">  url = &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">bind_hostname = yes <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line">                       # This option tells gmond to use a source address</span><br><span class="line">                       # that resolves to the machine&#x27;s hostname.  Without</span><br><span class="line">                       # this, the metrics may appear to come from any</span><br><span class="line">                       # interface and the DNS names associated with</span><br><span class="line">                       # those IPs will be used to create the RRDs.</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  host = 192.168.1.102</span><br><span class="line">  port = 8649</span><br><span class="line">  ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  port = 8649</span><br><span class="line">  bind = 192.168.1.102</span><br><span class="line">  retry_bind = true</span><br><span class="line"><span class="meta">  #</span><span class="bash"> Size of the UDP buffer. If you are handling lots of metrics you really</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> buffer = 10485760</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>修改配置文件/etc/selinux/config</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure><p><font color="red">修改为：</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure><p><font color="red">提示</font>：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo setenforce 0</span><br></pre></td></tr></table></figure></li></ol><ol start="8"><li><p>启动ganglia</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo service httpd start</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo service gmetad start</span><br><span class="line">[atguigu@hadoop102 flume]$ sudo service gmond start</span><br></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>打开网页浏览ganglia页面</p><p><a target="_blank" rel="noopener" href="http://192.168.1.102/ganglia">http://192.168.1.102/ganglia</a></p><p><font color="red">提示</font>：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ sudo chmod -R 777 &#x2F;var&#x2F;lib&#x2F;ganglia</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-2-操作Flume测试监控"><a href="#4-2-操作Flume测试监控" class="headerlink" title="4.2 操作Flume测试监控"></a>4.2 操作Flume测试监控</h4><ol><li><p>修改/opt/module/flume/conf目录下的flume-env.sh配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS&#x3D;&quot;-Dflume.monitoring.type&#x3D;ganglia</span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.1.102:8649</span><br><span class="line">-Xms100m</span><br><span class="line">-Xmx200m&quot;</span><br></pre></td></tr></table></figure></li><li><p>启动Flume任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin&#x2F;flume-ng agent \</span><br><span class="line">--conf conf&#x2F; \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf-file job&#x2F;flume-netcat-logger.conf \</span><br><span class="line">-Dflume.root.logger&#x3D;&#x3D;INFO,console \</span><br><span class="line">-Dflume.monitoring.type&#x3D;ganglia \</span><br><span class="line">-Dflume.monitoring.hosts&#x3D;192.168.1.102:8649</span><br></pre></td></tr></table></figure></li><li><p>发送数据观察ganglia监测图</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ nc localhost 44444</span><br></pre></td></tr></table></figure><p><strong>样式如图：</strong></p><p><img src="https://i.loli.net/2020/10/27/dBG5X83zocSWuOI.png"></p><p><strong>图例说明：</strong></p><table><thead><tr><th>字段（图表名称）</th><th>字段含义</th></tr></thead><tbody><tr><td>EventPutAttemptCount</td><td>source尝试写入channel的事件总数量</td></tr><tr><td>EventPutSuccessCount</td><td>成功写入channel且提交的事件总数量</td></tr><tr><td>EventTakeAttemptCount</td><td>sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。</td></tr><tr><td>EventTakeSuccessCount</td><td>sink成功读取的事件的总数量</td></tr><tr><td>StartTime</td><td>channel启动的时间（毫秒）</td></tr><tr><td>StopTime</td><td>channel停止的时间（毫秒）</td></tr><tr><td>ChannelSize</td><td>目前channel中事件的总数量</td></tr><tr><td>ChannelFillPercentage</td><td>channel占用百分比</td></tr><tr><td>ChannelCapacity</td><td>channel的容量</td></tr></tbody></table></li></ol><h3 id="第五章-自定义Interceptor"><a href="#第五章-自定义Interceptor" class="headerlink" title="第五章 自定义Interceptor"></a>第五章 自定义Interceptor</h3><h3 id="第六章-自定义Source"><a href="#第六章-自定义Source" class="headerlink" title="第六章 自定义Source"></a>第六章 自定义Source</h3><h4 id="6-1-介绍"><a href="#6-1-介绍" class="headerlink" title="6.1 介绍"></a>6.1 介绍</h4><p><font color="red">Source是负责接收数据到Flume Agent的组件。</font>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。</p><p>官方也提供了自定义source的接口：</p><p><a href="#source">https://flume.apache.org/FlumeDeveloperGuide.html#source</a>根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p><p>实现相应方法：</p><p><code>getBackOffSleepIncrement()</code>//暂不用</p><p><code>getMaxBackOffSleepInterval()</code>//暂不用</p><p><code>configure(Context context)</code>//初始化context（读取配置文件内容）</p><p><code>process()</code>//获取数据封装成event并写入channel，<font color="red">这个方法将被循环调用。</font></p><p>使用场景：读取MySQL数据或者其他文件系统。</p><h4 id="6-2-需求"><a href="#6-2-需求" class="headerlink" title="6.2 需求"></a>6.2 需求</h4><p>使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。</p><p><img src="https://i.loli.net/2020/10/27/HkaSKRlrtq4hbLo.png"></p><h4 id="6-3-分析"><a href="#6-3-分析" class="headerlink" title="6.3 分析"></a>6.3 分析</h4><p><img src="https://i.loli.net/2020/10/27/92KqrxYAdecPwDs.png"></p><h4 id="6-4-编码"><a href="#6-4-编码" class="headerlink" title="6.4 编码"></a>6.4 编码</h4><ol><li><p>导入pom依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>写java代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义配置文件将来要读取的字段</span></span><br><span class="line">    <span class="keyword">private</span> Long delay;</span><br><span class="line">    <span class="keyword">private</span> String field;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化配置信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        delay = context.getLong(<span class="string">&quot;delay&quot;</span>);</span><br><span class="line">        field = context.getString(<span class="string">&quot;field&quot;</span>, <span class="string">&quot;Hello!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建事件头信息</span></span><br><span class="line">            HashMap&lt;String, String&gt; hearderMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="comment">//创建事件</span></span><br><span class="line">            SimpleEvent event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">            <span class="comment">//循环封装事件</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                <span class="comment">//给事件设置头信息</span></span><br><span class="line">                event.setHeaders(hearderMap);</span><br><span class="line">                <span class="comment">//给事件设置内容</span></span><br><span class="line">                event.setBody((field + i).getBytes());</span><br><span class="line">                <span class="comment">//将事件写入channel</span></span><br><span class="line">                getChannelProcessor().processEvent(event);</span><br><span class="line">                Thread.sleep(delay);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-5-测试"><a href="#6-5-测试" class="headerlink" title="6.5 测试"></a>6.5 测试</h4><ol><li><p>打包</p><p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume)下。</p></li><li><p>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; com.atguigu.MySource</span><br><span class="line">a1.sources.r1.delay &#x3D; 1000</span><br><span class="line">#a1.sources.r1.field &#x3D; atguigu</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>开启任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ pwd</span><br><span class="line">/opt/module/flume</span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysource.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><p><img src="https://i.loli.net/2020/10/27/1uVWsDKybXSUk3B.png"></p></li></ol><h3 id="第七章-自定义Sink"><a href="#第七章-自定义Sink" class="headerlink" title="第七章 自定义Sink"></a>第七章 自定义Sink</h3><h4 id="7-1-介绍"><a href="#7-1-介绍" class="headerlink" title="7.1 介绍"></a>7.1 介绍</h4><p><font color="red">Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent.</font></p><p>Sink是完全事务性地。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写入到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。</p><p><font color="red">官方也提供了自定义source的接口:</font></p><p><a href="#sink">https://flume.apache.org/FlumeDeveloperGuide.html#sink</a>根据官方说明自定义MySink需要继承AbstractSink类并实现Configurable接口。</p><p>实现相应方法：</p><p><code>configure(Context context)</code>//初始化context（读取配置文件内容）</p><p><code>process()</code>//从Channel读取获取数据（event），这个方法将被循环调用。</p><p>使用场景：读取Channel数据写入MySQL或者其他文件系统。</p><h4 id="7-2-需求"><a href="#7-2-需求" class="headerlink" title="7.2 需求"></a>7.2 需求</h4><p>使用flume接收数据，并在Sink端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume任务配置文件中配置。</p><p>流程分析</p><p><img src="https://i.loli.net/2020/10/27/e4PE23m7rh9nKyg.png"></p><h4 id="7-3-编码"><a href="#7-3-编码" class="headerlink" title="7.3 编码"></a>7.3 编码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Logger对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(AbstractSink.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前Sink绑定的Channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取Channel中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> String(event.getBody()) + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefix&quot;</span>, <span class="string">&quot;hello:&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">&quot;suffix&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="7-4-测试"><a href="#7-4-测试" class="headerlink" title="7.4 测试"></a>7.4 测试</h4><ol><li><p>打包</p><p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p></li><li><p>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; com.atguigu.MySink</span><br><span class="line">#a1.sinks.k1.prefix &#x3D; atguigu:</span><br><span class="line">a1.sinks.k1.suffix &#x3D; :atguigu</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure></li><li><p>开启任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ pwd</span><br><span class="line">/opt/module/flume</span><br><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">[atguigu@hadoop102 ~]$ nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br><span class="line">atguigu</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><p><img src="https://i.loli.net/2020/10/27/ILe48pEPrfyzbus.png"></p></li></ol><h3 id="第八章-企业真实面试题（重点）"><a href="#第八章-企业真实面试题（重点）" class="headerlink" title="第八章 企业真实面试题（重点）"></a>第八章 企业真实面试题（重点）</h3><h4 id="8-1-你是如何实现Flume数据传输的监控的"><a href="#8-1-你是如何实现Flume数据传输的监控的" class="headerlink" title="8.1 你是如何实现Flume数据传输的监控的"></a>8.1 你是如何实现Flume数据传输的监控的</h4><p>​ 使用第三方框架Ganglia实时监控Flume</p><h4 id="8-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？"><a href="#8-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？" class="headerlink" title="8.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？"></a>8.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？</h4><p><strong>1.作用</strong></p><p>（1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</p><p>（2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中</p><p>（3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</p><p>2.我们公司采用的Source类型为：</p><p>（1）监控后台日志：exec</p><p>（2）监控后台产生日志的端口：netcat</p><p><font color="red">exec spooldir</font></p><h4 id="8-3-Flume的Channel-Selectors"><a href="#8-3-Flume的Channel-Selectors" class="headerlink" title="8.3 Flume的Channel Selectors"></a>8.3 Flume的Channel Selectors</h4><p><img src="https://i.loli.net/2020/10/27/b3ym6Jj5s7rlxTq.png"></p><h4 id="8-4-Flume参数调优"><a href="#8-4-Flume参数调优" class="headerlink" title="8.4 Flume参数调优"></a>8.4 Flume参数调优</h4><ol><li><strong>Source</strong></li></ol><p>增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</p><p>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</p><ol start="2"><li><strong>Channel</strong></li></ol><p>type 选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。</p><p>使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</p><p>Capacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</p><ol start="3"><li><strong>Sink</strong></li></ol><p>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</p><p>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</p><h4 id="8-5-Flume的事务机制"><a href="#8-5-Flume的事务机制" class="headerlink" title="8.5 Flume的事务机制"></a>8.5 Flume的事务机制</h4><p>Flume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。</p><h4 id="8-6-Flume采集数据会丢失吗"><a href="#8-6-Flume采集数据会丢失吗" class="headerlink" title="8.6 Flume采集数据会丢失吗"></a>8.6 Flume采集数据会丢失吗</h4><p>不会，Channel存储可以存储在File中，数据传输自身有事务。</p></div></article></div><div class="aside"><div class="box widget"><div class="introduction"><p><img src="/images/ironman.jpg" alt="head-sculpture"></p><p class="name">罗明辉Eric</p><p class="slogan">个人博客，分享经验，分享快乐</p></div></div><div class="box widget"><div class="title">最新</div><ul class="item-box"><li><a href="/2020/09/26/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1/">数据仓库-建模</a></li><li><a href="/2020/09/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/">数据仓库-架构</a></li><li><a href="/2020/09/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/">数据仓库-概述</a></li><li><a href="/2020/09/18/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a></li><li><a href="/2020/09/05/BigDataFrame/HDFS%E9%AB%98%E5%8F%AF%E7%94%A8/">HDFS高可用</a></li><li><a href="/2020/09/03/BigDataFrame/NM%E5%92%8CSNM%E5%92%8CDN/">NameNode|SecondaryNameNode|DataNode</a></li><li><a href="/2020/09/01/BigDataFrame/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/">HDFS读写流程</a></li><li><a href="/2020/07/28/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E8%B0%83%E4%BC%98/">SQL语句调优</a></li></ul></div><div class="box widget"><div class="title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BI%E5%B7%A5%E5%85%B7/">BI工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ETL%E5%B7%A5%E5%85%B7/">ETL工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div><div class="box widget"><div class="title">归档</div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">2020-09</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">2020-07</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">2020-06</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">2020-05</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">2020-01</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">2019-12</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">2019-07</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">2019-03</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">2018-09</a><span class="archive-list-count">1</span></li></ul></div></div></section><footer class="footer"><div class="global-width footer-box"><div class="copyright"><span>Copyright &copy; 2020</span> <span class="dotted">|</span> <span>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a></span> <span class="dotted">|</span></div></div></footer></div><script>hljs.initHighlightingOnLoad()</script></body></html>