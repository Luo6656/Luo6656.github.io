<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="author" content="Eric"><meta name="keywords" content=""><meta name="description" content="第七章 函数 / 第八章 压缩和存储 


第七章 函数7.1 系统内置函数
查看系统自带的函数
1show functions;

显示自带的函数的用法
1desc function upper;

详细显示自带的函数的用法
1desc function extended upper;


..."><meta name="Robots" content="all"><title>Eric个人博客 | Hive高阶</title><link rel="icon" href="/images/icon.svg"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/atom-one-dark.css"><link rel="stylesheet" href="/css/style.css"><script src="/js/highlight.min.js"></script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Eric个人博客" type="application/atom+xml"></head><body><div class="main-container"><header class="header"><div class="global-width"><nav class="nav-box"><a class="nav-item" href="/">主页</a> <a class="nav-item" href="/resume">项目</a> <a class="nav-item" href="/mood" target="_blank">热点观点</a> <a class="nav-item" href="/2018/01/01/introduce/" target="_blank">个人介绍</a> <a class="nav-item" href="/fuye.md">关于</a></nav></div></header><section class="content global-width"><div class="main"><article class="box post"><div class="post-title align-center detail-title">Hive高阶</div><div class="post-meta align-center"><span class="label">原创</span> <span class="dotted">|</span> <i class="fa fa-calendar"></i> <time>2020-07-05</time> <span class="dotted">|</span> <i class="fa fa-user"></i> Eric <span class="dotted">|</span> <i class="fa fa-folder-open-o"></i> <a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a></div><div class="post-content"><p>第七章 函数 / 第八章 压缩和存储</p><a id="more"></a><h3 id="第七章-函数"><a href="#第七章-函数" class="headerlink" title="第七章 函数"></a>第七章 函数</h3><h4 id="7-1-系统内置函数"><a href="#7-1-系统内置函数" class="headerlink" title="7.1 系统内置函数"></a>7.1 系统内置函数</h4><ol><li><p>查看系统自带的函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br></pre></td></tr></table></figure></li><li><p>显示自带的函数的用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function upper;</span><br></pre></td></tr></table></figure></li><li><p>详细显示自带的函数的用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function extended upper;</span><br></pre></td></tr></table></figure></li></ol><h4 id="7-2-自定义函数"><a href="#7-2-自定义函数" class="headerlink" title="7.2 自定义函数"></a>7.2 自定义函数</h4><ol><li><p>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</p></li><li><p>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户定义函数（UDF：user-defined function）</p></li><li><p>根据用户自定义函数类别分为以下三种 （多针对的是行）</p><p>（1）UDF（User-Defined-Function）</p><p>​ 一进一出</p><p>（2）UDAF（User-Defined Aggregation Function）</p><p>​ 聚集函数，多进一出</p><p>​ 类似于：count/max/min</p><p>（3）UDTF（User-Defined Table-Generating Functions）</p><p>​ 一进多出</p><p>​ 如lateral view explore()</p></li><li><p>官方文档地址</p><blockquote><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p></blockquote></li><li><p>编程步骤</p><p>（1）<font color="red">继承org.apache.hadoop.hive.ql.exec.UDF</font></p><p>（2）<font color="red">需要实现evaluate函数；evaluate函数支持重载；</font></p><p>（3）<font color="red">在hive的命令行窗口创建函数</font></p><p>​ a）<font color="red">添加jar</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure><p>​ b）<font color="red">创建function</font></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [<span class="keyword">temporary</span>] <span class="keyword">function</span> [dbname.]function_name <span class="keyword">AS</span> class_name;</span><br></pre></td></tr></table></figure><p>​ （4）在hive的命令行窗口删除函数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Drop</span> [<span class="keyword">temporary</span>] <span class="keyword">function</span> [<span class="keyword">if</span> <span class="keyword">exists</span>] [dbname.]function_name;</span><br></pre></td></tr></table></figure></li><li><p>注意事项</p><p>（1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</p></li></ol><h4 id="7-3-自定义UDF函数"><a href="#7-3-自定义UDF函数" class="headerlink" title="7.3 自定义UDF函数"></a>7.3 自定义UDF函数</h4><ol><li><p>创建一个Maven工程Hive</p></li><li><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>创建一个类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lower</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span> <span class="params">(<span class="keyword">final</span> String s)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> s.toLowerCase();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打包成jar包上传到服务器/opt/module/jars/udf.jar</p></li><li><p>将jar包添加到hive的classpath</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/datas/udf.jar;</span><br></pre></td></tr></table></figure></li><li><p>创建临时函数与开发好的Java class关联</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create temporary function mylower as &quot;com.atguigu.hive.Lower&quot;; </span><br><span class="line"><span class="comment">/*退出hive就不可用了*/</span></span><br></pre></td></tr></table></figure></li><li><p>即可在hql中使用自定义的函数strip</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename, mylower(ename) lowername from emp;</span><br></pre></td></tr></table></figure></li></ol><h3 id="第八章-压缩和存储"><a href="#第八章-压缩和存储" class="headerlink" title="第八章 压缩和存储"></a>第八章 压缩和存储</h3><h4 id="8-1-Hadoop源码编译支持Snappy压缩"><a href="#8-1-Hadoop源码编译支持Snappy压缩" class="headerlink" title="8.1 Hadoop源码编译支持Snappy压缩"></a>8.1 Hadoop源码编译支持Snappy压缩</h4><h5 id="8-1-1-资源准备"><a href="#8-1-1-资源准备" class="headerlink" title="8.1.1 资源准备"></a>8.1.1 资源准备</h5><ol><li><p>CentOS联网</p><p>配置CentOS能连接外网。Linux虚拟机ping <a target="_blank" rel="noopener" href="http://www.baidu.com是畅通的/">www.baidu.com是畅通的</a></p><p>注意：<font color="red">采用root角色编译</font>，减少文件夹权限出现问题</p></li><li><p>jar包准备(hadoop源码、JDK8、maven、protobuf)</p><p>（1）hadoop-2.7.2-src.tar.gz</p><p>（2）jdk-8u144-linux-x64.tar.gz</p><p>（3）snappy-1.1.3.tar.gz</p><p>（4）apache-maven-3.0.5-bin.tar.gz</p><p>（5）protobuf-2.5.0.tar.gz</p></li></ol><h5 id="8-1-2-jar包安装"><a href="#8-1-2-jar包安装" class="headerlink" title="8.1.2 jar包安装"></a>8.1.2 jar包安装</h5><p>​ <font color="red">注意：所有操作必须在root用户下完成</font></p><ol><li><p>JDK解压、配置环境变量JAVA_HOME和PATH，验证<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/javase">java</a>-version(如下都需要验证是否配置成功)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 software]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@hadoop101 software]#source /etc/profile</span><br></pre></td></tr></table></figure><p>验证命令：java -version</p></li><li><p>Maven解压、配置 MAVEN_HOME和PATH</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.0.5</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line">[root@hadoop101 software]#source /etc/profile</span><br></pre></td></tr></table></figure><p>验证命令：mvn -version</p></li></ol><h5 id="8-1-3-编译源码"><a href="#8-1-3-编译源码" class="headerlink" title="8.1.3 编译源码"></a>8.1.3 编译源码</h5><ol><li><p>准备编译环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# yum install svn</span><br><span class="line">[root@hadoop101 software]# yum install autoconf automake libtool cmake</span><br><span class="line">[root@hadoop101 software]# yum install ncurses-devel</span><br><span class="line">[root@hadoop101 software]# yum install openssl-devel</span><br><span class="line">[root@hadoop101 software]# yum install gcc*</span><br></pre></td></tr></table></figure></li><li><p>编译安装snappy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 module]# cd snappy-1.1.3/</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# ./configure</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# make</span><br><span class="line">[root@hadoop101 snappy-1.1.3]# make install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看snappy库文件</span></span><br><span class="line">[root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy</span><br></pre></td></tr></table></figure></li><li><p>编译安装protobuf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span><br><span class="line">[root@hadoop101 module]# cd protobuf-2.5.0/</span><br><span class="line">[root@hadoop101 protobuf-2.5.0]# ./configure </span><br><span class="line">[root@hadoop101 protobuf-2.5.0]#  make </span><br><span class="line">[root@hadoop101 protobuf-2.5.0]#  make install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看protobuf版本以测试是否安装成功</span></span><br><span class="line">[root@hadoop101 protobuf-2.5.0]# protoc --version</span><br></pre></td></tr></table></figure></li><li><p>编译hadoop native</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz</span><br><span class="line">[root@hadoop101 software]# cd hadoop-2.7.2-src/</span><br><span class="line">[root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy</span><br></pre></td></tr></table></figure><p>执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/hadoop">hadoop</a>-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。</p></li></ol><h4 id="8-2-Hadoop压缩配置"><a href="#8-2-Hadoop压缩配置" class="headerlink" title="8.2 Hadoop压缩配置"></a>8.2 Hadoop压缩配置</h4><h5 id="8-2-1-MR支持的压缩编码"><a href="#8-2-1-MR支持的压缩编码" class="headerlink" title="8.2.1 MR支持的压缩编码"></a>8.2.1 MR支持的压缩编码</h5><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFLATE</td><td>无</td><td>DEFLATE</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>lzop</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>无</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较：</p><p>表6-10</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table><p><a target="_blank" rel="noopener" href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p><h5 id="8-2-2-压缩参数配置"><a href="#8-2-2-压缩参数配置" class="headerlink" title="8.2.2 压缩参数配置"></a>8.2.2 压缩参数配置</h5><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs （在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><h4 id="8-3-开启Map输出阶段压缩"><a href="#8-3-开启Map输出阶段压缩" class="headerlink" title="8.3 开启Map输出阶段压缩"></a>8.3 开启Map输出阶段压缩</h4><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p><p><strong>案例实操</strong></p><ol><li><p>开启hive中间传输数据压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure></li><li><p>开启mapreduce中map输出压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce中map输出数据的压缩方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li><li><p>执行查询语句</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure></li></ol><h4 id="8-4-开启Reduce输出阶段压缩"><a href="#8-4-开启Reduce输出阶段压缩" class="headerlink" title="8.4 开启Reduce输出阶段压缩"></a>8.4 开启Reduce输出阶段压缩</h4><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p><p><strong>案例实操</strong></p><ol><li><p>开启hive最终输出数据压缩功能</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure></li><li><p>开启mapreduce最终输出数据压缩</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce最终数据输出压缩方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec =</span><br><span class="line"> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li><li><p>设置mapreduce最终数据输出压缩为块压缩</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure></li><li><p>测试一下输出结果是否是压缩文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/module/datas/distribute-result&#x27; <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="8-5-文件存储格式"><a href="#8-5-文件存储格式" class="headerlink" title="8.5 文件存储格式"></a>8.5 文件存储格式</h4><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h5 id="8-5-1-列式存储和行式存储"><a href="#8-5-1-列式存储和行式存储" class="headerlink" title="8.5.1 列式存储和行式存储"></a>8.5.1 列式存储和行式存储</h5><p><img src="https://i.loli.net/2020/10/27/oWu7lQi6reEbjKI.png"></p><p>如图6-10所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><p>1．行存储的特点</p><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><p>2．列存储的特点</p><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><p>​ <font color="red">TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</font></p><p>​ <font color="red">ORC和PARQUET是基于列式存储的。</font></p><h5 id="8-5-2-TextFile格式"><a href="#8-5-2-TextFile格式" class="headerlink" title="8.5.2 TextFile格式"></a>8.5.2 TextFile格式</h5><p>​ 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h5 id="8-5-3-Orc格式"><a href="#8-5-3-Orc格式" class="headerlink" title="8.5.3 Orc格式"></a>8.5.3 Orc格式</h5><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p><p>如图6-11所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p><p><img src="https://i.loli.net/2020/10/27/SZj1Bvfl3KchVGb.png"></p><p>​ 1）Index Data：一个轻量级的index，默认是<font color="red">每隔1W行做一个索引</font>。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p><pre><code>  2）Row Data：存的是具体的数据，&lt;font color=&quot;red&quot;&gt;先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储&lt;/font&gt;。

  3）Stripe Footer：存的是各个Stream的类型，长度等信息。</code></pre><p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h5 id="8-5-4-Parquet格式"><a href="#8-5-4-Parquet格式" class="headerlink" title="8.5.4 Parquet格式"></a>8.5.4 Parquet格式</h5><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，<font color="red">因此Parquet格式文件是自解析的。</font></p><ol><li><p>行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p></li><li><p>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p></li><li><p>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p></li></ol><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把<font color="red">每一个行组由一个Mapper任务处理，增大任务执行并行度。</font>Parquet文件的格式如图6-12所示。</p><p><img src="https://i.loli.net/2020/10/27/h7MIWijkGQXaut1.png"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h5 id="8-5-5-主流文件存储格式对比实验"><a href="#8-5-5-主流文件存储格式对比实验" class="headerlink" title="8.5.5 主流文件存储格式对比实验"></a>8.5.5 主流文件存储格式对比实验</h5><p>从存储文件的压缩比和查询速度两个角度对比。</p><p><strong>存储文件的压缩比测试：</strong></p><ol><li><p>测试数据</p><p>log.data</p></li><li><p>TextFile</p><p>(1)创建表，存储数据格式为TEXTFILE</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/log.data&#x27; into table log_text ;</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;</span><br></pre></td></tr></table></figure><p>18.1 M /user/hive/warehouse/log_text/log.data</p></li><li><p>ORC（ORC格式默认开启压缩）</p><p>(1)创建表，存储数据格式为ORC</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc ;</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc select * from log_text ; /*不能使用load，因为load是直接put的*/</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/;</span><br><span class="line">2.8 M  /user/hive/warehouse/log_orc/000000_0</span><br></pre></td></tr></table></figure></li><li><p>Parquet</p><p>(1)创建表，存储数据格式为parquet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet ;	</span><br></pre></td></tr></table></figure><p>(2)向表中加载数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure><p>(3)查看表中数据大小</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;</span><br><span class="line">13.1 M /user/hive/warehouse/log_parquet/000000_0</span><br></pre></td></tr></table></figure><p>存储文件的压缩比总结：</p><p>ORC &gt; Parquet &gt; textFile</p></li></ol><p><strong>存储文件的查询速度测试：</strong></p><p>TextFile</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_text;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 21.54 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 21.08 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 19.298 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>ORC</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_orc;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 20.867 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 22.667 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 18.36 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>Parquet</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from log_parquet;</span><br><span class="line">_c0</span><br><span class="line">100000</span><br><span class="line">Time taken: 22.922 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 21.074 seconds, Fetched: 1 row(s)</span><br><span class="line">Time taken: 18.384 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>存储文件的查询速度总结：查询速度相近。</p><h4 id="8-6-存储和压缩结合"><a href="#8-6-存储和压缩结合" class="headerlink" title="8.6 存储和压缩结合"></a>8.6 存储和压缩结合</h4><h5 id="8-6-1-修改Hadoop集群具有Snappy压缩方法"><a href="#8-6-1-修改Hadoop集群具有Snappy压缩方法" class="headerlink" title="8.6.1 修改Hadoop集群具有Snappy压缩方法"></a>8.6.1 修改Hadoop集群具有Snappy压缩方法</h5><ol><li><p>查看hadoop checknative命令使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 hadoop-2.7.2]$ hadoop</span><br><span class="line">  		checknative [-a|-h]  check native hadoop and compression libraries availability</span><br></pre></td></tr></table></figure></li><li><p>查看hadoop支持的压缩方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">	[atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative</span><br><span class="line">17&#x2F;12&#x2F;24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load&#x2F;initialize native-bzip2 library system-native, will use pure-Java version</span><br><span class="line">17&#x2F;12&#x2F;24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libhadoop.so</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  false </span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   false</span><br></pre></td></tr></table></figure></li><li><p>将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中</p></li></ol><ol start="4"><li><p>解压hadoop-2.7.2.tar.gz到当前路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 native]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;software&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native</span><br><span class="line">[atguigu@hadoop102 native]$ ll</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  472950 9月   1 10:19 libsnappy.a</span><br><span class="line">-rwxr-xr-x. 1 atguigu atguigu     955 9月   1 10:19 libsnappy.la</span><br><span class="line">lrwxrwxrwx. 1 atguigu atguigu      18 12月 24 20:39 libsnappy.so -&gt; libsnappy.so.1.3.0</span><br><span class="line">lrwxrwxrwx. 1 atguigu atguigu      18 12月 24 20:39 libsnappy.so.1 -&gt; libsnappy.so.1.3.0</span><br><span class="line">-rwxr-xr-x. 1 atguigu atguigu  228177 9月   1 10:19 libsnappy.so.1.3.0</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 native]$ cp ..&#x2F;native&#x2F;* &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>分发集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$ xsync native&#x2F;</span><br></pre></td></tr></table></figure></li><li><p>再次查看hadoop支持的压缩类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ hadoop checknative</span><br><span class="line">17&#x2F;12&#x2F;24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load&#x2F;initialize native-bzip2 library system-native, will use pure-Java version</span><br><span class="line">17&#x2F;12&#x2F;24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libhadoop.so</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  true &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;lib&#x2F;native&#x2F;libsnappy.so.1</span><br><span class="line">lz4:     true revision:99</span><br><span class="line">bzip2:   false</span><br></pre></td></tr></table></figure></li><li><p>重新启动hadoop集群和hive</p></li></ol><h5 id="8-6-2-测试存储和压缩"><a href="#8-6-2-测试存储和压缩" class="headerlink" title="8.6.2 测试存储和压缩"></a>8.6.2 测试存储和压缩</h5><p>官网：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p><p>ORC存储方式的压缩：</p><table><thead><tr><th>Key</th><th>Default</th><th>Notes</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>high level compression (one of NONE, ZLIB, SNAPPY)</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>number of bytes in each compression chunk</td></tr><tr><td>orc.stripe.size</td><td>268,435,456</td><td>number of bytes in each stripe</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>number of rows between index entries (must be &gt;= 1000)</td></tr><tr><td>orc.create.index</td><td>true</td><td>whether to create row indexes</td></tr><tr><td>orc.bloom.filter.columns</td><td>“”</td><td>comma separated list of column names for which bloom filter should be created</td></tr><tr><td>orc.bloom.filter.fpp</td><td>0.05</td><td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td></tr></tbody></table><p>注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现</p><ol><li><p>创建一个非压缩的ORC存储方式</p><p>（1）建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">&quot;orc.compress&quot;</span>=<span class="string">&quot;NONE&quot;</span>);</span><br></pre></td></tr></table></figure><p>（2）插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc_none select * from log_text ;</span><br></pre></td></tr></table></figure><p>（3）查看插入后数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ;</span><br><span class="line">7.7 M  /user/hive/warehouse/log_orc_none/000000_0</span><br></pre></td></tr></table></figure></li><li><p>创建一个SNAPPY压缩的ORC存储方式</p><p>（1）建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">&quot;orc.compress&quot;</span>=<span class="string">&quot;SNAPPY&quot;</span>);</span><br></pre></td></tr></table></figure><p>（2）插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc_snappy select * from log_text ;</span><br></pre></td></tr></table></figure><p>（3）查看插入后数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ;</span><br></pre></td></tr></table></figure><p>3.8 M /user/hive/warehouse/log_orc_snappy/000000_0</p></li><li><p>上一节中默认创建的ORC存储方式，导入数据后的大小为2.8 M /user/hive/warehouse/log_orc/000000_0</p><p>比Snappy压缩的还小。原因是orc存储文件默认采用<font color="red">ZLIB压缩</font>，ZLIB采用的是deflate压缩算法。比snappy压缩的小。</p></li><li><p>存储方式和压缩总结</p><p>在实际的项目开发当中，hive表的数据存储格式一般选择:orc或parquet。压缩方式一般选择snappy，lzo</p></li></ol><h3 id="第九章-企业级调优"><a href="#第九章-企业级调优" class="headerlink" title="第九章 企业级调优"></a>第九章 企业级调优</h3><h4 id="9-1-Fetch抓取"><a href="#9-1-Fetch抓取" class="headerlink" title="9.1 Fetch抓取"></a>9.1 Fetch抓取</h4><p>Fetch抓取是指，<font color="red">Hive中对某些情况的查询可以不必使用MapReduce计算</font>。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是<font color="red">minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce</font>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;more&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some <span class="keyword">select</span> queries can be converted <span class="keyword">to</span> single <span class="keyword">FETCH</span> task minimizing latency.</span><br><span class="line">      Currently the <span class="keyword">query</span> should be single sourced <span class="keyword">not</span> <span class="keyword">having</span> <span class="keyword">any</span> subquery <span class="keyword">and</span> should <span class="keyword">not</span> have <span class="keyword">any</span> aggregations <span class="keyword">or</span> distincts (which incurs RS), <span class="keyword">lateral</span> views <span class="keyword">and</span> joins.</span><br><span class="line">      <span class="number">0.</span> <span class="keyword">none</span> : <span class="keyword">disable</span> hive.fetch.task.conversion</span><br><span class="line">      <span class="number">1.</span> minimal : <span class="keyword">SELECT</span> STAR, FILTER <span class="keyword">on</span> <span class="keyword">partition</span> <span class="keyword">columns</span>, <span class="keyword">LIMIT</span> <span class="keyword">only</span></span><br><span class="line">      <span class="number">2.</span> more  : <span class="keyword">SELECT</span>, FILTER, <span class="keyword">LIMIT</span> <span class="keyword">only</span> (support <span class="keyword">TABLESAMPLE</span> <span class="keyword">and</span> <span class="keyword">virtual</span> <span class="keyword">columns</span>)</span><br><span class="line">    &lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><p><strong>案例实操：</strong></p><p>​ 1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.fetch.task.conversion=none;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp limit 3;</span><br></pre></td></tr></table></figure><p>​ 2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.fetch.task.conversion=more;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp limit 3;</span><br></pre></td></tr></table></figure><h4 id="9-2-本地模式"><a href="#9-2-本地模式" class="headerlink" title="9.2 本地模式"></a>9.2 本地模式</h4><p>​ 大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，<font color="red">Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</font></p><p>​ 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化，默认是false。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  //开启本地mr</span><br><span class="line">//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line">//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max=<span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><strong>案例实操：</strong></p><p>1）开启本地模式，并执行查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.exec.mode.local.auto=true; </span><br><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">Time taken: 1.328 seconds, Fetched: 14 row(s)</span><br></pre></td></tr></table></figure><p>2）关闭本地模式，并执行查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.exec.mode.local.auto=false; </span><br><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">Time taken: 20.09 seconds, Fetched: 14 row(s)</span><br></pre></td></tr></table></figure><h4 id="9-3-表的优化"><a href="#9-3-表的优化" class="headerlink" title="9.3 表的优化"></a>9.3 表的优化</h4><h5 id="9-3-1-小表、大表Join"><a href="#9-3-1-小表、大表Join" class="headerlink" title="9.3.1 小表、大表Join"></a>9.3.1 小表、大表Join</h5><p>​ 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p>​ <font color="red">实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</font></p><p><strong>案例实操</strong></p><ol><li><p>需求</p><p>测试大表join小表和小表join大表的效率</p></li><li><p>建大表、小表和JOIN后表的语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建大表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建小表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建join后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>分别向大表和小表中导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/bigtable&#x27; into table bigtable;</span><br><span class="line">hive (default)&gt;load data local inpath &#x27;/opt/module/datas/smalltable&#x27; into table smalltable;</span><br></pre></td></tr></table></figure></li><li><p>关闭mapjoin功能（默认是打开的）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">false</span>;</span><br></pre></td></tr></table></figure></li><li><p>执行小表JOIN大表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable  b</span><br><span class="line"><span class="keyword">on</span> b.id = s.id;</span><br></pre></td></tr></table></figure><p>Time taken: 35.921 seconds</p><p>No rows affected (44.456 seconds)</p></li><li><p>执行大表JOIN小表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> smalltable  s</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 34.196 seconds</p><p>No rows affected (26.287 seconds)</p></li></ol><h5 id="9-3-2-大表join大表"><a href="#9-3-2-大表join大表" class="headerlink" title="9.3.2 大表join大表"></a>9.3.2 大表join大表</h5><ol><li><p>空key过滤</p><p>​ 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p><p><strong>案例实操</strong></p><p>（1）配置历史服务器</p><p>配置mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><p>查看jobhistroy</p><p><a target="_blank" rel="noopener" href="http://hadoop102:19888/jobhistory">http://hadoop102:19888/jobhistory</a></p><p>（2）创建原始数据表、空id表、合并后数据表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 创建原始表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建空id表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> nullidtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">// 创建join后表的语句</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword <span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）分别加载原始数据和空id数据到对应表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/ori&#x27; into table ori;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/nullid&#x27; into table nullidtable;</span><br></pre></td></tr></table></figure><p>（4）测试不过滤空id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table jointable select n.* from nullidtable n</span><br><span class="line">left join ori o on n.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 42.038 seconds</p><p>Time taken: 37.284 seconds</p><p>（5）测试过滤空id</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table jointable select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 31.725 seconds</p><p>Time taken: 28.876 seconds</p></li><li><p>空key转换</p><p>​ 有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如：</p><p><strong>案例实操</strong>：</p><p>不随机分布空null值：</p><p>（1）设置5个reduce个数</p><p>​ set mapreduce.job.reduces = 5;</p><p>（2）join两张表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> ori b <span class="keyword">on</span> n.id = b.id;</span><br></pre></td></tr></table></figure><p><strong>结果：可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</strong></p><p><img src="https://i.loli.net/2020/10/27/pd6UjBXHtNvPcem.png"></p><p>（1）设置分布空null值</p><p>​ set mapreduce.job.reduces = 5;</p><p>（2）JOIN两张表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">&#x27;hive&#x27;</span>, <span class="keyword">rand</span>()) <span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span><br></pre></td></tr></table></figure><p><strong>结果：如图6-14所示，可以看出来，消除了数据倾斜，负载均衡reducer的资源消耗</strong></p><p><img src="https://i.loli.net/2020/10/27/OlcrFoHfz28nY5m.png"></p></li></ol><h5 id="9-3-3-MapJoin-小表join大表"><a href="#9-3-3-MapJoin-小表join大表" class="headerlink" title="9.3.3 MapJoin(小表join大表)"></a>9.3.3 MapJoin(小表join大表)</h5><p>​ 如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p><ol><li><p>开启MapJoin参数设置</p><p>（1）设置自动选择mapjoin</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; <span class="comment">/*默认为true*/</span></span><br></pre></td></tr></table></figure><p>（2）大表小表的阈值设置（默认25M一下认为是小表）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>MapJoin工作机制，如下图</p><p><img src="https://i.loli.net/2020/10/27/wNUjZQpnEmACX26.png"></p></li></ol><p><strong>案例实操</strong></p><p>​ （1）开启Mapjoin功能</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>; 默认为true</span><br></pre></td></tr></table></figure><p>​ （2）执行小表join大表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">join</span> bigtable  b</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 24.594 seconds</p><p>​ （3）执行大表join小表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line"><span class="keyword">join</span> smalltable  s</span><br><span class="line"><span class="keyword">on</span> s.id = b.id;</span><br></pre></td></tr></table></figure><p>Time taken: 24.315 seconds</p><h5 id="9-3-4-Group-By"><a href="#9-3-4-Group-By" class="headerlink" title="9.3.4 Group By"></a>9.3.4 Group By</h5><p><font color="red">默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了</font>。</p><p><img src="https://i.loli.net/2020/10/27/S6WBaLj18eMoAtw.png"></p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><p><strong>开启Map端聚合参数设置</strong></p><ol><li><p>是否在Map端进行聚合，默认为true</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>在Map端进行聚合操作的条目数目</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval = <span class="number">100000</span></span><br></pre></td></tr></table></figure></li><li><p>有数据倾斜的时候进行负载均衡（默认是false）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select deptno from emp group by deptno;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 23.68 sec   HDFS Read: 19987 HDFS Write: 9 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 23 seconds 680 msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line">10</span><br><span class="line">20</span><br><span class="line">30</span><br></pre></td></tr></table></figure><p>优化之后</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set hive.groupby.skewindata = true;</span><br><span class="line">hive (default)&gt; select deptno from emp group by deptno;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 28.53 sec   HDFS Read: 18209 HDFS Write: 534 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 1  Reduce: 5   Cumulative CPU: 38.32 sec   HDFS Read: 15014 HDFS Write: 9 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 minutes 6 seconds 850 msec</span><br><span class="line">OK</span><br><span class="line">deptno</span><br><span class="line">10</span><br><span class="line">20</span><br><span class="line">30</span><br></pre></td></tr></table></figure></li></ol><h5 id="9-3-5-Count-distinct-去重统计—-会内存溢出"><a href="#9-3-5-Count-distinct-去重统计—-会内存溢出" class="headerlink" title="9.3.5 Count(distinct) 去重统计—-会内存溢出"></a>9.3.5 Count(distinct) 去重统计—-会内存溢出</h5><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT的全聚合操作，即使设定了reduce task个数，set mapred.reduce.tasks=100；hive也只会启动一个reducer。，这就造成一个Reduce处理的数据量太大，导致整个Job很难完成，<font color="red">一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</font></p><p><strong>案例实操</strong></p><ol><li><p>创建一张大表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table bigtable(id bigint, time bigint, uid string, keyword</span><br><span class="line">string, url_rank int, click_num int, click_url string) row format delimited</span><br><span class="line">fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>加载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/bigtable&#x27; into table</span><br><span class="line"> bigtable;</span><br></pre></td></tr></table></figure></li><li><p>设置5个reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces = <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li><li><p>执行去重id查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(distinct id) from bigtable;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.12 sec   HDFS Read: 120741990 HDFS Write: 7 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 7 seconds 120 msec</span><br><span class="line">OK</span><br><span class="line">c0</span><br><span class="line">100001</span><br><span class="line">Time taken: 23.607 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li><li><p>采用group by去重id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(id) from (select id from bigtable group by id) a;</span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 17.53 sec   HDFS Read: 120752703 HDFS Write: 580 SUCCESS</span><br><span class="line">Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.29 sec   HDFS Read: 9409 HDFS Write: 7 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 21 seconds 820 msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">100001</span><br><span class="line">Time taken: 50.795 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p></li></ol><h5 id="9-3-6-笛卡尔积"><a href="#9-3-6-笛卡尔积" class="headerlink" title="9.3.6 笛卡尔积"></a>9.3.6 笛卡尔积</h5><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h5 id="9-3-7-行列过滤"><a href="#9-3-7-行列过滤" class="headerlink" title="9.3.7 行列过滤"></a>9.3.7 行列过滤</h5><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</p><p><strong>案例实操：</strong></p><ol><li><p>测试先关联两张表，再用where条件过滤</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select o.id from bigtable b</span><br><span class="line">join ori o on o.id = b.id</span><br><span class="line">where o.id &lt;= 10;</span><br></pre></td></tr></table></figure><p>Time taken: 34.406 seconds, Fetched: 100 row(s)</p></li><li><p>通过子查询后，再关联表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select b.id from bigtable b</span><br><span class="line">join (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> ori <span class="keyword">where</span> <span class="keyword">id</span> &lt;= <span class="number">10</span> ) o <span class="keyword">on</span> b.id = o.id;</span><br></pre></td></tr></table></figure><p>Time taken: 30.058 seconds, Fetched: 100 row(s)</p></li></ol><h5 id="9-3-8-动态分区调整"><a href="#9-3-8-动态分区调整" class="headerlink" title="9.3.8 动态分区调整"></a>9.3.8 动态分区调整</h5><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p><ol><li><p>开启动态分区参数设置</p><p>(1)开启动态分区功能（默认true，开启）</p><p><font color="red">hive.exec.dynamic.partition=true</font></p><p>(2)设置为非严格模式（动态分区的模式，默认strict,表示必须指定一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区）</p><p>注：在严格模式下，插入数据必须指定一个分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode&#x3D;nonstrict</span><br></pre></td></tr></table></figure><p>(3)在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure><p>(4)在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。与上面哪个设置成一样大。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode&#x3D;100</span><br></pre></td></tr></table></figure><p>(5)整个MR job中，最大可以创建多少个HDFS文件，默认100000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files&#x3D;100000</span><br></pre></td></tr></table></figure><p>(6)当有空分区生成时，是否抛出异常。一般不需要设置。默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition&#x3D;false</span><br></pre></td></tr></table></figure></li><li><p><strong>案例实操</strong></p><p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p><p>(1)创建目标分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(id int, name string) partitioned</span><br><span class="line">by (location int) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>(2)设置动态分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">hive (default)&gt; insert into table dept_partition partition(location) select deptno, dname, loc from dept; /*最后一个字段loc来的*/</span><br></pre></td></tr></table></figure><p>(3)查看目标分区表的分区情况</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure><p><font color="red">思考：目标分区表是如何匹配到分区字段的？</font></p></li></ol><h5 id="9-3-9-分桶"><a href="#9-3-9-分桶" class="headerlink" title="9.3.9 分桶"></a>9.3.9 分桶</h5><p>详见6.6章。</p><h5 id="9-3-10-分区"><a href="#9-3-10-分区" class="headerlink" title="9.3.10 分区"></a>9.3.10 分区</h5><p>详见4.6章。</p><h4 id="9-4-MR优化"><a href="#9-4-MR优化" class="headerlink" title="9.4 MR优化"></a>9.4 MR优化</h4><h5 id="9-4-1-合理设置Map数"><a href="#9-4-1-合理设置Map数" class="headerlink" title="9.4.1 合理设置Map数"></a>9.4.1 合理设置Map数</h5><ol><li><p>通常情况下，作业会通过input的目录产生一个或者多个map任务</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p></li><li><p>是不是map数越多越好</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p></li><li><p>是不是保证每个map处理接近128m的文件块，就高枕无忧了?</p><p>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</p></li></ol><h5 id="9-4-1-复杂文件增加map数"><a href="#9-4-1-复杂文件增加map数" class="headerlink" title="9.4.1 复杂文件增加map数"></a>9.4.1 复杂文件增加map数</h5><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p><strong>案例实操</strong></p><ol><li><p>执行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) from emp;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br></pre></td></tr></table></figure></li><li><p>设置最大切片值为100个字节</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100;</span><br><span class="line">hive (default)&gt; select count(*) from emp;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</span><br></pre></td></tr></table></figure></li></ol><h5 id="9-4-2-小文件进行合并"><a href="#9-4-2-小文件进行合并" class="headerlink" title="9.4.2 小文件进行合并"></a>9.4.2 小文件进行合并</h5><p>（1）在map执行前合并小文件，减少map数： CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><p>（2）在Map-Reduce的任务结束时合并小文件的设置：</p><p>在map-only任务结束时合并小文件，默认true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.mapfiles &#x3D; true;</span><br></pre></td></tr></table></figure><p>在map-reduce任务结束时合并小文件，默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.mapredfiles &#x3D; true;</span><br></pre></td></tr></table></figure><p>合并文件的大小，默认256M</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.size.per.task &#x3D; 268435456;</span><br></pre></td></tr></table></figure><p>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET hive.merge.smallfiles.avgsize &#x3D; 16777216;</span><br></pre></td></tr></table></figure><h5 id="9-4-3-合理设置Reduce数"><a href="#9-4-3-合理设置Reduce数" class="headerlink" title="9.4.3 合理设置Reduce数"></a>9.4.3 合理设置Reduce数</h5><p>1．调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer&#x3D;256000000</span><br></pre></td></tr></table></figure><p>（2）每个任务最大的reduce数，默认为1009</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.reducers.max&#x3D;1009</span><br></pre></td></tr></table></figure><p>（3）计算reducer数的公式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N&#x3D;min(参数2，总输入数据量&#x2F;参数1)</span><br></pre></td></tr></table></figure><p>2．调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces &#x3D; 15;</span><br></pre></td></tr></table></figure><p>3．reduce个数并不是越多越好</p><p>1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：<font color="red">处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</font></p><h4 id="9-5-并行执行"><a href="#9-5-并行执行" class="headerlink" title="9.5 并行执行"></a>9.5 并行执行</h4><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;              //打开任务并行执行</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>;  //同一个sql允许最大并行度，默认为8。</span><br></pre></td></tr></table></figure><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h4 id="9-6-严格模式"><a href="#9-6-严格模式" class="headerlink" title="9.6 严格模式"></a>9.6 严格模式</h4><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p><p>通过设置属性hive.mapred.mode值为默认是非严格模式<font color="red">nonstrict</font> 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The mode in which the Hive operations are being performed. </span><br><span class="line">      In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">        Cartesian Product.</span><br><span class="line">        No partition being picked up for a query.</span><br><span class="line">        Comparing bigints and strings.</span><br><span class="line">        Comparing bigints and doubles.</span><br><span class="line">        Orderby without limit.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li><p>对于分区表，<font color="red">除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。</font>换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p></li><li><p>对于<font color="red">使用了order by语句的查询，要求必须使用limit语句。</font>因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p></li><li><p><font color="red">限制笛卡尔积的查询</font>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p></li></ol><h4 id="9-7-JVM重用"><a href="#9-7-JVM重用" class="headerlink" title="9.7 JVM重用"></a>9.7 JVM重用</h4><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;mapreduce.job.jvm.numtasks&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;10&lt;&#x2F;value&gt;</span><br><span class="line"> &lt;description&gt;How many tasks to run per jvm. If set to -1, there is</span><br><span class="line"> no limit. </span><br><span class="line"> &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h4 id="9-8-推测执行"><a href="#9-8-推测执行" class="headerlink" title="9.8 推测执行"></a>9.8 推测执行</h4><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置，默认是true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>不过hive本身也提供了配置项来控制reduce-side的推测执行：默认是true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h4 id="9-9-压缩"><a href="#9-9-压缩" class="headerlink" title="9.9 压缩"></a>9.9 压缩</h4><p>详见第8章</p><h4 id="9-10-执行计划（Explain）"><a href="#9-10-执行计划（Explain）" class="headerlink" title="9.10 执行计划（Explain）"></a>9.10 执行计划（Explain）</h4><p>1．基本语法</p><p>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</p><p>2．案例实操</p><p>（1）查看下面这条语句的执行计划</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; explain select * from emp;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure><p>（2）查看详细执行计划</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; explain extended select * from emp;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure><h3 id="第十章-Hive实战之谷粒影音"><a href="#第十章-Hive实战之谷粒影音" class="headerlink" title="第十章 Hive实战之谷粒影音"></a>第十章 Hive实战之谷粒影音</h3><h4 id="10-1-需求描述"><a href="#10-1-需求描述" class="headerlink" title="10.1 需求描述"></a>10.1 需求描述</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><p>–统计视频观看数Top10</p><p>–统计视频类别热度Top10</p><p>–统计视频观看数Top20所属类别以及类别包含的Top20的视频个数</p><p>–统计视频观看数Top50所关联视频的所属类别Rank</p><p>–统计每个类别中的视频热度Top10</p><p>–统计每个类别中视频流量Top10</p><p>–统计上传视频最多的用户Top10以及他们上传的观看次数前20视频</p><p>–统计每个类别视频观看数Top10</p><h4 id="10-2-项目"><a href="#10-2-项目" class="headerlink" title="10.2 项目"></a>10.2 项目</h4><h5 id="10-2-1-数据结构"><a href="#10-2-1-数据结构" class="headerlink" title="10.2.1 数据结构"></a>10.2.1 数据结构</h5><ol><li><p>视频表</p><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>video id</td><td>视频唯一id</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别</td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分</td><td>满分5分</td></tr><tr><td>Ratings</td><td>流量</td><td>视频的流量，整型数字</td></tr><tr><td>conments</td><td>评论数</td><td>一个视频的整数评论数</td></tr><tr><td>related ids</td><td>相关视频id</td><td>相关视频的id，最多20个</td></tr></tbody></table></li><li><p>用户表</p><table><thead><tr><th>字段</th><th>备注</th><th>字段类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table></li></ol><h5 id="10-2-2-ETL原始数据"><a href="#10-2-2-ETL原始数据" class="headerlink" title="10.2.2 ETL原始数据"></a>10.2.2 ETL原始数据</h5></div></article></div><div class="aside"><div class="box widget"><div class="introduction"><p><img src="/images/ironman.jpg" alt="head-sculpture"></p><p class="name">罗明辉Eric</p><p class="slogan">个人博客，分享经验，分享快乐</p></div></div><div class="box widget"><div class="title">最新</div><ul class="item-box"><li><a href="/2020/09/18/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a></li><li><a href="/2020/07/28/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E8%AF%AD%E5%8F%A5%E8%B0%83%E4%BC%98/">SQL语句调优</a></li><li><a href="/2020/07/26/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql%E5%8E%9F%E7%90%86/">Mysql底层原理</a></li><li><a href="/2020/07/25/%E6%95%B0%E6%8D%AE%E5%BA%93/MySql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%BC%95%E6%93%8E/">MySql数据库的引擎</a></li><li><a href="/2020/07/15/BigDataFrame/HBase%E5%85%A5%E9%97%A8/">HBase入门</a></li><li><a href="/2020/07/05/BigDataFrame/Hive%E9%AB%98%E9%98%B6/">Hive高阶</a></li><li><a href="/2020/07/01/BigDataFrame/Hive%E5%85%A5%E9%97%A8/">Hive入门</a></li><li><a href="/2020/06/25/BigDataFrame/Kafka%E5%85%A5%E9%97%A8/">Kafka入门</a></li></ul></div><div class="box widget"><div class="title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BI%E5%B7%A5%E5%85%B7/">BI工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ETL%E5%B7%A5%E5%85%B7/">ETL工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div><div class="box widget"><div class="title">归档</div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">2020-09</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">2020-07</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">2020-06</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">2020-05</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">2020-01</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">2019-12</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">2019-07</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">2019-03</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">2018-09</a><span class="archive-list-count">1</span></li></ul></div></div></section><footer class="footer"><div class="global-width footer-box"><div class="copyright"><span>Copyright &copy; 2020</span> <span class="dotted">|</span> <span>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a></span> <span class="dotted">|</span></div></div></footer></div><script>hljs.initHighlightingOnLoad()</script></body></html>