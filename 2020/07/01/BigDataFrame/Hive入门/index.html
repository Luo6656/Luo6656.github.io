<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="author" content="Eric"><meta name="keywords" content=""><meta name="description" content="第一章 / 第二章 Hive安装 / 第三章 Hive数据类型 /第四章 DDL数据定义 / 第五章 DML数据操作 / 第六章 查询


Hive第一章 Hive基本概念1.1 什么是Hive(仅仅是一个客户端)​        Hive：由Facebook开源用于解决海量结构化日志的数据统..."><meta name="Robots" content="all"><title>Eric个人博客 | Hive入门</title><link rel="icon" href="/images/icon.svg"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/atom-one-dark.css"><link rel="stylesheet" href="/css/style.css"><script src="/js/highlight.min.js"></script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Eric个人博客" type="application/atom+xml"></head><body><div class="main-container"><header class="header"><div class="global-width"><nav class="nav-box"><a class="nav-item" href="/">主页</a> <a class="nav-item" href="/resume">项目</a> <a class="nav-item" href="/mood" target="_blank">热点观点</a> <a class="nav-item" href="/2018/01/01/introduce/" target="_blank">个人介绍</a> <a class="nav-item" href="/fuye.md">关于</a></nav></div></header><section class="content global-width"><div class="main"><article class="box post"><div class="post-title align-center detail-title">Hive入门</div><div class="post-meta align-center"><span class="label">原创</span> <span class="dotted">|</span> <i class="fa fa-calendar"></i> <time>2020-07-01</time> <span class="dotted">|</span> <i class="fa fa-user"></i> Eric <span class="dotted">|</span> <i class="fa fa-folder-open-o"></i> <a class="category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a></div><div class="post-content"><p>第一章 / 第二章 Hive安装 / 第三章 Hive数据类型 /<br>第四章 DDL数据定义 / 第五章 DML数据操作 / 第六章 查询</p><a id="more"></a><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="第一章-Hive基本概念"><a href="#第一章-Hive基本概念" class="headerlink" title="第一章 Hive基本概念"></a>第一章 Hive基本概念</h3><h4 id="1-1-什么是Hive-仅仅是一个客户端"><a href="#1-1-什么是Hive-仅仅是一个客户端" class="headerlink" title="1.1 什么是Hive(仅仅是一个客户端)"></a>1.1 什么是Hive(仅仅是一个客户端)</h4><p>​ Hive：由Facebook开源用于解决海量结构化日志的数据统计。</p><p>​ Hive是基于Hadoop的一个数据仓库工具（本身不存储数据），可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p><p>​ <font color="red">本质是：将HQL转化为MapReduce程序</font></p><p><img src="https://i.loli.net/2020/10/27/6FuaSCvWrAP3fkV.png" alt="img"></p><ul><li>​ Hive处理的数据存储在HDFS</li><li>​ Hive分析数据底层的<font color="red">默认实现是MapReducer</font>，也可使用 Spark来作为底层实现。</li><li>​ 执行程序运行在Yarn上</li></ul><h4 id="1-2-Hive的优缺点"><a href="#1-2-Hive的优缺点" class="headerlink" title="1.2 Hive的优缺点"></a>1.2 Hive的优缺点</h4><h5 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h5><ul><li>操作接口采用类SQL语法，提供快速开发的能力（简答、容易上手）。</li><li>避免了去写MapReduce，减少了开发人员的学习成本。</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li></ul><h5 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h5><ol><li>Hive的HQL表达能力有限<ul><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长，由于MapReducer数据处理流程的限制，效率更高的算法却无法实现。</li></ul></li><li>Hive的效率比较低<ul><li>Hive自动生成的MapReduce作业，通常情况下不够智能化。</li><li>Hive调优比较困难，粒度较粗</li></ul></li></ol><h4 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h4><p><img src="https://i.loli.net/2020/10/27/paMvQozwPeyS6Vf.png" alt="img"></p><ol><li><p>用户接口：Client</p><p>CLI(command-line interface)、JDBC/ODBC(jdbc访问hive)、WEBUI(浏览器访问hive)</p></li><li><p>元数据：Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在的目录等；</p><p><font color="red">默认存储在自带的derby数据库中，推荐使用MySql存储Metastore</font></p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReducer进行计算。</p></li><li><p>驱动器：Driver</p><p>（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p><p>（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。</p><p>（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。</p><p>（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</p><p><img src="https://i.loli.net/2020/10/27/pESwWXNvCPoy6gk.png" alt="img"></p><p>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p></li></ol><h4 id="1-4-Hive和数据库比较"><a href="#1-4-Hive和数据库比较" class="headerlink" title="1.4 Hive和数据库比较"></a>1.4 Hive和数据库比较</h4><p>​ 由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><h5 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h5><p>​ 由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><h5 id="1-4-2-数据存储位置"><a href="#1-4-2-数据存储位置" class="headerlink" title="1.4.2 数据存储位置"></a>1.4.2 数据存储位置</h5><p>​ Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p><h5 id="1-4-3-数据更新"><a href="#1-4-3-数据更新" class="headerlink" title="1.4.3 数据更新"></a>1.4.3 数据更新</h5><p>​ 由于Hive是针对数据仓库应用设计的，而<font color="red">数据仓库的内容是读多写少的。</font>因此，<font color="red">Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</font>而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h5 id="1-4-4-执行"><a href="#1-4-4-执行" class="headerlink" title="1.4.4 执行"></a>1.4.4 执行</h5><p>​ Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。</p><h5 id="1-4-5-执行延迟"><a href="#1-4-5-执行延迟" class="headerlink" title="1.4.5 执行延迟"></a>1.4.5 执行延迟</h5><p>​ Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h5 id="1-4-6-可扩展性"><a href="#1-4-6-可扩展性" class="headerlink" title="1.4.6 可扩展性"></a>1.4.6 可扩展性</h5><p>​ 由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle在理论上的扩展能力也只有100台左右。</p><h5 id="1-4-7-数据规模"><a href="#1-4-7-数据规模" class="headerlink" title="1.4.7 数据规模"></a>1.4.7 数据规模</h5><p>​ 由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</p><h3 id="第二章-Hive安装"><a href="#第二章-Hive安装" class="headerlink" title="第二章 Hive安装"></a>第二章 Hive安装</h3><h4 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h4><ul><li>Hive官网地址：<a target="_blank" rel="noopener" href="http://hive.apache.org/">http://hive.apache.org/</a></li><li>文档查看地址：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li><li>下载地址：<a target="_blank" rel="noopener" href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></li><li>github地址：<a target="_blank" rel="noopener" href="https://github.com/apache/hive">https://github.com/apache/hive</a></li></ul><h4 id="2-2-Hive安装部署"><a href="#2-2-Hive安装部署" class="headerlink" title="2.2 Hive安装部署"></a>2.2 Hive安装部署</h4><h5 id="2-2-1-Hive安装及配置"><a href="#2-2-1-Hive安装及配置" class="headerlink" title="2.2.1 Hive安装及配置"></a>2.2.1 Hive安装及配置</h5><ol><li><p>把apche-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure></li><li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure></li><li><p>配置hive-env.sh文件</p><ol><li><p>配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure></li><li><p>配置HIVE_CONF_DIR路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure></li></ol></li></ol><h5 id="2-2-2-Hadoop集群配置"><a href="#2-2-2-Hadoop集群配置" class="headerlink" title="2.2.2 Hadoop集群配置"></a>2.2.2 Hadoop集群配置</h5><ol><li><p>必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-2-3-Hive基本操作"><a href="#2-2-3-Hive基本操作" class="headerlink" title="2.2.3 Hive基本操作"></a>2.2.3 Hive基本操作</h5><ol><li><p>启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>打开默认数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use default;</span></span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>创建一张表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table student(id int, name string);</span></span><br></pre></td></tr></table></figure></li><li><p>显示数据库中有几张表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>查看表的结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc student;</span></span><br></pre></td></tr></table></figure></li><li><p>向表中插入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> insert into student values(1000,<span class="string">&quot;ss&quot;</span>);</span></span><br></pre></td></tr></table></figure></li><li><p>查询表中数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from student;</span></span><br></pre></td></tr></table></figure></li><li><p>退出hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> quit;</span></span><br></pre></td></tr></table></figure></li></ol><p>说明：（查看hive在hdfs中的结构）</p><p><font color="red">数据库</font>：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹</p><p><font color="red">表</font>：在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据。</p><h4 id="2-3-将本地文件导入Hive案例"><a href="#2-3-将本地文件导入Hive案例" class="headerlink" title="2.3 将本地文件导入Hive案例"></a>2.3 将本地文件导入Hive案例</h4><h5 id="2-3-1-需求"><a href="#2-3-1-需求" class="headerlink" title="2.3.1 需求"></a>2.3.1 需求</h5><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p><h5 id="2-3-2-数据准备"><a href="#2-3-2-数据准备" class="headerlink" title="2.3.2 数据准备"></a>2.3.2 数据准备</h5><p>在/opt/module/datas这个目录下创建datas</p><ol><li><p>在/opt/module/目录下创建datas</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mkdir datas</span><br></pre></td></tr></table></figure></li><li><p>在/opt/module/datas/目录下创建student.txt文件并添加数据</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch student.txt</span><br><span class="line">[atguigu@hadoop102 datas]$ vi student.txt</span><br><span class="line">1001	zhangshan</span><br><span class="line">1002	lishi</span><br><span class="line">1003	zhaoliu</span><br></pre></td></tr></table></figure><p>​ <font color="red">注意以tab键间隔</font>。</p><h5 id="2-3-3-Hive实际操作"><a href="#2-3-3-Hive实际操作" class="headerlink" title="2.3.3 Hive实际操作"></a>2.3.3 Hive实际操作</h5><ol><li><p>启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>显示数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>使用default数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use default;</span></span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> show tables;</span></span><br></pre></td></tr></table></figure></li><li><p>删除已创建的student表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> drop table student;</span></span><br></pre></td></tr></table></figure></li><li><p>创建student表，并声明文件分隔符’\t’</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED</span></span><br><span class="line"> BY &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>加载/opt/module/datas/student.txt文件到student数据库表中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> into table student;</span></span><br></pre></td></tr></table></figure></li><li><p>Hive查询结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from student;</span></span><br><span class="line">OK</span><br><span class="line">1001	zhangshan</span><br><span class="line">1002	lishi</span><br><span class="line">1003	zhaoliu</span><br><span class="line">Time taken: 0.266 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-3-4-遇到的问题"><a href="#2-3-4-遇到的问题" class="headerlink" title="2.3.4 遇到的问题"></a>2.3.4 遇到的问题</h5><p><font color="red">再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常</font></p><p>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;</p><p><font color="red">为什么表在/user/hive/warehouse里呢：因为/user/hive/warehouse是创建的default数据库的路径。</font></p><h4 id="2-4-MySql安装"><a href="#2-4-MySql安装" class="headerlink" title="2.4 MySql安装"></a>2.4 MySql安装</h4><h5 id="2-4-1-安装包准备"><a href="#2-4-1-安装包准备" class="headerlink" title="2.4.1 安装包准备"></a>2.4.1 安装包准备</h5><ol><li><p>查看mysql是否安装，如果安装了，卸载mysql</p><ol><li><p>查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -qa|grep mysql</span><br><span class="line"></span><br><span class="line">mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure></li><li><p>卸载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</span><br></pre></td></tr></table></figure></li></ol></li><li><p>解压mysql-libs.zip文件到当前目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 software]# unzip mysql-libs.zip</span><br><span class="line">[root@hadoop102 software]# ls</span><br><span class="line">mysql-libs.zip</span><br><span class="line">mysql-libs</span><br></pre></td></tr></table></figure></li><li><p>进入到mysql-libs文件夹下</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# ll</span><br><span class="line"></span><br><span class="line">总用量 76048</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz</span><br><span class="line"></span><br><span class="line">-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure><h5 id="2-4-2-安装MySql服务器"><a href="#2-4-2-安装MySql服务器" class="headerlink" title="2.4.2 安装MySql服务器"></a>2.4.2 安装MySql服务器</h5><ol><li><p>安装mysql服务端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>查看产生的随机密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# cat /root/.mysql_secret</span><br><span class="line"></span><br><span class="line">OEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure></li><li><p>查看mysql状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql status</span><br></pre></td></tr></table></figure></li><li><p>启动mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# service mysql start</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-4-3-安装Mysql客户端"><a href="#2-4-3-安装Mysql客户端" class="headerlink" title="2.4.3 安装Mysql客户端"></a>2.4.3 安装Mysql客户端</h5><ol><li><p>安装mysql客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure></li><li><p>链接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</span><br></pre></td></tr></table></figure></li><li><p>修改密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">SET PASSWORD=PASSWORD(<span class="string">&#x27;000000&#x27;</span>);</span></span><br></pre></td></tr></table></figure></li><li><p>退出mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"><span class="built_in">exit</span></span></span><br></pre></td></tr></table></figure></li></ol><h5 id="2-4-4-MySql中user表中主机配置"><a href="#2-4-4-MySql中user表中主机配置" class="headerlink" title="2.4.4 MySql中user表中主机配置"></a>2.4.4 MySql中user表中主机配置</h5><p>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。</p><ol><li><p>进入mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# mysql -uroot -p123456789</span><br></pre></td></tr></table></figure></li><li><p>显示数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">show databases;</span></span><br></pre></td></tr></table></figure></li><li><p>使用mysql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br></pre></td></tr></table></figure></li><li><p>展示mysql数据库中的所有表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show tables;</span><br></pre></td></tr></table></figure></li><li><p>展示user表的结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;desc user;</span><br></pre></td></tr></table></figure></li><li><p>查询user表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;select User, Host, Password from user;</span><br></pre></td></tr></table></figure></li><li><p>修改user表，把host表内容修改为%</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;update user set host&#x3D;&#39;%&#39; where host&#x3D;&#39;localhost&#39;;</span><br></pre></td></tr></table></figure></li><li><p>删除root用户的其他host</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;hadoop102&#39;;</span><br><span class="line"></span><br><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;127.0.0.1&#39;;</span><br><span class="line"></span><br><span class="line">mysql&gt;delete from user where Host&#x3D;&#39;::1&#39;;</span><br></pre></td></tr></table></figure></li><li><p>刷新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure></li><li><p>退出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;quit;</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-5-Hive元数据配置到Mysql"><a href="#2-5-Hive元数据配置到Mysql" class="headerlink" title="2.5 Hive元数据配置到Mysql"></a>2.5 Hive元数据配置到Mysql</h4><h5 id="2-5-1-驱动拷贝"><a href="#2-5-1-驱动拷贝" class="headerlink" title="2.5.1 驱动拷贝"></a>2.5.1 驱动拷贝</h5><ol><li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line"></span><br><span class="line"> /opt/module/hive/lib/</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-5-2-配置Metastore到Mysql"><a href="#2-5-2-配置Metastore到Mysql" class="headerlink" title="2.5.2 配置Metastore到Mysql"></a>2.5.2 配置Metastore到Mysql</h5><ol><li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ touch hive-site.xml</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><blockquote><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动Hadoop集群）</p></li></ol><h5 id="2-5-3-多窗口启动Hive测试"><a href="#2-5-3-多窗口启动Hive测试" class="headerlink" title="2.5.3 多窗口启动Hive测试"></a>2.5.3 多窗口启动Hive测试</h5><ol><li><p>先启动MySql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 mysql-libs]$ mysql -uroot -p123456789</span><br></pre></td></tr></table></figure></li><li><p>再次打开多个窗口，分别启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>启动hive后，回到MySql窗口查看数据库，显示<font color="red">增加了metastore数据库</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| metastore          |</span><br><span class="line">| mysql             |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test               |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure></li></ol><h4 id="2-6-HiveJDBC访问"><a href="#2-6-HiveJDBC访问" class="headerlink" title="2.6 HiveJDBC访问"></a>2.6 HiveJDBC访问</h4><h5 id="2-6-1-启动hiveserver2服务"><a href="#2-6-1-启动hiveserver2服务" class="headerlink" title="2.6.1 启动hiveserver2服务"></a>2.6.1 启动hiveserver2服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure><h5 id="2-6-2-启动beeline"><a href="#2-6-2-启动beeline" class="headerlink" title="2.6.2 启动beeline"></a>2.6.2 启动beeline</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2-6-3-连接hiveserver2"><a href="#2-6-3-连接hiveserver2" class="headerlink" title="2.6.3 连接hiveserver2"></a>2.6.3 连接hiveserver2</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop102:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://hadoop102:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车）</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop102:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default        |</span><br><span class="line">| hive_db2       |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure><h4 id="2-7-Hive常用交互命令"><a href="#2-7-Hive常用交互命令" class="headerlink" title="2.7 Hive常用交互命令"></a>2.7 Hive常用交互命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--help                        Print help information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure><ol><li><p>“e” 不进入hive的交互窗口执行sql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>“f” 执行脚本中的sql语句</p><ol><li><p>在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>文件中写入正确的sql语句</p><p>select * from student;</p></li><li><p>执行文件中的sql语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句并将结果写入文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="2-8-Hive其他命令操作"><a href="#2-8-Hive其他命令操作" class="headerlink" title="2.8 Hive其他命令操作"></a>2.8 Hive其他命令操作</h4><ol><li><p>退出hive窗口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;dfs -ls &#x2F;;</span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure></li><li><p>查看在hive中输入的所有历史命令</p><ol><li>进入到当前用户的根目录/root或/home/atguigu</li><li>查看.hivehistory文件</li></ol></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure><h4 id="2-9-Hive常见配置属性"><a href="#2-9-Hive常见配置属性" class="headerlink" title="2.9 Hive常见配置属性"></a>2.9 Hive常见配置属性</h4><h5 id="2-9-1-Hive数据仓库位置的配置"><a href="#2-9-1-Hive数据仓库位置的配置" class="headerlink" title="2.9.1 Hive数据仓库位置的配置"></a>2.9.1 Hive数据仓库位置的配置</h5><ol><li><p>Default数据仓库的最原始位置是在hdfs上的:/user/hive/warehouse路径下。</p></li><li><p><font color="red">在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹</font></p></li><li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-9-2-查询后信息显示的配置"><a href="#2-9-2-查询后信息显示的配置" class="headerlink" title="2.9.2 查询后信息显示的配置"></a>2.9.2 查询后信息显示的配置</h5><ol><li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>重新启动hive，对比配置前后差异。</p><ol><li><p>配置前，如图所示</p><p><img src="https://i.loli.net/2020/10/27/wZdMS2s5pQ4ulaR.png"></p></li><li><p>配置后，如图所示</p></li></ol></li></ol><p><img src="https://i.loli.net/2020/10/27/1c4uR3ZUpnmDJfz.png"></p><h5 id="2-9-3-Hive运行日志信息的配置"><a href="#2-9-3-Hive运行日志信息的配置" class="headerlink" title="2.9.3 Hive运行日志信息的配置"></a>2.9.3 Hive运行日志信息的配置</h5><ol><li><p>Hive的log默认存放在/tmp/atguigu/hive.log目录下（当前用户名下）</p></li><li><p>修改hive的log存放日志到/opt/module/hive/logs</p><ol><li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure></li><li><p>在hive-log4j.properties文件中修改log存放位置</p></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li></ol><h5 id="2-9-4-参数配置方式"><a href="#2-9-4-参数配置方式" class="headerlink" title="2.9.4 参数配置方式"></a>2.9.4 参数配置方式</h5><ol><li><p>查看当前所有的配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set;</span><br></pre></td></tr></table></figure></li><li><p>参数的配置三种方式</p><p>（1）配置文件方式</p><p>​ 默认配置文件：hive-default.xml</p><p>​ 用户自定义配置文件：hive-site.xlm</p><p><font color="red">注意</font>：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p><p>（2）命令行参数方式</p><p>​ 启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</p><p>​ 例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>​ <font color="red">注意：仅对本次hive启动有效</font></p><p>​ 查看参数设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>（3）参数声明方式</p><p>​ 可以在HQL中使用SET关键字设定参数</p><p>​ 例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks&#x3D;100;</span><br></pre></td></tr></table></figure><p>​ <font color="red">注意：仅对本次hive启动有效</font></p><p>​ 查看参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>​ 上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p></li></ol><h3 id="第三章-Hive数据类型"><a href="#第三章-Hive数据类型" class="headerlink" title="第三章 Hive数据类型"></a>第三章 Hive数据类型</h3><h4 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h4><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>​ 对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h4 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h4><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()例如struct&lt;street:string, city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()例如array<string></string></td></tr></tbody></table><p>​ Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><p>​ <strong>案例实操</strong></p><p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问格式为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据</p><p>​ 创建本地测试文件test.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line"></span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>​ <font color="red">注意</font>：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p><p>3）Hive上创建测试表test</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table test(</span><br><span class="line">name string,</span><br><span class="line">friends array&lt;string&gt;,</span><br><span class="line">children map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;street:string, city:string&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;</span><br><span class="line">collection items terminated by &#39;_&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><p>字段解释：</p><p>row format delimited fields terminated by ‘,’ – 列分隔符</p><p>collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p>map keys terminated by ‘:’ – MAP中的key与value的分隔符</p><p>lines terminated by ‘\n’; – 行分隔符</p><p>4）导入文本数据到测试表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘&#x2F;opt&#x2F;module&#x2F;datas&#x2F;test.txt’into table test</span><br></pre></td></tr></table></figure><p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from test</span><br><span class="line">where name&#x3D;&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><h4 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h4><p>​ Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><h5 id="3-3-1-隐式类型转换规则如下"><a href="#3-3-1-隐式类型转换规则如下" class="headerlink" title="3.3.1 隐式类型转换规则如下"></a>3.3.1 隐式类型转换规则如下</h5><ol><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li><li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li><li>TINYINT、SMALLINT、INT都可以转换成FLOAT</li><li>BOOLEAN类型不可以转换为任何其他地类型</li></ol><h5 id="3-3-2-可以使用CAST操作显示进行数据类型转换"><a href="#3-3-2-可以使用CAST操作显示进行数据类型转换" class="headerlink" title="3.3.2 可以使用CAST操作显示进行数据类型转换"></a>3.3.2 可以使用CAST操作显示进行数据类型转换</h5><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;hadoop102:10000&gt; select &#39;1&#39;+2, cast(&#39;1&#39;as int) + 2;</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br><span class="line"></span><br><span class="line">| _c0  | _c1  |</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br><span class="line"></span><br><span class="line">| 3.0  | 3   |</span><br><span class="line"></span><br><span class="line">+------+------+--+</span><br></pre></td></tr></table></figure><h3 id="第四章-DDL数据定义"><a href="#第四章-DDL数据定义" class="headerlink" title="第四章 DDL数据定义"></a>第四章 DDL数据定义</h3><p><font color="red">DDL都是操作的元数据</font></p><h4 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><ol><li><p>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure></li><li><p>避免要创建的数据库已经存在的错误，增加if not exists判断(标准写法)</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure></li><li><p>创建一个数据库，指定数据库在HDFS上存放的位置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h4><h5 id="4-2-1-显示数据库"><a href="#4-2-1-显示数据库" class="headerlink" title="4.2.1 显示数据库"></a>4.2.1 显示数据库</h5><ol><li><p>显示数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>过滤显示查询数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases like &#39;db_hive*&#39;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h5><ol><li><p>显示数据库信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hive		hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db	atguiguUSER	</span><br></pre></td></tr></table></figure></li><li><p>显示数据库详细信息，extended</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hive		hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db	atguiguUSER</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-2-3-切换当前数据库"><a href="#4-2-3-切换当前数据库" class="headerlink" title="4.2.3 切换当前数据库"></a>4.2.3 切换当前数据库</h5><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h4 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h4><p>​ 用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。<font color="red">数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</font></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line"></span><br><span class="line">db_name <span class="keyword">comment</span> location     owner_name    owner_type    <span class="keyword">parameters</span></span><br><span class="line"></span><br><span class="line">db_hive     hdfs://hadoop102:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db   atguigu <span class="keyword">USER</span>   &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h4><ol><li><p>删除空数据库</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果删除的数据库不存在，最好采用if exists判断数据库是否存在</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line"></span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line"></span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果数据库不为空，可以采用cascade命令，强制删除</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h4><h5 id="4-5-1-建表语句以及建表参数"><a href="#4-5-1-建表语句以及建表参数" class="headerlink" title="4.5.1 建表语句以及建表参数"></a>4.5.1 建表语句以及建表参数</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">/*注释*/</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]  <span class="comment">/*分区，大部分都是分区表，分的是文件夹*/</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">/*分桶，分的是文件*/</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure><p><strong>字段解释说明</strong></p><p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p><p>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），<font color="red">在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</font></p><p>（3）COMMENT：为表和列添加注释。</p><p>（4）PARTITIONED BY创建分区表</p><p>（5）CLUSTERED BY创建分桶表</p><p>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</p><p>（7）ROW FORMAT</p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p><p>​ [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</p><p>| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，<font color="red">Hive通过SerDe确定表的具体的列的数据。</font></p><p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p><p>（8）STORED AS指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p><p><font color="red">（9）LOCATION ：指定表在HDFS上的存储位置。</font></p><p><font color="red">（10）AS：后跟查询语句</font>，根据查询结果创建表。</p><p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p><h5 id="4-5-2-管理表（内部表）"><a href="#4-5-2-管理表（内部表）" class="headerlink" title="4.5.2 管理表（内部表）"></a>4.5.2 管理表（内部表）</h5><ol><li><p>理论</p><p>默认创建的表都是所谓的管理表，有时也被称为<font color="red">内部表</font>。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 <font color="red">当我们删除一个管理表时，Hive也会删除这个表中数据</font>。管理表不适合和其他工具共享数据。</p></li><li><p>案例操作</p><ol><li><p>普通创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student2&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据查询结果创建表（查询的结果会添加到新创建的表中)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>根据已经存在的表结构创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE  </span><br></pre></td></tr></table></figure></li></ol></li></ol><h5 id="4-5-3-外部表"><a href="#4-5-3-外部表" class="headerlink" title="4.5.3 外部表"></a>4.5.3 外部表</h5><ol><li><p>理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<font color="red">删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</font></p></li><li><p>管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p></li><li><p>案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据</p><ol><li><p>上传数据到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>建表语句（创建外部表）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table stu_external(</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27; </span><br><span class="line">location &#x27;/student&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>查看创建的表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br><span class="line">1003    zhaoliu</span><br></pre></td></tr></table></figure></li><li><p>查看表格式化数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</font></p><p>​</p></li></ol><h5 id="4-5-4-管理表与外部表的互相转换（注意大小写）"><a href="#4-5-4-管理表与外部表的互相转换（注意大小写）" class="headerlink" title="4.5.4 管理表与外部表的互相转换（注意大小写）"></a>4.5.4 管理表与外部表的互相转换（注意大小写）</h5><ol><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li><li><p>查询内部表student2为外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span>=<span class="string">&#x27;TRUE&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改外部表student2为内部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span>=<span class="string">&#x27;FALSE&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p><font color="red">注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</font></p></li></ol><h4 id="4-6-分区表"><a href="#4-6-分区表" class="headerlink" title="4.6 分区表"></a>4.6 分区表</h4><p>​ 分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<font color="red">Hive中的分区就是分目录</font>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h5 id="4-6-1-分区表基本操作"><a href="#4-6-1-分区表基本操作" class="headerlink" title="4.6.1 分区表基本操作"></a>4.6.1 分区表基本操作</h5><ol><li><p>引入分区表（需要根据日期对日志进行管理）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170702&#x2F;20170702.log</span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170703&#x2F;20170703.log</span><br><span class="line">&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20170704&#x2F;20170704.log</span><br></pre></td></tr></table></figure></li><li><p>创建分区表语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int, dname string, loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p></li><li><p>加载数据到分区表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201709&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201708&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition partition(month=&#x27;201707’);</span><br></pre></td></tr></table></figure><p>注意：分区表加载数据时，必须指定分区</p><p><img src="https://i.loli.net/2020/10/27/W7BVeQ6LygNRiKH.png"></p><p>​ 加载数据到分区表</p><p><img src="https://i.loli.net/2020/10/27/hmvedFiYBXGtyZn.png"></p><p>​ 分区表</p></li></ol><ol start="4"><li><p>查询分区表中数据</p><p>（1）单分区查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure><p>（2）多分区联合查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;</span><br><span class="line">              union</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201708&#x27;</span></span><br><span class="line">              <span class="keyword">union</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201707&#x27;</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure></li><li><p>增加分区</p><p>（1）创建单个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201706&#x27;) ;</span><br></pre></td></tr></table></figure><p>（2）同时创建多个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month=&#x27;201705&#x27;) partition(month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure></li><li><p>删除分区</p><p>（1）删除单个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201704&#x27;);</span><br></pre></td></tr></table></figure><p>（2）同时删除多个分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201705&#x27;), partition (month=&#x27;201706&#x27;);</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li><p>查看分区表有多少分区</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>查看分区表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string    </span><br></pre></td></tr></table></figure></li></ol><h5 id="4-6-2-分区表注意事项"><a href="#4-6-2-分区表注意事项" class="headerlink" title="4.6.2 分区表注意事项"></a>4.6.2 分区表注意事项</h5><ol><li><p>创建二级分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, dname string, loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>正常的加载数据</p><p>（1）加载数据到二级分区表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> default.dept_partition2 partition(month=&#x27;201709&#x27;, day=&#x27;13&#x27;);</span><br></pre></td></tr></table></figure><p>（2）查询分区数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><p>（1）方式一：上传数据后修复</p><p>​ a.上传数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><p>​ b.查询数据（不能查到，元数据里没有）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>​ c.执行修复命令</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>​ d.再次查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure><p>（2）方式二：上传数据后添加分区</p><p>​ a.上传数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>​ b.执行添加分区</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month=&#x27;201709&#x27;,</span><br><span class="line"> day=&#x27;11&#x27;);</span><br></pre></td></tr></table></figure><p>​ c.查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;</span><br></pre></td></tr></table></figure><p>（3）方式三：创建文件夹后load数据到分区</p><p>​ a.创建目录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>​ b.上传数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line"> dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);</span><br></pre></td></tr></table></figure><p>​ c.查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-7-修改表"><a href="#4-7-修改表" class="headerlink" title="4.7 修改表"></a>4.7 修改表</h4><h5 id="4-7-1-重命名表"><a href="#4-7-1-重命名表" class="headerlink" title="4.7.1 重命名表"></a>4.7.1 重命名表</h5><ol><li><p>语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li><li><p>实操案例</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure></li></ol><h5 id="4-7-2-增加、修改和删除分区表"><a href="#4-7-2-增加、修改和删除分区表" class="headerlink" title="4.7.2 增加、修改和删除分区表"></a>4.7.2 增加、修改和删除分区表</h5><p><font color="red">详见4.6.1分区表基本操作</font></p><h5 id="4-7-3-增加-修改-替换列信息"><a href="#4-7-3-增加-修改-替换列信息" class="headerlink" title="4.7.3 增加/修改/替换列信息"></a>4.7.3 增加/修改/替换列信息</h5><ol><li><p>语法</p><ol><li><p>更新列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure></li><li><p>增加和替换列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...) </span><br></pre></td></tr></table></figure><p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，<font color="red">REPLACE则是表示替换表中所有字段。</font></p></li></ol></li><li><p>实操案例</p><p>（1）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（2）添加列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure><p>（3）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（4）更新列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure><p>（5）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure><p>（6）替换列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure><p>（7）查询表结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-8-删除表"><a href="#4-8-删除表" class="headerlink" title="4.8 删除表"></a>4.8 删除表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h3 id="第五章-DML数据操作"><a href="#第五章-DML数据操作" class="headerlink" title="第五章 DML数据操作"></a>第五章 DML数据操作</h3><h4 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h4><h5 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h5><ol><li><p>语法</p><p>hive&gt; load data [local] inpath ‘/opt/module/datas/student.txt’ [overwrite] into table student [partition (partcol1=val1,…)];</p><p>（1）load data:表示加载数据</p><p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p>（3）inpath:表示加载数据的路径</p><p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p><p>（5）into table:表示加载到哪张表</p><p>（6）student:表示具体的表</p><p>（7）partition:表示上传到指定分区</p></li></ol><ol start="2"><li><p>实操案例</p><p>（1）创建一张表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（2）加载本地文件到hive</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（3）加载HDFS文件到hive中</p><p>​ a.上传文件到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;</span><br></pre></td></tr></table></figure><p>​ b.加载HDFS上数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure><p>（4）加载数据覆盖表中已有的数据</p><p>​ a.上传文件到HDFS</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;</span><br></pre></td></tr></table></figure><p>​ b.加载数据覆盖表中已有的数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h5><p>（1）创建一张分区表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure><p>（2）基本插入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month=&#x27;201709&#x27;) values(1,&#x27;wangwu&#x27;),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure><p>（3）基本模式插入（根据单张表查询结果）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month=&#x27;201708&#x27;)</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure><p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p><p>insert overwrite：会覆盖表或分区中已存在的数据</p><p>（4）多表（多分区）插入模式（根据多张表查询结果）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">&#x27;201707&#x27;</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">&#x27;201706&#x27;</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure><h5 id="5-1-3-查询语句中创建表并加载数据（as-select）"><a href="#5-1-3-查询语句中创建表并加载数据（as-select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（as select）"></a>5.1.3 查询语句中创建表并加载数据（as select）</h5><p>​ <font color="red">详见4.5.1 章创建表</font></p><p>​ 根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h5 id="5-1-4-创建表时通过location指定加载数据路径"><a href="#5-1-4-创建表时通过location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过location指定加载数据路径"></a>5.1.4 创建表时通过location指定加载数据路径</h5><ol><li><p>上传数据到hdfs上</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>创建表，并指定载hdfs上的位置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">              location &#x27;/student;</span><br></pre></td></tr></table></figure></li><li><p>查询数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-1-5-import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）"><a href="#5-1-5-import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）" class="headerlink" title="5.1.5 import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）"></a>5.1.5 import数据到指定Hive表中（必须是export导出的文件夹，必须是空表）</h5><p>​ <font color="red">注意：先用export导出后，再将数据导入</font></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month=&#x27;201709&#x27;) from</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><h4 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h4><h5 id="5-2-1-insert导出"><a href="#5-2-1-insert导出" class="headerlink" title="5.2.1 insert导出"></a>5.2.1 insert导出</h5><ol><li><p>将查询的结果导出到本地</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;</span><br><span class="line">            <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果格式化导出到本地</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;</span><br><span class="line">           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果导出到HDFS上（没有local）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/atguigu/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li></ol><h5 id="5-2-2-Hadoop命令导出到本地"><a href="#5-2-2-Hadoop命令导出到本地" class="headerlink" title="5.2.2 Hadoop命令导出到本地"></a>5.2.2 Hadoop命令导出到本地</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h5 id="5-2-3-Hive-Shell命令导出"><a href="#5-2-3-Hive-Shell命令导出" class="headerlink" title="5.2.3 Hive Shell命令导出"></a>5.2.3 Hive Shell命令导出</h5><p>​ 基本语法：（hive -f/-e 执行语句或者脚本 &gt;file）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h5 id="5-2-4-export导出到HDFS上"><a href="#5-2-4-export导出到HDFS上" class="headerlink" title="5.2.4 export导出到HDFS上"></a>5.2.4 export导出到HDFS上</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive(default)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure><p>​ export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h5 id="5-2-5-sqoop导出"><a href="#5-2-5-sqoop导出" class="headerlink" title="5.2.5 sqoop导出"></a>5.2.5 sqoop导出</h5><p>​ 后续课程专门讲</p><h4 id="5-3-清除表中数据（truncate"><a href="#5-3-清除表中数据（truncate" class="headerlink" title="5.3 清除表中数据（truncate)"></a>5.3 清除表中数据（truncate)</h4><p>​ <font color="red">注意：truncate只能删除管理表，不能删除外部表中数据</font></p><h3 id="第六章-查询"><a href="#第六章-查询" class="headerlink" title="第六章 查询"></a>第六章 查询</h3><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></p><p>查询顺序：from–&gt; join on–&gt; where–&gt; group by–&gt; select|having –&gt; order by–&gt; limit</p><p>查询语句语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*]    (Note: <span class="keyword">Only</span> available</span><br><span class="line"> <span class="keyword">starting</span> <span class="keyword">with</span> Hive <span class="number">0.13</span><span class="number">.0</span>)</span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">    | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><h4 id="6-1-基本查询"><a href="#6-1-基本查询" class="headerlink" title="6.1 基本查询"></a>6.1 基本查询</h4><h5 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h5><p>​ 创建部门表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>​ 创建员工表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>​ 导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table</span><br><span class="line">dept;</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table emp;</span><br></pre></td></tr></table></figure><ol><li><p>全表查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span><br></pre></td></tr></table></figure></li><li><p>选择特定列查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select empno, ename from emp;</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意</font>：（1）SQL语言大小写不敏感</p><p>​ （2）SQL可以写在一行或者多行</p><p>​ （3）关键字不能被缩写也不能分行</p><p>​ （4）各子句一般要分行写</p><p>​ （5）使用缩进提高语句的可读性</p><h5 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h5><ol><li><p>重命名一个列</p></li><li><p>便于计算</p></li><li><p>紧跟列名，<font color="red">也可以在列名和别名之间加入关键字 as</font></p></li><li><p>案例实操</p><p>查询名称和部门</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h5><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B 相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B 相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A|B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table><p>​ 案例实操</p><p>​ 查询出所有员工的薪水后加1显示</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sal +1 from emp;</span><br></pre></td></tr></table></figure><h5 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h5><ol><li><p>求总行数（count）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的最大值（max）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的最小值（min）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure></li><li><p>求工资的总和（sum）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure></li><li><p>求工资的平均值（avg）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-1-5-limit语句"><a href="#6-1-5-limit语句" class="headerlink" title="6.1.5 limit语句"></a>6.1.5 limit语句</h5><p>​ 典型的查询会返回多行数据。limit子句用于限制返回的行数。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br></pre></td></tr></table></figure><h4 id="6-2-where语句"><a href="#6-2-where语句" class="headerlink" title="6.2 where语句"></a>6.2 where语句</h4><ol><li><p>使用where子句，将不满足条件的行过滤掉</p></li><li><p>where子句紧跟from子句</p></li><li><p>案例实操</p><p>查询出薪水大于1000的所有员工</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000;</span><br></pre></td></tr></table></figure><p><font color="red">注意：where子句中不能使用字段别名</font></p></li></ol><h5 id="6-2-1-比较运算符（between-in-is-null）"><a href="#6-2-1-比较运算符（between-in-is-null）" class="headerlink" title="6.2.1 比较运算符（between/in/is null）"></a>6.2.1 比较运算符（between/in/is null）</h5><p>(1) 下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中.</p><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table><p>(2) <strong>案例实操</strong></p><ol><li><p>查询出薪水等于5000的所有员工</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal=<span class="number">5000</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询工资在500到1000的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">between</span> <span class="number">500</span> <span class="keyword">and</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询comm为空的所有员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询工资是1500或5000的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">in</span> (<span class="number">1500</span>,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-2-2-like和rlike"><a href="#6-2-2-like和rlike" class="headerlink" title="6.2.2 like和rlike"></a>6.2.2 like和rlike</h5><ol><li><p>使用like运算选择类似的值</p></li><li><p>选择条件可以包含字符或数字</p></li><li><p>rlike子句是Hive中这个功能的一个扩展,其可以通过<font color="red">Java的正则表达式</font>这个更强大的语言来指定匹配条件</p></li><li><p><strong>案例实操</strong></p><p>(1) 查找以2开头薪水的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">like</span> <span class="string">&#x27;2%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>(2) 查找第二个数值为2的薪水的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">like</span> <span class="string">&#x27;_2%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>(3) 查找薪水中含有2的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">rlike</span> <span class="string">&#x27;[2]&#x27;</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-2-3-逻辑运算符（and-or-not）"><a href="#6-2-3-逻辑运算符（and-or-not）" class="headerlink" title="6.2.3 逻辑运算符（and/or/not）"></a>6.2.3 逻辑运算符（and/or/not）</h5><table><thead><tr><th>操作符</th><th>含义</th></tr></thead><tbody><tr><td>AND</td><td>逻辑并</td></tr><tr><td>OR</td><td>逻辑或</td></tr><tr><td>NOT</td><td>逻辑否</td></tr></tbody></table><p>​ <strong>案例实操</strong></p><ol><li><p>查询薪水大于1000，部门是30</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">and</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询薪水大于1000，或者部门是304</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal&gt;<span class="number">1000</span> <span class="keyword">or</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询除了20部门和30部门以外的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">in</span>(<span class="number">30</span>,<span class="number">200</span>);</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-3-分组"><a href="#6-3-分组" class="headerlink" title="6.3 分组"></a>6.3 分组</h4><h5 id="6-3-1-group-by语句"><a href="#6-3-1-group-by语句" class="headerlink" title="6.3.1 group by语句"></a>6.3.1 group by语句</h5><p>​ group by语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p><p>​ <strong>案例实操</strong></p><ol><li><p>计算emp表每个部门的平局工资</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno,<span class="keyword">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure></li><li><p>计算emp每个部门中每个岗位的最高薪水</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.deptno,t.job,<span class="keyword">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno,t.job;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-3-2-Having语句"><a href="#6-3-2-Having语句" class="headerlink" title="6.3.2 Having语句"></a>6.3.2 Having语句</h5><ol><li><p>having与where不同点</p><p>（1）where后面不能写分组函数，而having后面可以使用分组函数。</p><p>（2）having只用于group by分组统计语句</p></li><li><p><strong>案例实操</strong></p><p>求每个部门的平均薪水大于2000的部门</p><p>（1）求每个部门的平均工资</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> deptno,<span class="keyword">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p>（2）求每个部门的平均薪水大于2000的部门</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> deptno,<span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal&gt;<span class="number">2000</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="6-4-join语句"><a href="#6-4-join语句" class="headerlink" title="6.4 join语句"></a>6.4 join语句</h4><h5 id="6-4-1-等值join"><a href="#6-4-1-等值join" class="headerlink" title="6.4.1 等值join"></a>6.4.1 等值join</h5><p>​ Hive支持通常的SQL join语句，但是<font color="red">只支持等值连接，不支持非等值连接</font></p><p>​ <strong>案例实操</strong></p><p>​ 根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-2-表的别名"><a href="#6-4-2-表的别名" class="headerlink" title="6.4.2 表的别名"></a>6.4.2 表的别名</h5><ol><li><p>好处</p><p>（1）使用别名可以简化查询</p><p>（2）使用表名前缀可以提高执行效率</p></li><li><p>案例实操</p><p>合并员工表和部门表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-4-3-内连接"><a href="#6-4-3-内连接" class="headerlink" title="6.4.3 内连接"></a>6.4.3 内连接</h5><p>​ <font color="red">内连接</font>：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-4-左外连接"><a href="#6-4-4-左外连接" class="headerlink" title="6.4.4 左外连接"></a>6.4.4 左外连接</h5><p>​ <font color="red">左外连接</font>：join操作符左边表中符合where子句的所有记录将会被返回</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-5-右外连接"><a href="#6-4-5-右外连接" class="headerlink" title="6.4.5 右外连接"></a>6.4.5 右外连接</h5><p>​ <font color="red">右外连接</font>：join操作符右边表中符合where子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><h5 id="6-4-6-满外连接"><a href="#6-4-6-满外连接" class="headerlink" title="6.4.6 满外连接"></a>6.4.6 满外连接</h5><p>​ <font color="red">满外连接</font>：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值得话，那么就使用null值替代。</p><h5 id="6-4-7-多表连接"><a href="#6-4-7-多表连接" class="headerlink" title="6.4.7 多表连接"></a>6.4.7 多表连接</h5><p>​ <font color="red">注意：连接n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</font></p><p>​ <strong>案例实操</strong></p><ol><li><p>​ 创建位置表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> location(</span><br><span class="line">loc <span class="built_in">int</span>,</span><br><span class="line">loc_name <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li><li><p>​ 导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span><br></pre></td></tr></table></figure></li><li><p>​ 多表连接查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename, d.dname, l.loc_name <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> d.deptno = e.deptno <span class="keyword">join</span> location l <span class="keyword">on</span> d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>​ 大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p><p>​ 注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</p><p>​ <font color="red">优化：当对3个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。</font></p></li></ol><h5 id="6-4-8-笛卡尔积（少用）"><a href="#6-4-8-笛卡尔积（少用）" class="headerlink" title="6.4.8 笛卡尔积（少用）"></a>6.4.8 笛卡尔积（少用）</h5><ol><li><p>笛卡尔积会在下面条件下产生</p><p>（1）省略连接条件</p><p>（2）连接条件无效</p><p>（3）所有表中得所有行互相连接</p></li><li><p>案例实操</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> empno,dname <span class="keyword">from</span> emp,dept;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-4-9-连接谓词中不支持or"><a href="#6-4-9-连接谓词中不支持or" class="headerlink" title="6.4.9 连接谓词中不支持or"></a>6.4.9 连接谓词中不支持or</h5><p>​ hive join目前不支持在on子句中使用谓词or</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno <span class="keyword">or</span> e.ename=d.ename;   错误的</span><br></pre></td></tr></table></figure><h4 id="6-5-排序"><a href="#6-5-排序" class="headerlink" title="6.5 排序"></a>6.5 排序</h4><h5 id="6-5-1-全局排序"><a href="#6-5-1-全局排序" class="headerlink" title="6.5.1 全局排序"></a>6.5.1 全局排序</h5><p><font color="red">order by：全局排序，只有一个reducer</font></p><ol><li><p>使用order by子句排序</p><p><font color="red">（1）asc（ascend）：升序（默认）</font></p><p><font color="red">（2）desc（descend）：降序</font></p></li><li><p>order by子句在select语句的结尾</p></li><li><p><strong>案例实操</strong></p><p>（1）查询员工信息按工资升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span><br></pre></td></tr></table></figure><p>（2）查询员工信息按工资降序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-5-2-按照别名排序"><a href="#6-5-2-按照别名排序" class="headerlink" title="6.5.2 按照别名排序"></a>6.5.2 按照别名排序</h5><p><strong>案例实操</strong></p><p>按照员工薪水的2倍排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename, sal*<span class="number">2</span> doublesal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> doublesal;</span><br></pre></td></tr></table></figure><h5 id="6-5-3-多个列排序"><a href="#6-5-3-多个列排序" class="headerlink" title="6.5.3 多个列排序"></a>6.5.3 多个列排序</h5><p><strong>案例实操</strong></p><p>按照部门和工资升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno,dal;</span><br></pre></td></tr></table></figure><h5 id="6-5-4-每个MapReduce内部排序（sort-by）"><a href="#6-5-4-每个MapReduce内部排序（sort-by）" class="headerlink" title="6.5.4 每个MapReduce内部排序（sort by）"></a>6.5.4 每个MapReduce内部排序（sort by）</h5><p>​ Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p><p>​ Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure></li><li><p>查看设置reduce个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br></pre></td></tr></table></figure></li><li><p>根据部门编号降序查看员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件夹中（按照部门编号降序排序）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">&#x27;/opt/module/datas/sortby-result&#x27;</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-5-5-分区排序（distribute-by）"><a href="#6-5-5-分区排序（distribute-by）" class="headerlink" title="6.5.5 分区排序（distribute by）"></a>6.5.5 分区排序（distribute by）</h5><p>​ Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。*<strong>*distribute by**</strong> 子句可以做这件事。*<strong>*distribute by**</strong>类似MR中partition（自定义分区），进行分区，结合sort by使用。</p><p>​ 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p><strong>案例实操</strong></p><p>先按照部门编号分区，再按照员工编号降序排序</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>distrubute by 的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li><li><font color="red">Hive要求distribute by语句要写在sort by语句之前</font></li></ol><h5 id="6-5-6-cluster-by"><a href="#6-5-6-cluster-by" class="headerlink" title="6.5.6 cluster by"></a>6.5.6 cluster by</h5><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>1）以下两种写法等价</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p><h5 id="6-5-6-几个排序函数的区别（重要）"><a href="#6-5-6-几个排序函数的区别（重要）" class="headerlink" title="6.5.6 几个排序函数的区别（重要）"></a>6.5.6 几个排序函数的区别（重要）</h5><ol><li>order by：全局排序，reduce个数为1，设置多个也没用</li><li>sort by：区内排序，通常结合distribute by使用，reduce多个，会产生数据倾斜</li><li>cluster by：当分区字段和排序字段相同的时候，可以使用cluster by代替。</li></ol><h4 id="6-6-分桶及抽样查询"><a href="#6-6-分桶及抽样查询" class="headerlink" title="6.6 分桶及抽样查询"></a>6.6 分桶及抽样查询</h4><h5 id="6-6-1-分桶表数据存储"><a href="#6-6-1-分桶表数据存储" class="headerlink" title="6.6.1 分桶表数据存储"></a>6.6.1 分桶表数据存储</h5><p>​ 1. 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p><p>​ 2. 分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p><p>​ 3. <font color="red">分区针对的是数据的存储路径；分桶针对的是数据文件。</font></p><p><strong>案例实操</strong></p><ol><li><p>先创建分桶表，通过直接导入数据文件的方式</p><p>（1）数据准备</p><p>​ 准备student.txt文件</p><p>（2）创建分桶表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）查看表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted stu_buck;</span><br><span class="line">Num Buckets:	4</span><br></pre></td></tr></table></figure><p>（4）导入数据到分桶表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck;</span><br></pre></td></tr></table></figure><p>（5）查看创建的分桶表中是否分成4个桶，如下图</p><p>​ 发现并未分桶，那是什么原因呢（load相当于是put，所以不会分桶）</p></li></ol><ol start="2"><li><p>创建分桶表时，数据通过子查询的方式导入</p><p>（1）先创建一个普通的stu表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（2）先普通的stu表中导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><p>（3）清空stu_buck表中数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> stu_buck;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck;</span><br></pre></td></tr></table></figure><p>（4）导入数据到分桶表，通过子查询的方式（走MR）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>（5）发现还是只有一个分桶</p><p>（6）需要设置一个属性</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>; <span class="comment">/*开启分桶功能*/</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">-1</span>; <span class="comment">/*-1表示reduce会根据桶的个数分配reduce个数*/</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/10/27/jurRfc7bzdG6IiP.png"></p><p>（7）查询分桶的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_buck;</span><br><span class="line">OK</span><br><span class="line">stu_buck.id     stu_buck.name</span><br><span class="line">1004    ss4</span><br><span class="line">1008    ss8</span><br><span class="line">1012    ss12</span><br><span class="line">1016    ss16</span><br><span class="line">1001    ss1</span><br><span class="line">1005    ss5</span><br><span class="line">1009    ss9</span><br><span class="line">1013    ss13</span><br><span class="line">1002    ss2</span><br><span class="line">1006    ss6</span><br><span class="line">1010    ss10</span><br><span class="line">1014    ss14</span><br><span class="line">1003    ss3</span><br><span class="line">1007    ss7</span><br><span class="line">1011    ss11</span><br><span class="line">1015    ss15</span><br></pre></td></tr></table></figure><p><font color="red">分桶规则：</font></p><p>根据结果可知：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p></li></ol><h5 id="6-6-2-分桶抽样查询"><a href="#6-6-2-分桶抽样查询" class="headerlink" title="6.6.2 分桶抽样查询"></a>6.6.2 分桶抽样查询</h5><p>​ 对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p><p>​ 查询表stu_buck中的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p>​ 注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>​ y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>​ <font color="red">x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。</font>例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p>​ <font color="red">注意：x的值必须小于等于y的值，否则</font></p><p>​ FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p><h4 id="6-7-其他常用查询函数"><a href="#6-7-其他常用查询函数" class="headerlink" title="6.7 其他常用查询函数"></a>6.7 其他常用查询函数</h4><h5 id="6-7-1-空字段赋值"><a href="#6-7-1-空字段赋值" class="headerlink" title="6.7.1 空字段赋值"></a>6.7.1 空字段赋值</h5><ol><li><p>函数说明</p><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。</p></li><li><p>数据准备：采用员工表</p></li><li><p>查询：如果员工的comm为NULL，则用-1代替</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive(default) &gt; select comm,nvl(comm, -1) from emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    -1.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br><span class="line">NULL    -1.0</span><br></pre></td></tr></table></figure></li><li><p>查询：如果员工的comm为NULL，则用领导id代替</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> comm, nvl(comm, mgr) <span class="keyword">from</span> emp;</span><br><span class="line">OK</span><br><span class="line">comm    _c1</span><br><span class="line">NULL    7902.0</span><br><span class="line">300.0   300.0</span><br><span class="line">500.0   500.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">1400.0  1400.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7839.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    NULL</span><br><span class="line">0.0     0.0</span><br><span class="line">NULL    7788.0</span><br><span class="line">NULL    7698.0</span><br><span class="line">NULL    7566.0</span><br><span class="line">NULL    7782.0</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-2-case-when"><a href="#6-7-2-case-when" class="headerlink" title="6.7.2 case when"></a>6.7.2 case when</h5><ol><li><p>数据准备</p><table><thead><tr><th>name</th><th>dept_id</th><th>sex</th></tr></thead><tbody><tr><td>悟空</td><td>A</td><td>男</td></tr><tr><td>大海</td><td>A</td><td>男</td></tr><tr><td>宋宋</td><td>B</td><td>男</td></tr><tr><td>凤姐</td><td>A</td><td>女</td></tr><tr><td>婷姐</td><td>B</td><td>女</td></tr><tr><td>婷婷</td><td>B</td><td>女</td></tr></tbody></table></li><li><p>需求</p><p>求出不同部门男女各多少人，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A     2       1</span><br><span class="line">B     1       2</span><br></pre></td></tr></table></figure></li><li><p>创建本地emp_sex.txt，导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi emp_sex.txt</span><br><span class="line">悟空	A	男</span><br><span class="line">大海	A	男</span><br><span class="line">宋宋	B	男</span><br><span class="line">凤姐	A	女</span><br><span class="line">婷姐	B	女</span><br><span class="line">婷婷	B	女</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">dept_id <span class="keyword">string</span>, </span><br><span class="line">sex <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp_sex.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> dept_id,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;女&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span> emp_sex <span class="keyword">group</span> <span class="keyword">by</span> dept_id;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-3-行转列（对多个列进行合并）"><a href="#6-7-3-行转列（对多个列进行合并）" class="headerlink" title="6.7.3 行转列（对多个列进行合并）"></a>6.7.3 行转列（对多个列进行合并）</h5><ol><li><p>相关函数说明</p><p>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p></li><li><p>数据准备</p><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table></li><li><p>需求</p><p>把星座和血型一样的人归类到一起。结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure></li><li><p>创建本地constellation.txt，导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi constellation.txt</span><br><span class="line">孙悟空	白羊座	A</span><br><span class="line">大海	     射手座	A</span><br><span class="line">宋宋	     白羊座	B</span><br><span class="line">猪八戒    白羊座	A</span><br><span class="line">凤姐	     射手座	A</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">constellation <span class="keyword">string</span>, </span><br><span class="line">blood_type <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/constellation.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">&#x27;|&#x27;</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">name</span>,</span><br><span class="line">        <span class="keyword">concat</span>(constellation, <span class="string">&quot;,&quot;</span>, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-4-列转行（把列拆分）"><a href="#6-7-4-列转行（把列拆分）" class="headerlink" title="6.7.4 列转行（把列拆分）"></a>6.7.4 列转行（把列拆分）</h5><ol><li><p>函数说明</p><p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p></li><li><p>数据准备</p><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table></li><li><p>需求</p><p>将电影分类中的数组数据展开，结果如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure></li><li><p>创建本地movie.txt，导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi movie.txt</span><br><span class="line">《疑犯追踪》	悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》	悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》	战争,动作,灾难</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">    movie <span class="keyword">string</span>, </span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;,&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/movie.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-5-窗口函数（开窗函数）"><a href="#6-7-5-窗口函数（开窗函数）" class="headerlink" title="6.7.5 窗口函数（开窗函数）"></a>6.7.5 窗口函数（开窗函数）</h5><ol><li><p>相关函数说明</p><p>（1）OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。</p><p>（2）CURRENT ROW：当前行</p><p>（3）n PRECEDING：往前n行数据</p><p>（4）n FOLLOWING：往后n行数据</p><p>（5）UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>（6）LAG(col,n,default_val)：往前第n行数据</p><p>（7）LEAD(col,n, default_val)：往后第n行数据</p><p>（8）NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<font color="red">注意：n必须为int类型。</font></p></li><li><p>数据准备：name，orderdate，cost</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure></li><li><p>需求</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><p>（2）查询顾客的购买明细及月购买总额</p><p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p><p>（4）查询每个顾客上次的购买时间</p><p>（5）查询前20%时间的订单信息</p></li><li><p>创建本地business.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi business.txt</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">orderdate <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">cost</span> <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&quot;/opt/module/datas/business.txt&quot;</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><p>（1）查询在2017年4月份购买过的顾客及总人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">count</span>(*) <span class="keyword">over</span> () </span><br><span class="line"><span class="keyword">from</span> business </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">&#x27;2017-04&#x27;</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>（2）查询顾客的购买明细及月购买总额</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,<span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) <span class="keyword">from</span></span><br><span class="line"> business;</span><br></pre></td></tr></table></figure><p>（3）上述的场景，将每个顾客的cost按照日期进行累加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行 </span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>rows必须跟在order by子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量。</p><p>（4）查看顾客上次的购买时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line">lag(orderdate,<span class="number">1</span>,<span class="string">&#x27;1900-01-01&#x27;</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1, lag(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2 </span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>（5）查询前20%时间的订单信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> sorted = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-7-6-rank"><a href="#6-7-6-rank" class="headerlink" title="6.7.6 rank"></a>6.7.6 rank</h5><ol><li><p>函数说明</p><p>（1）RANK() 排序相同时会重复，总数不会变</p><p>（2）DENSE_RANK() 排序相同时会重复，总数会减少</p><p>（3）ROW_NUMBER() 会根据顺序计算</p></li><li><p>数据准备</p><table><thead><tr><th>name</th><th>subject</th><th>score</th></tr></thead><tbody><tr><td>孙悟空</td><td>语文</td><td>87</td></tr><tr><td>孙悟空</td><td>数学</td><td>95</td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td></tr><tr><td>大海</td><td>语文</td><td>94</td></tr><tr><td>大海</td><td>数学</td><td>56</td></tr><tr><td>大海</td><td>英语</td><td>84</td></tr><tr><td>宋宋</td><td>语文</td><td>64</td></tr><tr><td>宋宋</td><td>数学</td><td>86</td></tr><tr><td>宋宋</td><td>英语</td><td>84</td></tr><tr><td>婷婷</td><td>语文</td><td>65</td></tr><tr><td>婷婷</td><td>数学</td><td>85</td></tr><tr><td>婷婷</td><td>英语</td><td>78</td></tr></tbody></table></li><li><p>需求</p><p>计算每门学科成绩排名</p></li><li><p>创建本地score.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 datas]$ vi score.txt</span><br></pre></td></tr></table></figure></li><li><p>创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subject <span class="keyword">string</span>, </span><br><span class="line">score <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">&quot;\t&quot;</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/score.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure></li><li><p>按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,</span><br><span class="line">subject,</span><br><span class="line">score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rp,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) drp,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rmp</span><br><span class="line"><span class="keyword">from</span> score;</span><br><span class="line"></span><br><span class="line">name    subject score   rp      drp     rmp</span><br><span class="line">孙悟空  数学    95      1       1       1</span><br><span class="line">宋宋    数学    86      2       2       2</span><br><span class="line">婷婷    数学    85      3       3       3</span><br><span class="line">大海    数学    56      4       4       4</span><br><span class="line">宋宋    英语    84      1       1       1</span><br><span class="line">大海    英语    84      1       1       2</span><br><span class="line">婷婷    英语    78      3       2       3</span><br><span class="line">孙悟空  英语    68      4       3       4</span><br><span class="line">大海    语文    94      1       1       1</span><br><span class="line">孙悟空  语文    87      2       2       2</span><br><span class="line">婷婷    语文    65      3       3       3</span><br><span class="line">宋宋    语文    64      4       4       4</span><br></pre></td></tr></table></figure><p><font color="red">扩展：求出每门学科前三名的学生</font></p></li></ol><h5 id="6-7-7-时间类"><a href="#6-7-7-时间类" class="headerlink" title="6.7.7 时间类"></a>6.7.7 时间类</h5><ol><li><p>date_format:格式化时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">&#x27;1987-5-23&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>date_add:时间跟天数相加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select date_add(&#x27;2019-06-05&#x27;,5);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">2019-06-10</span><br></pre></td></tr></table></figure></li><li><p>date_sub:时间跟天数相减</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select date_sub(&#x27;2019-06-05&#x27;,5);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">2019-05-31</span><br><span class="line">Time taken: 0.343 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li><li><p>datediff:两个时间相减</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select datediff(&#x27;2019-06-29&#x27;,&#x27;2019-07-05&#x27;);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">-6</span><br></pre></td></tr></table></figure></li></ol></div></article></div><div class="aside"><div class="box widget"><div class="introduction"><p><img src="/images/ironman.jpg" alt="head-sculpture"></p><p class="name">罗明辉Eric</p><p class="slogan">个人博客，分享经验，分享快乐</p></div></div><div class="box widget"><div class="title">最新</div><ul class="item-box"><li><a href="/2020/11/24/BigDataFrame/Kylin/">Kylin</a></li><li><a href="/2020/11/13/BigDataFrame/Flink/">Flink</a></li><li><a href="/2020/11/05/BigDataFrame/canal/">Canal</a></li><li><a href="/2020/09/26/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1/">数据仓库-建模</a></li><li><a href="/2020/09/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/">数据仓库-架构</a></li><li><a href="/2020/09/19/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/">数据仓库-概述</a></li><li><a href="/2020/09/18/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a></li><li><a href="/2020/09/10/BigDataFrame/YARN/">YARN</a></li></ul></div><div class="box widget"><div class="title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BI%E5%B7%A5%E5%85%B7/">BI工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ETL%E5%B7%A5%E5%85%B7/">ETL工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/">大数据框架</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a><span class="category-list-count">1</span></li></ul></div><div class="box widget"><div class="title">归档</div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">2020-11</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">2020-09</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">2020-07</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">2020-06</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">2020-05</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">2020-01</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">2019-12</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">2019-07</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">2019-03</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">2018-09</a><span class="archive-list-count">1</span></li></ul></div></div></section><footer class="footer"><div class="global-width footer-box"><div class="copyright"><span>Copyright &copy; 2020</span> <span class="dotted">|</span> <span>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a></span> <span class="dotted">|</span></div></div></footer></div><script>hljs.initHighlightingOnLoad()</script></body></html>